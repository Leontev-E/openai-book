<!doctype html><meta charset="utf-8">
<link rel="stylesheet" href="/cookbook-assets/all.css"><title>Fine-tuning with gpt-oss and Hugging Face Transformers</title><article><div class="flex flex-col items-center pt-8 pb-32 px-4 sm:px-8"><div class="w-full"><div class="w-full"><div class="mx-auto max-w-3xl px-4 sm:px-8"><div class="mb-12 mt-4"><h3 class="text-sm text-gray-500 mb-2 sm:mb-8 text-center">Aug 5, 2025</h3><h1 class="text-xl sm:text-4xl font-bold mb-12 sm:mb-16 text-center">Fine-tuning with gpt-oss and Hugging Face Transformers</h1><div class="flex justify-between items-end"><div class="flex space-x-2 items-center"><div class="flex"><button class="transform transition-all cursor-pointer ml-0" style="z-index:3" rel="noopener noreferrer" aria-label="View profile of edbeeching"><div class="relative rounded-full h-8 w-8 bg-white"><div class="rounded-full h-8 w-8 bg-gray-300 flex items-center justify-center text-white text-sm  border-white border-2">EB</div></div></button><button class="transform transition-all cursor-pointer -ml-2" style="z-index:2" rel="noopener noreferrer" aria-label="View profile of qgallouedec"><div class="relative rounded-full h-8 w-8 bg-white"><div class="rounded-full h-8 w-8 bg-gray-300 flex items-center justify-center text-white text-sm  border-white border-2">QG</div></div></button><button class="transform transition-all cursor-pointer -ml-2" style="z-index:1" rel="noopener noreferrer" aria-label="View profile of lewtun"><div class="relative rounded-full h-8 w-8 bg-white"><div class="rounded-full h-8 w-8 bg-gray-300 flex items-center justify-center text-white text-sm  border-white border-2">LT</div></div></button></div><div class="flex flex-col justify-center"><div><span class="text-sm text-primary"><button aria-label="View profile of edbeeching" rel="noopener noreferrer">Edward Beeching</button>, <button aria-label="View profile of qgallouedec" rel="noopener noreferrer">Quentin Gallouédec</button>, <button aria-label="View profile of lewtun" rel="noopener noreferrer">Lewis Tunstall</button></span></div></div></div><div class="flex flex-row gap-x-0"><a href="https://github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/fine-tune-transfomers.ipynb" target="_blank" rel="noopener noreferrer" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">Open in GitHub</span></a><a href="https://nbviewer.org/format/script/github/openai/openai-cookbook/blob/main/articles/gpt-oss/fine-tune-transfomers.ipynb" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center" target="_blank" rel="noopener noreferrer"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M3 2.5C3 2.22386 3.22386 2 3.5 2H9.08579C9.21839 2 9.34557 2.05268 9.43934 2.14645L11.8536 4.56066C11.9473 4.65443 12 4.78161 12 4.91421V12.5C12 12.7761 11.7761 13 11.5 13H3.5C3.22386 13 3 12.7761 3 12.5V2.5ZM3.5 1C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H11.5C12.3284 14 13 13.3284 13 12.5V4.91421C13 4.51639 12.842 4.13486 12.5607 3.85355L10.1464 1.43934C9.86514 1.15804 9.48361 1 9.08579 1H3.5ZM4.5 4C4.22386 4 4 4.22386 4 4.5C4 4.77614 4.22386 5 4.5 5H7.5C7.77614 5 8 4.77614 8 4.5C8 4.22386 7.77614 4 7.5 4H4.5ZM4.5 7C4.22386 7 4 7.22386 4 7.5C4 7.77614 4.22386 8 4.5 8H10.5C10.7761 8 11 7.77614 11 7.5C11 7.22386 10.7761 7 10.5 7H4.5ZM4.5 10C4.22386 10 4 10.2239 4 10.5C4 10.7761 4.22386 11 4.5 11H10.5C10.7761 11 11 10.7761 11 10.5C11 10.2239 10.7761 10 10.5 10H4.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">View as Markdown</span></a></div></div></div></div><div id="content" class="w-full"><div class="w-full px-6 lg:px-8"><div class="grid grid-cols-1 lg:grid-cols-[clamp(12rem,20vw,18rem)_minmax(0,1fr)] gap-14"><div class="mx-auto w-full max-w-[clamp(60ch,72vw,90ch)] px-4 sm:px-8"><div class="flex flex-col space-y-6"><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Authored by: <a href="https://huggingface.co/edbeeching">Edward Beeching</a>, <a href="https://huggingface.co/qgallouedec">Quentin Gallouédec</a>, and <a href="https://huggingface.co/lewtun">Lewis Tunstall</a></p>
<p>Large reasoning models like <a href="https://openai.com/index/introducing-o3-and-o4-mini/">OpenAI o3</a> generate a chain-of-thought to improve the accuracy and quality of their responses. However, most of these models reason in English, even when a question is asked in another language.</p>
<p>In this notebook, we show how OpenAI's open-weight reasoning model <a href="https://huggingface.co/openai/gpt-oss-20b">OpenAI gpt-oss-20b</a> can be fine-tuned to reason effectively in multiple languages. We'll do this by adding a new <em>"reasoning language"</em> option to the model's system prompt, and applying <a href="https://huggingface.co/learn/llm-course/chapter11/1">supervised fine-tuning</a> with Hugging Face's <a href="https://github.com/huggingface/trl">TRL library</a> on a multilingual reasoning dataset.</p>
<p>We'll cover the following steps:</p>
<ol>
<li><strong>Setup:</strong> Install the required libraries.</li>
<li><strong>Prepare the dataset:</strong>  Download and format the dataset for fine-tuning.</li>
<li><strong>Prepare the model:</strong> Loading the base model and configure it for fine-tuning <a href="https://huggingface.co/learn/llm-course/chapter11/4">LoRA</a>, a memory efficient technique.</li>
<li><strong>Fine-tuning:</strong> Train the model with our multilingual reasoning data.</li>
<li><strong>Inference:</strong> Generate reasoning responses in different languages using the fine-tuned model.</li>
</ol>
<p>The end result is a multilingual reasoning model that can generate a chain-of-thought in English, Spanish, French, Italian, or German. You can even <em>mix languages</em>—for example, ask a question in Spanish, request reasoning in German, and receive the final response in Spanish:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="txt" data-theme="default" style="display:grid"><span data-line=""><span style="color:#e1e4e8">User:</span></span>
<span data-line=""><span style="color:#e1e4e8">    ¿Cuál es el capital de Australia?</span></span>
<span data-line=""><span style="color:#e1e4e8">Assistant reasoning:</span></span>
<span data-line=""><span style="color:#e1e4e8">    Okay, der Benutzer fragt nach der Hauptstadt Australiens. Ich erinnere mich, dass Canberra die Hauptstadt ist. Ich</span></span>
<span data-line=""><span style="color:#e1e4e8">    sollte das bestätigen. Lass mich sehen, ob es irgendwelche potenziellen Verwirrungen gibt. Der Benutzer könnte auch</span></span>
<span data-line=""><span style="color:#e1e4e8">    an der größten Stadt interessiert sein. Die größte Stadt ist Sydney, aber die Hauptstadt ist Canberra. Ich sollte</span></span>
<span data-line=""><span style="color:#e1e4e8">    das klarstellen. Vielleicht auch erwähnen, dass Canberra eine geplante Stadt ist und nicht die größte. Der Benutzer</span></span>
<span data-line=""><span style="color:#e1e4e8">    könnte auch nach der Geografie fragen. Vielleicht erwähne ich, dass Canberra im südwestlichen Teil der Australian</span></span>
<span data-line=""><span style="color:#e1e4e8">    Capital Territory liegt. Ich sollte die Antwort präzise und freundlich halten. Vielleicht auch erwähnen, dass</span></span>
<span data-line=""><span style="color:#e1e4e8">    Canberra oft mit Sydney verwechselt wird. Ich sollte sicherstellen, dass die Antwort klar und korrekt ist.</span></span>
<span data-line=""><span style="color:#e1e4e8">Assistant response:</span></span>
<span data-line=""><span style="color:#e1e4e8">    La capital de Australia es **Canberra**. Aunque es la ciudad más pequeña de las principales capitales del país, fue</span></span>
<span data-line=""><span style="color:#e1e4e8">    elegida en 1908 como la sede del gobierno federal para equilibrar la influencia entre las ciudades de Sydney y</span></span>
<span data-line=""><span style="color:#e1e4e8">    Melbourne. Canberra está ubicada en el Territorio de la Capital Australiana (ACT), en el este de Australia.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>We hope this tutorial will enable AI developers working with under-represented languages to improve the interpretability of <a href="https://huggingface.co/openai/gpt-oss-20b"><code>openai/gpt-oss-20b</code></a> in their native languages.</p>
<blockquote>
<p><strong>Note:</strong> This notebook is designed to be run on a single H100 GPU with 80GB of memory. If you have access to a smaller GPU, you can reduce the batch size and sequence length in the hyperparameters below.</p>
</blockquote></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="setup"><a class="heading-link" href="#setup">Setup</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>To get started, let’s install all the necessary libraries. First install PyTorch:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">%</span><span style="color: #E1E4E8">pip install torch </span><span style="color: #FDAEB7; font-style: italic">--</span><span style="color: #E1E4E8">index</span><span style="color: #F97583">-</span><span style="color: #E1E4E8">url https:</span><span style="color: #F97583">//</span><span style="color: #E1E4E8">download.pytorch.org</span><span style="color: #F97583">/</span><span style="color: #E1E4E8">whl</span><span style="color: #F97583">/</span><span style="color: #E1E4E8">cu128</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Next, install the remaining dependencies:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">%</span><span style="color: #E1E4E8">pip install </span><span style="color: #9ECBFF">"trl&gt;=0.20.0"</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"peft&gt;=0.17.0"</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"transformers&gt;=4.55.0"</span><span style="color: #E1E4E8"> trackio</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Finally, log into your Hugging Face account as follows:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> huggingface_hub </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> notebook_login</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">notebook_login()</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Now that we've installed the required libraries, let's take a look at the dataset that we will use for fine-tuning.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="prepare-the-dataset"><a class="heading-link" href="#prepare-the-dataset">Prepare the dataset</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>We will be using <a href="https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking">Multilingual-Thinking</a>, which is a reasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning <code>openai/gpt-oss-20b</code> on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><iframe src="https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking/embed/viewer/default/train" width="100%" height="560px"></iframe></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Let's download this dataset from the Hugging Face Hub:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> datasets </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> load_dataset</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">dataset </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> load_dataset(</span><span style="color: #9ECBFF">"HuggingFaceH4/Multilingual-Thinking"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">split</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"train"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">dataset</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>This is a small dataset of 1,000 examples, but this is usually more than sufficient for models like <code>openai/gpt-oss-20b</code> which have undergone extensive post-training. Let's take a look at one of the training examples:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">dataset[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>The <code>gpt-oss</code> models were trained on the Harmony response format for defining conversation structures, generating reasoning output and structuring function calls. The format is designed to mimic the OpenAI Responses API, and the table below summarizes the different message types used in the dataset:</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert">
































<table><thead><tr><th style="text-align:left"></th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left"><code>developer</code></td><td style="text-align:left">The developer message is used to provide custom instructions for the model (what we usually call the <code>system</code> role).</td></tr><tr><td style="text-align:left"><code>user</code></td><td style="text-align:left">The user message is used to provide the input to the model.</td></tr><tr><td style="text-align:left"><code>assistant</code></td><td style="text-align:left">Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.</td></tr><tr><td style="text-align:left"><code>analysis</code></td><td style="text-align:left">These are messages that are being used by the model for its chain-of-thought</td></tr><tr><td style="text-align:left"><code>final</code></td><td style="text-align:left">Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.</td></tr><tr><td style="text-align:left"><code>messages</code></td><td style="text-align:left">The list of messages that combine the content of the above to produce a full conversation. This is the input to the model.</td></tr></tbody></table></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>If you're familiar with <a href="https://platform.openai.com/docs/api-reference/messages/object">OpenAI's messages format</a>, you will recognise this as being quite similar, but with an important difference:</p>
<blockquote>
<p>The <code>assistant</code> turn contains two special fields: a <code>thinking</code> one which contains the model's reasoning process, and a <code>content</code> one which contains the final response to the user.</p>
</blockquote>
<p>In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's <a href="https://huggingface.co/docs/transformers/chat_templating"><em>chat template</em></a> and then tokenizing the resulting text. The TRL library does this automatically, but let's walk through it step by step to understand how it works.</p>
<p>To do so, let's first load the tokenizer:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> transformers </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">tokenizer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> AutoTokenizer.from_pretrained(</span><span style="color: #9ECBFF">"openai/gpt-oss-20b"</span><span style="color: #E1E4E8">)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Then we can use the tokenizer's <code>apply_chat_template()</code> method to format the messages:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">messages </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> dataset[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">][</span><span style="color: #9ECBFF">"messages"</span><span style="color: #E1E4E8">]</span></span>
<span class="line"><span style="color: #E1E4E8">conversation </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.apply_chat_template(messages, </span><span style="color: #FFAB70">tokenize</span><span style="color: #F97583">=</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(conversation)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>This chat template is quite sophisticated, so let's take a closer look at it! First, we can see there are special tokens <code>&lt;|start|&gt;</code> and <code>&lt;|end|&gt;</code> that indicate the start and end of each message. There is also a <code>&lt;|return|&gt;</code> token that marks the end of the conversation. These tokens help the model understand the structure of the conversation.</p>
<p>We can also see there are <em>two</em> types of system message:</p>
<ul>
<li>A default <code>system</code> one that is used for all messages. In the example above, this refers to the text <em>"You are ChatGPT, a large language model trained by OpenAI..."</em></li>
<li>A special <code>developer</code> one that contains custom instructions (defined by the <code>system</code> role in our <code>messages</code> object). This allows us to provide additional context to the model about how it should behave for a given conversation. In the example above, this refers to the text <em>"You are an AI chatbot with a lively and energetic personality."</em></li>
</ul>
<p>Finally, we can see that the assistant response is contained in a series of <em>channels</em>:</p>
<ul>
<li>The <code>analysis</code> channel is used for the model's reasoning process, where it can think step by step about the user's question. In the example above, this refers to the French text <em>"D'accord, l'utilisateur demande les tendances Twitter..."</em></li>
<li>The <code>final</code> channel is used for the model's final response to the user. In the example above, this refers to the text <em>"Hey there!  While I can't check Twitter..."</em></li>
</ul></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Now that we understand how the dataset will be prepared, let's move on to preparing the model for training.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="prepare-the-model"><a class="heading-link" href="#prepare-the-model">Prepare the model</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>To prepare the model for training, let's first download the weights from the <a href="https://huggingface.co">Hugging Face Hub</a>. We will use the <code>AutoModelForCausalLM</code> class from 🤗 Transformers to load the model:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> torch</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> transformers </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> AutoModelForCausalLM, Mxfp4Config</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">quantization_config </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> Mxfp4Config(</span><span style="color: #FFAB70">dequantize</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">model_kwargs </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">dict</span><span style="color: #E1E4E8">(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">attn_implementation</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"eager"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">torch_dtype</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">torch.bfloat16,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">quantization_config</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">quantization_config,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">use_cache</span><span style="color: #F97583">=</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">device_map</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"auto"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">model </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> AutoModelForCausalLM.from_pretrained(</span><span style="color: #9ECBFF">"openai/gpt-oss-20b"</span><span style="color: #E1E4E8">, </span><span style="color: #F97583">**</span><span style="color: #E1E4E8">model_kwargs)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>This will load the model with the necessary configurations for training. The <code>attn_implementation</code> is set to <code>eager</code> for better performance, and <code>use_cache</code> is set to <code>False</code> since we will fine-tune the model with gradient checkpointing.</p>
<p>If you're familiar with 🤗 Transformers, you might notice that we are using the <code>Mxfp4Config</code> for quantization. This is a specific configuration for the OpenAI models that allows us to use mixed precision training with a special 4-bit floating point format called <a href="https://en.wikipedia.org/wiki/Block_floating_point">MXFP4</a> that is optimized for AI workloads.</p>
<p>Before we train the model, let's generate a sample response to see how the model behaves with the default settings. To do so, we need to tokenize a sample prompt and then use the model to generate a response:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">messages </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> [</span></span>
<span class="line"><span style="color: #E1E4E8">    {</span><span style="color: #9ECBFF">"role"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"user"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"content"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"¿Cuál es el capital de Australia?"</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">input_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color: #E1E4E8">    messages,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">add_generation_prompt</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">return_tensors</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"pt"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">output_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> model.generate(input_ids, </span><span style="color: #FFAB70">max_new_tokens</span><span style="color: #F97583">=</span><span style="color: #79B8FF">512</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">response </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.batch_decode(output_ids)[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(response)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>In this example, we can see that the model first reasons about the question in English, and then provides a final response in Spanish. This is the default behavior of the model, but let's see if we can change it with a bit of fine-tuning.</p>
<p>To do so, we will use a technique called <a href="https://huggingface.co/learn/llm-course/chapter11/4">LoRA</a> (Low-Rank Adaptation) to fine-tune the model. This technique allows us to tune a few specific layers of the model, which is particularly useful for large models like <code>openai/gpt-oss-20b</code>.</p>
<p>First we need to wrap the model as a <code>PeftModel</code> and define the LoRA configuration. We will use the <code>LoraConfig</code> class from the <a href="https://github.com/huggingface/peft">PEFT library</a> to do this:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> peft </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> LoraConfig, get_peft_model</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">peft_config </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> LoraConfig(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">r</span><span style="color: #F97583">=</span><span style="color: #79B8FF">8</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">lora_alpha</span><span style="color: #F97583">=</span><span style="color: #79B8FF">16</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">target_modules</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"all-linear"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">target_parameters</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"7.mlp.experts.gate_up_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"7.mlp.experts.down_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"15.mlp.experts.gate_up_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"15.mlp.experts.down_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"23.mlp.experts.gate_up_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #9ECBFF">"23.mlp.experts.down_proj"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    ],</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">peft_model </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> get_peft_model(model, peft_config)</span></span>
<span class="line"><span style="color: #E1E4E8">peft_model.print_trainable_parameters()</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Here we've used some basic hyperparameters for LoRA, but you can experiment with different values to see how they affect the model's performance. For instance, if you increase <code>r</code> you will enable more trainable parameters, which may produce a better model at the expense of requiring more VRAM and time to train.</p>
<p><strong>Note:</strong> The <code>openai/gpt-oss-20b</code> model is a <a href="https://huggingface.co/blog/moe">Mixture-of-Experts (MoE)</a> architecture. In addition to targeting the attention layers (<code>target_modules="all-linear"</code>), it’s also important to include the projection layers within the expert modules. PEFT facilitates this via the <code>target_parameters</code> argument, which allows you to specify expert-specific layers such as <code>mlp.experts.down_proj</code> and <code>mlp.experts.gate_up_proj</code>. In this example, we target a subset of these projection layers, but you are encouraged to experiment with different configurations.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Now that we have the model and dataset ready, we can define the hyperparameters for training.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="fine-tuning"><a class="heading-link" href="#fine-tuning">Fine-tuning</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>TRL provides a convenient way to define hyperparameters for training using the <code>SFTConfig</code> class. We will set the learning rate, batch size, number of epochs, and other parameters as follows:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> trl </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> SFTConfig</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">training_args </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> SFTConfig(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">learning_rate</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2e-4</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">gradient_checkpointing</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">num_train_epochs</span><span style="color: #F97583">=</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">logging_steps</span><span style="color: #F97583">=</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">per_device_train_batch_size</span><span style="color: #F97583">=</span><span style="color: #79B8FF">4</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">gradient_accumulation_steps</span><span style="color: #F97583">=</span><span style="color: #79B8FF">4</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">max_length</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2048</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">warmup_ratio</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.03</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">lr_scheduler_type</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"cosine_with_min_lr"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">lr_scheduler_kwargs</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">{</span><span style="color: #9ECBFF">"min_lr_rate"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">0.1</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">output_dir</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"gpt-oss-20b-multilingual-reasoner"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">report_to</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"trackio"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">push_to_hub</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Note that the <code>per_device_train_batch_size</code> is set to 4, and the <code>gradient_accumulation_steps</code> is set to 4. This means that we will effectively have a batch size of 4 x 4 = 16 across 1 GPU. You may need to adjust these values based on your hardware setup. We also use <a href="https://huggingface.co/blog/trackio">Trackio</a> to log the training progress and metrics, but you can use any other logging library of your choice.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>We now have all the pieces needed to train the model. We will use the <code>SFTTrainer</code> class from TRL to handle the training process. The trainer will take care of formatting the dataset, applying the chat template, and training the model:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> trl </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> SFTTrainer</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">trainer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> SFTTrainer(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">peft_model,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">args</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">training_args,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">train_dataset</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">dataset,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">processing_class</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">tokenizer,</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">trainer.train()</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>On a H100 GPU, this takes about 18 minutes to train, but may take longer depending on your hardware.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="save-the-model-and-push-to-the-hugging-face-hub"><a class="heading-link" href="#save-the-model-and-push-to-the-hugging-face-hub">Save the model and push to the Hugging Face Hub</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Finally, you can push the fine-tuned model to your Hub repository to share with the community:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">trainer.save_model(training_args.output_dir)</span></span>
<span class="line"><span style="color: #E1E4E8">trainer.push_to_hub(</span><span style="color: #FFAB70">dataset_name</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"HuggingFaceH4/Multilingual-Thinking"</span><span style="color: #E1E4E8">)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p><strong>Note</strong>: To avoid out-of-memory (OOM) errors, we recommend restarting the kernel at this point. The trained model is still occupying GPU memory, but it's no longer needed.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="inference"><a class="heading-link" href="#inference">Inference</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Once the model is uploaded to Hub, we can use it for inference. To do so we first initialize the original base model and its tokenizer. Next, we need to merge the fine-tuned weights with the base model for fast inference:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> transformers </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> peft </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> PeftModel</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Load the tokenizer</span></span>
<span class="line"><span style="color: #E1E4E8">tokenizer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> AutoTokenizer.from_pretrained(</span><span style="color: #9ECBFF">"openai/gpt-oss-20b"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Load the original model first</span></span>
<span class="line"><span style="color: #E1E4E8">model_kwargs </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">dict</span><span style="color: #E1E4E8">(</span><span style="color: #FFAB70">attn_implementation</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"eager"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">torch_dtype</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"auto"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">use_cache</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">device_map</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"auto"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">base_model </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> AutoModelForCausalLM.from_pretrained(</span><span style="color: #9ECBFF">"openai/gpt-oss-20b"</span><span style="color: #E1E4E8">, </span><span style="color: #F97583">**</span><span style="color: #E1E4E8">model_kwargs).cuda()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Merge fine-tuned weights with the base model</span></span>
<span class="line"><span style="color: #E1E4E8">peft_model_id </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"gpt-oss-20b-multilingual-reasoner"</span></span>
<span class="line"><span style="color: #E1E4E8">model </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> PeftModel.from_pretrained(base_model, peft_model_id)</span></span>
<span class="line"><span style="color: #E1E4E8">model </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> model.merge_and_unload()</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Now that the model is loaded, the final step is to generate some tokens from it! Here we use the model's <code>generate</code> method to produce output based on the input prompt. Let's first define the prompt:</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Now we can tokenize the prompt and generate the output. Finally, we can decode the output tokens to get the final response:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #79B8FF">REASONING_LANGUAGE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"German"</span></span>
<span class="line"><span style="color: #79B8FF">SYSTEM_PROMPT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"reasoning language: </span><span style="color: #79B8FF">{REASONING_LANGUAGE}</span><span style="color: #9ECBFF">"</span></span>
<span class="line"><span style="color: #79B8FF">USER_PROMPT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"¿Cuál es el capital de Australia?"</span><span style="color: #E1E4E8">  </span><span style="color: #6A737D"># Spanish for "What is the capital of Australia?"</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">messages </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> [</span></span>
<span class="line"><span style="color: #E1E4E8">    {</span><span style="color: #9ECBFF">"role"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"system"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"content"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">SYSTEM_PROMPT</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">    {</span><span style="color: #9ECBFF">"role"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"user"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"content"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">USER_PROMPT</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">input_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color: #E1E4E8">    messages,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">add_generation_prompt</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">return_tensors</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"pt"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">gen_kwargs </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> {</span><span style="color: #9ECBFF">"max_new_tokens"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">512</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"do_sample"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"temperature"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">0.6</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"top_p"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">None</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"top_k"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">None</span><span style="color: #E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">output_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> model.generate(input_ids, </span><span style="color: #F97583">**</span><span style="color: #E1E4E8">gen_kwargs)</span></span>
<span class="line"><span style="color: #E1E4E8">response </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.batch_decode(output_ids)[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(response)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Let's also try with languages that the model has not been explicitly fine-tuned on, such as Chinese and Hindi:</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #79B8FF">REASONING_LANGUAGE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"Chinese"</span><span style="color: #E1E4E8">  </span><span style="color: #6A737D"># or Hindi, or any other language...</span></span>
<span class="line"><span style="color: #79B8FF">SYSTEM_PROMPT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"reasoning language: </span><span style="color: #79B8FF">{REASONING_LANGUAGE}</span><span style="color: #9ECBFF">"</span></span>
<span class="line"><span style="color: #79B8FF">USER_PROMPT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"What is the national symbol of Canada?"</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">messages </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> [</span></span>
<span class="line"><span style="color: #E1E4E8">    {</span><span style="color: #9ECBFF">"role"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"system"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"content"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">SYSTEM_PROMPT</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">    {</span><span style="color: #9ECBFF">"role"</span><span style="color: #E1E4E8">: </span><span style="color: #9ECBFF">"user"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"content"</span><span style="color: #E1E4E8">: </span><span style="color: #79B8FF">USER_PROMPT</span><span style="color: #E1E4E8">},</span></span>
<span class="line"><span style="color: #E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">input_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color: #E1E4E8">    messages,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">add_generation_prompt</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">return_tensors</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"pt"</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">output_ids </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> model.generate(input_ids, </span><span style="color: #F97583">**</span><span style="color: #E1E4E8">gen_kwargs)</span></span>
<span class="line"><span style="color: #E1E4E8">response </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> tokenizer.batch_decode(output_ids)[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(response)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Great, it works - we've now fine-tuned <code>openai/gpt-oss-20b</code> to reason in multiple languages!</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="conclusion"><a class="heading-link" href="#conclusion">Conclusion</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Congratulations! You have successfully fine-tuned a multilingual reasoning model using the TRL library and LoRA. The steps in this notebook can be adapted to fine-tune <a href="https://huggingface.co/openai/gpt-oss-20b"><code>openai/gpt-oss-20b</code></a> on many other <a href="https://huggingface.co/datasets">datasets</a> on the Hugging Face Hub - we are excited to see what you'll build!</p></article></div></div></div></div></div></div></div></div><next-route-announcer style="position: absolute;"></next-route-announcer></article>