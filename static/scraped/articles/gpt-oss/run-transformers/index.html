<!doctype html><meta charset="utf-8">
<link rel="stylesheet" href="/cookbook-assets/all.css"><title>How to run gpt-oss with Transformers</title><article><div class="flex flex-col items-center pt-8 pb-32 px-4 sm:px-8"><div class="w-full"><div class="w-full"><div class="mx-auto max-w-3xl px-4 sm:px-8"><div class="mb-12 mt-4"><h3 class="text-sm text-gray-500 mb-2 sm:mb-8 text-center">Aug 5, 2025</h3><h1 class="text-xl sm:text-4xl font-bold mb-12 sm:mb-16 text-center">How to run gpt-oss with Transformers</h1><div class="flex justify-between items-end"><div class="flex space-x-2 items-center"><div class="flex"><button class="transform transition-all cursor-pointer ml-0" style="z-index:1" rel="noopener noreferrer" aria-label="View profile of dkundel-openai"><div class="relative rounded-full h-8 w-8 bg-white"><img alt="Dominik Kundel" draggable="false" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" class="rounded-full h-8 w-8 border border-gray-30" style="color:transparent" srcset="/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=32&amp;q=75 1x, /_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=64&amp;q=75 2x" src="https://cookbook.openai.com/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=64&amp;q=75"><div class="absolute bottom-[-2px] right-[-2px] rounded-full bg-white h-4 w-4 flex items-center justify-center"><img alt="Verified" loading="lazy" width="512" height="512" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/openai-logomark-2.63a8dac2.svg"></div></div></button></div><div class="flex flex-col justify-center"><div><span class="text-sm text-primary"><button aria-label="View profile of dkundel-openai" rel="noopener noreferrer">Dominik Kundel<span class="ml-1 text-xs text-muted-foreground">(OpenAI)</span></button></span></div></div></div><div class="flex flex-row gap-x-0"><a href="https://github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-transformers.md" target="_blank" rel="noopener noreferrer" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">Open in GitHub</span></a></div></div></div></div><div id="content" class="w-full"><div class="w-full px-6 lg:px-8"><div class="grid grid-cols-1 lg:grid-cols-[clamp(12rem,20vw,18rem)_minmax(0,1fr)] gap-14"><div class="mx-auto w-full max-w-[clamp(60ch,72vw,90ch)] px-4 sm:px-8"><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>The Transformers library by Hugging Face provides a flexible way to load and run large language models locally or on a server. This guide will walk you through running <a href="https://huggingface.co/openai/gpt-oss-20b">OpenAI gpt-oss-20b</a> or <a href="https://huggingface.co/openai/gpt-oss-120b">OpenAI gpt-oss-120b</a> using Transformers, either with a high-level pipeline or via low-level <code>generate</code> calls with raw token IDs.</p>
<p>We'll cover the use of <a href="https://huggingface.co/openai/gpt-oss-20b">OpenAI gpt-oss-20b</a> or <a href="https://huggingface.co/openai/gpt-oss-120b">OpenAI gpt-oss-120b</a> with the high-level pipeline abstraction, low-level `generate` calls, and serving models locally with `transformers serve`, with in a way compatible with the Responses API.</p>
<p>In this guide we’ll run through various optimised ways to run the <strong>gpt-oss models via Transformers.</strong></p>
<p>Bonus: You can also fine-tune models via transformers, <a href="https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers">check out our fine-tuning guide here</a>.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="pick-your-model"><a class="heading-link" href="#pick-your-model">Pick your model</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Both <strong>gpt-oss</strong> models are available on Hugging Face:</p>
<ul>
<li><strong><code>openai/gpt-oss-20b</code></strong>
<ul>
<li>~16GB VRAM requirement when using MXFP4</li>
<li>Great for single high-end consumer GPUs</li>
</ul>
</li>
<li><strong><code>openai/gpt-oss-120b</code></strong>
<ul>
<li>Requires ≥60GB VRAM or multi-GPU setup</li>
<li>Ideal for H100-class hardware</li>
</ul>
</li>
</ul>
<p>Both are <strong>MXFP4 quantized</strong> by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.</p>
<p>If you use <code>bfloat16</code> instead of MXFP4, memory consumption will be larger (~48 GB for the 20b parameter model).</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="quick-setup"><a class="heading-link" href="#quick-setup">Quick setup</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<ol>
<li><strong>Install dependencies</strong><br>
It’s recommended to create a fresh Python environment. Install transformers, accelerate, as well as the Triton kernels for MXFP4 compatibility:</li>
</ol>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">pip</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">install</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-U</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">transformers</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">accelerate</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">torch</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">triton==</span><span style="color:#79B8FF">3.4</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">kernels</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<ol start="2">
<li><strong>(Optional) Enable multi-GPU</strong><br>
If you’re running large models, use Accelerate or torchrun to handle device mapping automatically.</li>
</ol>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="create-an-open-ai-responses--chat-completions-endpoint"><a class="heading-link" href="#create-an-open-ai-responses--chat-completions-endpoint">Create an Open AI Responses / Chat Completions endpoint</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>To launch a server, simply use the <code>transformers serve</code> CLI command:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">transformers</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">serve</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>The simplest way to interact with the server is through the transformers chat CLI</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">transformers</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">chat</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">localhost:8000</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">--model-name-or-path</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">openai/gpt-oss-20b</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>or by sending an HTTP request with cURL, e.g.</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">curl</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-X</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">POST</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">http://localhost:8000/v1/responses</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-H</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"Content-Type: application/json"</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-d</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">'{"messages": [{"role": "system", "content": "hello"}], "temperature": 0.9, "max_tokens": 1000, "stream": true, "model": "openai/gpt-oss-20b"}'</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Additional use cases, like integrating <code>transformers serve</code> with Cursor and other tools, are detailed in <a href="https://huggingface.co/docs/transformers/main/serving">the documentation</a>.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="quick-inference-with-pipeline"><a class="heading-link" href="#quick-inference-with-pipeline">Quick inference with pipeline</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>The easiest way to run the gpt-oss models is with the Transformers high-level <code>pipeline</code> API:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> pipeline</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">generator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pipeline(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#9ECBFF">"text-generation"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"openai/gpt-oss-20b"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">  </span><span style="color:#6A737D"># Automatically place on available GPUs</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span data-line=""><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Explain what MXFP4 quantization is."</span><span style="color:#E1E4E8">},</span></span>
<span data-line=""><span style="color:#E1E4E8">]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> generator(</span></span>
<span data-line=""><span style="color:#E1E4E8">    messages,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">200</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">temperature</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(result[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">"generated_text"</span><span style="color:#E1E4E8">])</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="advanced-inference-with-generate"><a class="heading-link" href="#advanced-inference-with-generate">Advanced inference with <code>.generate()</code></a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>If you want more control, you can load the model and tokenizer manually and invoke the <code>.generate()</code> method:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"openai/gpt-oss-20b"</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span data-line=""><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span></span>
<span data-line=""><span style="color:#E1E4E8">    model_name,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span data-line=""><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Explain what MXFP4 quantization is."</span><span style="color:#E1E4E8">},</span></span>
<span data-line=""><span style="color:#E1E4E8">]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span data-line=""><span style="color:#E1E4E8">    messages,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_dict</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">).to(model.device)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#F97583">**</span><span style="color:#E1E4E8">inputs,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">200</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">temperature</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.7</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(tokenizer.decode(outputs[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]))</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="chat-template-and-tool-calling"><a class="heading-link" href="#chat-template-and-tool-calling">Chat template and tool calling</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>OpenAI gpt-oss models use the <a href="https://cookbook.openai.com/article/harmony">harmony response format</a> for structuring messages, including reasoning and tool calls.</p>
<p>To construct prompts you can use the built-in chat template of Transformers. Alternatively, you can install and use the <a href="https://github.com/openai/harmony">openai-harmony library</a> for more control.</p>
<p>To use the chat template:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"openai/gpt-oss-20b"</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span data-line=""><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span></span>
<span data-line=""><span style="color:#E1E4E8">    model_name,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span data-line=""><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"system"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Always respond in riddles"</span><span style="color:#E1E4E8">},</span></span>
<span data-line=""><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"What is the weather like in Madrid?"</span><span style="color:#E1E4E8">},</span></span>
<span data-line=""><span style="color:#E1E4E8">]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span data-line=""><span style="color:#E1E4E8">    messages,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_dict</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">).to(model.device)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">generated </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span><span style="color:#F97583">**</span><span style="color:#E1E4E8">inputs, </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">100</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(tokenizer.decode(generated[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][inputs[</span><span style="color:#9ECBFF">"input_ids"</span><span style="color:#E1E4E8">].shape[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">] :]))</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>To integrate the <a href="https://github.com/openai/harmony"><code>openai-harmony</code></a> library to prepare prompts and parse responses, first install it like this:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">pip</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">install</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">openai-harmony</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Here’s an example of how to use the library to build your prompts and encode them to tokens:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">import</span><span style="color:#E1E4E8"> json</span></span>
<span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> openai_harmony </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> (</span></span>
<span data-line=""><span style="color:#E1E4E8">    HarmonyEncodingName,</span></span>
<span data-line=""><span style="color:#E1E4E8">    load_harmony_encoding,</span></span>
<span data-line=""><span style="color:#E1E4E8">    Conversation,</span></span>
<span data-line=""><span style="color:#E1E4E8">    Message,</span></span>
<span data-line=""><span style="color:#E1E4E8">    Role,</span></span>
<span data-line=""><span style="color:#E1E4E8">    SystemContent,</span></span>
<span data-line=""><span style="color:#E1E4E8">    DeveloperContent</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">encoding </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> load_harmony_encoding(HarmonyEncodingName.</span><span style="color:#79B8FF">HARMONY_GPT_OSS</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Build conversation</span></span>
<span data-line=""><span style="color:#E1E4E8">convo </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Conversation.from_messages([</span></span>
<span data-line=""><span style="color:#E1E4E8">    Message.from_role_and_content(Role.</span><span style="color:#79B8FF">SYSTEM</span><span style="color:#E1E4E8">, SystemContent.new()),</span></span>
<span data-line=""><span style="color:#E1E4E8">    Message.from_role_and_content(</span></span>
<span data-line=""><span style="color:#E1E4E8">        Role.</span><span style="color:#79B8FF">DEVELOPER</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">        DeveloperContent.new().with_instructions(</span><span style="color:#9ECBFF">"Always respond in riddles"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">    ),</span></span>
<span data-line=""><span style="color:#E1E4E8">    Message.from_role_and_content(Role.</span><span style="color:#79B8FF">USER</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"What is the weather like in SF?"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">])</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Render prompt</span></span>
<span data-line=""><span style="color:#E1E4E8">prefill_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> encoding.render_conversation_for_completion(convo, Role.</span><span style="color:#79B8FF">ASSISTANT</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">stop_token_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> encoding.stop_tokens_for_assistant_actions()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Load model</span></span>
<span data-line=""><span style="color:#E1E4E8">model_name </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"openai/gpt-oss-20b"</span></span>
<span data-line=""><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span data-line=""><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(model_name, </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">device_map</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Generate</span></span>
<span data-line=""><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">input_ids</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[prefill_ids],</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">128</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">eos_token_id</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">stop_token_ids</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Parse completion tokens</span></span>
<span data-line=""><span style="color:#E1E4E8">completion_ids </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> outputs[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(prefill_ids):]</span></span>
<span data-line=""><span style="color:#E1E4E8">entries </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> encoding.parse_messages_from_completion_tokens(completion_ids, Role.</span><span style="color:#79B8FF">ASSISTANT</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">for</span><span style="color:#E1E4E8"> message </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> entries:</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(json.dumps(message.to_dict(), </span><span style="color:#FFAB70">indent</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">))</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Note that the <code>Developer</code> role in Harmony maps to the <code>system</code> prompt in the chat template.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="multi-gpu--distributed-inference"><a class="heading-link" href="#multi-gpu--distributed-inference">Multi-GPU &amp; distributed inference</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>The large gpt-oss-120b fits on a single H100 GPU when using MXFP4. If you want to run it on multiple GPUs, you can:</p>
<ul>
<li>Use <code>tp_plan="auto"</code> for automatic placement and tensor parallelism</li>
<li>Launch with <code>accelerate launch or torchrun</code> for distributed setups</li>
<li>Leverage Expert Parallelism</li>
<li>Use specialised Flash attention kernels for faster inference</li>
</ul>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> transformers.distributed </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DistributedConfig</span></span>
<span data-line=""><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">model_path </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"openai/gpt-oss-120b"</span></span>
<span data-line=""><span style="color:#E1E4E8">tokenizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoTokenizer.from_pretrained(model_path, </span><span style="color:#FFAB70">padding_side</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"left"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">device_map </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#6A737D"># Enable Expert Parallelism</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#9ECBFF">"distributed_config"</span><span style="color:#E1E4E8">: DistributedConfig(</span><span style="color:#FFAB70">enable_expert_parallel</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">),</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#6A737D"># Enable Tensor Parallelism</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#9ECBFF">"tp_plan"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">}</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span></span>
<span data-line=""><span style="color:#E1E4E8">    model_path,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">torch_dtype</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"auto"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">attn_implementation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"kernels-community/vllm-flash-attn3"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#F97583">**</span><span style="color:#E1E4E8">device_map,</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">messages </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span data-line=""><span style="color:#E1E4E8">     {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Explain how expert parallelism works in large language models."</span><span style="color:#E1E4E8">}</span></span>
<span data-line=""><span style="color:#E1E4E8">]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span data-line=""><span style="color:#E1E4E8">    messages,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">add_generation_prompt</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_tensors</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"pt"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">return_dict</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">).to(model.device)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.generate(</span><span style="color:#F97583">**</span><span style="color:#E1E4E8">inputs, </span><span style="color:#FFAB70">max_new_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1000</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Decode and print</span></span>
<span data-line=""><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tokenizer.decode(outputs[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">])</span></span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Model response:"</span><span style="color:#E1E4E8">, response.split(</span><span style="color:#9ECBFF">"&lt;|channel|&gt;final&lt;|message|&gt;"</span><span style="color:#E1E4E8">)[</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">].strip())</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>You can then run this on a node with four GPUs via</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">torchrun</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">--nproc_per_node=4</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">generate.py</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div></article></div></div></div></div></div></div></div><next-route-announcer style="position: absolute;"></next-route-announcer></article>