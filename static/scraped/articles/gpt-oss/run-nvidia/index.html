<!doctype html><meta charset="utf-8">
<link rel="stylesheet" href="/cookbook-assets/all.css"><title>Using NVIDIA TensorRT-LLM to run gpt-oss-20b</title><article><div class="flex flex-col items-center pt-8 pb-32 px-4 sm:px-8"><div class="w-full"><div class="w-full"><div class="mx-auto max-w-3xl px-4 sm:px-8"><div class="mb-12 mt-4"><h3 class="text-sm text-gray-500 mb-2 sm:mb-8 text-center">Aug 5, 2025</h3><h1 class="text-xl sm:text-4xl font-bold mb-12 sm:mb-16 text-center">Using NVIDIA TensorRT-LLM to run gpt-oss-20b</h1><div class="flex justify-between items-end"><div class="flex space-x-2 items-center"><div class="flex"><button class="transform transition-all cursor-pointer ml-0" style="z-index:1" rel="noopener noreferrer" aria-label="View profile of jayrodge"><div class="relative rounded-full h-8 w-8 bg-white"><div class="rounded-full h-8 w-8 bg-gray-300 flex items-center justify-center text-white text-sm  border-white border-2">JR</div></div></button></div><div class="flex flex-col justify-center"><div><span class="text-sm text-primary"><button aria-label="View profile of jayrodge" rel="noopener noreferrer">Jay Rodge</button></span></div></div></div><div class="flex flex-row gap-x-0"><a href="https://github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-nvidia.ipynb" target="_blank" rel="noopener noreferrer" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">Open in GitHub</span></a><a href="https://nbviewer.org/format/script/github/openai/openai-cookbook/blob/main/articles/gpt-oss/run-nvidia.ipynb" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center" target="_blank" rel="noopener noreferrer"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M3 2.5C3 2.22386 3.22386 2 3.5 2H9.08579C9.21839 2 9.34557 2.05268 9.43934 2.14645L11.8536 4.56066C11.9473 4.65443 12 4.78161 12 4.91421V12.5C12 12.7761 11.7761 13 11.5 13H3.5C3.22386 13 3 12.7761 3 12.5V2.5ZM3.5 1C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H11.5C12.3284 14 13 13.3284 13 12.5V4.91421C13 4.51639 12.842 4.13486 12.5607 3.85355L10.1464 1.43934C9.86514 1.15804 9.48361 1 9.08579 1H3.5ZM4.5 4C4.22386 4 4 4.22386 4 4.5C4 4.77614 4.22386 5 4.5 5H7.5C7.77614 5 8 4.77614 8 4.5C8 4.22386 7.77614 4 7.5 4H4.5ZM4.5 7C4.22386 7 4 7.22386 4 7.5C4 7.77614 4.22386 8 4.5 8H10.5C10.7761 8 11 7.77614 11 7.5C11 7.22386 10.7761 7 10.5 7H4.5ZM4.5 10C4.22386 10 4 10.2239 4 10.5C4 10.7761 4.22386 11 4.5 11H10.5C10.7761 11 11 10.7761 11 10.5C11 10.2239 10.7761 10 10.5 10H4.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">View as Markdown</span></a></div></div></div></div><div id="content" class="w-full"><div class="w-full px-6 lg:px-8"><div class="grid grid-cols-1 lg:grid-cols-[clamp(12rem,20vw,18rem)_minmax(0,1fr)] gap-14"><div class="mx-auto w-full max-w-[clamp(60ch,72vw,90ch)] px-4 sm:px-8"><div class="flex flex-col space-y-6"><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>This notebook provides a step-by-step guide on how to optimizing <code>gpt-oss</code> models using NVIDIA's TensorRT-LLM for high-performance inference. TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.</p>
<p>TensorRT-LLM supports both models:</p>
<ul>
<li><code>gpt-oss-20b</code></li>
<li><code>gpt-oss-120b</code></li>
</ul>
<p>In this guide, we will run <code>gpt-oss-20b</code>, if you want to try the larger model or want more customization refer to <a href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog9_Deploying_GPT_OSS_on_TRTLLM.md">this</a> deployment guide.</p>
<p>Note: Your input prompts should use the <a href="http://cookbook.openai.com/articles/openai-harmony">harmony response</a> format for the model to work properly, though this guide does not require it.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h4 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="launch-on-nvidia-brev"><a class="heading-link" href="#launch-on-nvidia-brev">Launch on NVIDIA Brev</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h4>
<p>You can simplify the environment setup by using <a href="https://developer.nvidia.com/brev">NVIDIA Brev</a>. Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.</p>
<p>Once deployed, click on the "Open Notebook" button to get start with this guide</p>
<p><a href="https://brev.nvidia.com/launchable/deploy?launchableID=env-30i1YjHsRWT109HL6eYxLUeHIwF"><img src="https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg" alt="Launch on Brev"></a></p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="prerequisites"><a class="heading-link" href="#prerequisites">Prerequisites</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h3 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="hardware"><a class="heading-link" href="#hardware">Hardware</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h3>
<p>To run the gpt-oss-20b model, you will need an NVIDIA GPU with at least 20 GB of VRAM.</p>
<p>Recommended GPUs: NVIDIA Hopper (e.g., H100, H200), NVIDIA Blackwell (e.g., B100, B200), NVIDIA RTX PRO, NVIDIA RTX 50 Series (e.g., RTX 5090).</p>
<h3 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="software"><a class="heading-link" href="#software">Software</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h3>
<ul>
<li>CUDA Toolkit 12.8 or later</li>
<li>Python 3.12 or later</li>
</ul></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="installing-tensorrt-llm"><a class="heading-link" href="#installing-tensorrt-llm">Installing TensorRT-LLM</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>There are multiple ways to install TensorRT-LLM. In this guide, we'll cover using a pre-built Docker container from NVIDIA NGC as well as building from source.</p>
<p>If you're using NVIDIA Brev, you can skip this section.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="using-nvidia-ngc"><a class="heading-link" href="#using-nvidia-ngc">Using NVIDIA NGC</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Pull the pre-built <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release/tags">TensorRT-LLM container</a> for GPT-OSS from <a href="https://www.nvidia.com/en-us/gpu-cloud/">NVIDIA NGC</a>.
This is the easiest way to get started and ensures all dependencies are included.</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="bash" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">docker</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">pull</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">nvcr.io/nvidia/tensorrt-llm/release:gpt-oss-dev</span></span>
<span data-line=""><span style="color:#B392F0">docker</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">run</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">--gpus</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">all</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-it</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">--rm</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-v</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">$(</span><span style="color:#79B8FF">pwd</span><span style="color:#9ECBFF">):/workspace</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">nvcr.io/nvidia/tensorrt-llm/release:gpt-oss-dev</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="using-docker-build-from-source"><a class="heading-link" href="#using-docker-build-from-source">Using Docker (Build from Source)</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Alternatively, you can build the TensorRT-LLM container from source.
This approach is useful if you want to modify the source code or use a custom branch.
For detailed instructions, see the <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/feat/gpt-oss/docker">official documentation</a>.</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>TensorRT-LLM will be available through pip soon</p></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><blockquote>
<p>Note on GPU Architecture: The first time you run the model, TensorRT-LLM will build an optimized engine for your specific GPU architecture (e.g., Hopper, Ada, or Blackwell). If you see warnings about your GPU's CUDA capability (e.g., sm_90, sm_120) not being compatible with the PyTorch installation, ensure you have the latest NVIDIA drivers and a matching CUDA Toolkit version for your version of PyTorch.</p>
</blockquote></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h1 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="verifying-tensorrt-llm-installation"><a class="heading-link" href="#verifying-tensorrt-llm-installation">Verifying TensorRT-LLM Installation</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h1></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> tensorrt_llm </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">LLM</span><span style="color: #E1E4E8">, SamplingParams</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h1 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="utilizing-tensorrt-llm-python-api"><a class="heading-link" href="#utilizing-tensorrt-llm-python-api">Utilizing TensorRT-LLM Python API</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h1></article><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>In the next code cell, we will demonstrate how to use the TensorRT-LLM Python API to:</p>
<ol>
<li>Download the specified model weights from Hugging Face (using your HF_TOKEN for authentication).</li>
<li>Automatically build the TensorRT engine for your GPU architecture if it does not already exist.</li>
<li>Load the model and prepare it for inference.</li>
<li>Run a simple text generation example to verify everything is working.</li>
</ol>
<p><strong>Note</strong>: The first run may take several minutes as it downloads the model and builds the engine.
Subsequent runs will be much faster, as the engine will be cached.</p></article><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">llm </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> LLM(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"openai/gpt-oss-20b"</span><span style="color: #E1E4E8">)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><div><div class="relative"><div class="[&amp;&gt;*]:rounded-md [&amp;&gt;*]:p-4 [&amp;&gt;*]:text-xs [&amp;&gt;*]:overflow-x-auto"><pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">prompts </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> [</span><span style="color: #9ECBFF">"Hello, my name is"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"The capital of France is"</span><span style="color: #E1E4E8">]</span></span>
<span class="line"><span style="color: #E1E4E8">sampling_params </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> SamplingParams(</span><span style="color: #FFAB70">temperature</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.8</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">top_p</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.95</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #F97583">for</span><span style="color: #E1E4E8"> output </span><span style="color: #F97583">in</span><span style="color: #E1E4E8"> llm.generate(prompts, sampling_params):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"Prompt: </span><span style="color: #79B8FF">{</span><span style="color: #E1E4E8">output.prompt</span><span style="color: #F97583">!r</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF">, Generated text: </span><span style="color: #79B8FF">{</span><span style="color: #E1E4E8">output.outputs[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">].text</span><span style="color: #F97583">!r</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF">"</span><span style="color: #E1E4E8">)</span></span></code></pre></div><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><h1 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="conclusion-and-next-steps"><a class="heading-link" href="#conclusion-and-next-steps">Conclusion and Next Steps</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h1>
<p>Congratulations! You have successfully optimized and run a large language model using the TensorRT-LLM Python API.</p>
<p>In this notebook, you have learned how to:</p>
<ul>
<li>Set up your environment with the necessary dependencies.</li>
<li>Use the <code>tensorrt_llm.LLM</code> API to download a model from the Hugging Face Hub.</li>
<li>Automatically build a high-performance TensorRT engine tailored to your GPU.</li>
<li>Run inference with the optimized model.</li>
</ul>
<p>You can explore more advanced features to further improve performance and efficiency:</p>
<ul>
<li>
<p>Benchmarking: Try running a <a href="https://nvidia.github.io/TensorRT-LLM/performance/performance-tuning-guide/benchmarking-default-performance.html#benchmarking-with-trtllm-bench">benchmark</a> to compare the latency and throughput of the TensorRT-LLM engine against the original Hugging Face model. You can do this by iterating over a larger number of prompts and measuring the execution time.</p>
</li>
<li>
<p>Quantization: TensorRT-LLM <a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer">supports</a> various quantization techniques (like INT8 or FP8) to reduce model size and accelerate inference with minimal impact on accuracy. This is a powerful feature for deploying models on resource-constrained hardware.</p>
</li>
<li>
<p>Deploy with NVIDIA Dynamo: For production environments, you can deploy your TensorRT-LLM engine using the <a href="https://docs.nvidia.com/dynamo/latest/">NVIDIA Dynamo</a> for robust, scalable, and multi-model serving.</p>
</li>
</ul></article></div></div></div></div></div></div></div></div><next-route-announcer style="position: absolute;"></next-route-announcer></article>