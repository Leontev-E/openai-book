<!doctype html><meta charset="utf-8">
<link rel="stylesheet" href="/cookbook-assets/all.css"><title>How to run gpt-oss locally with Ollama</title><article><div class="flex flex-col items-center pt-8 pb-32 px-4 sm:px-8"><div class="w-full"><div class="w-full"><div class="mx-auto max-w-3xl px-4 sm:px-8"><div class="mb-12 mt-4"><h3 class="text-sm text-gray-500 mb-2 sm:mb-8 text-center">Aug 5, 2025</h3><h1 class="text-xl sm:text-4xl font-bold mb-12 sm:mb-16 text-center">How to run gpt-oss locally with Ollama</h1><div class="flex justify-between items-end"><div class="flex space-x-2 items-center"><div class="flex"><button class="transform transition-all cursor-pointer ml-0" style="z-index:1" rel="noopener noreferrer" aria-label="View profile of dkundel-openai"><div class="relative rounded-full h-8 w-8 bg-white"><img alt="Dominik Kundel" draggable="false" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" class="rounded-full h-8 w-8 border border-gray-30" style="color:transparent" srcset="/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=32&amp;q=75 1x, /_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=64&amp;q=75 2x" src="https://cookbook.openai.com/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F200841172%3Fv%3D4&amp;w=64&amp;q=75"><div class="absolute bottom-[-2px] right-[-2px] rounded-full bg-white h-4 w-4 flex items-center justify-center"><img alt="Verified" loading="lazy" width="512" height="512" decoding="async" data-nimg="1" style="color:transparent" src="/_next/static/media/openai-logomark-2.63a8dac2.svg"></div></div></button></div><div class="flex flex-col justify-center"><div><span class="text-sm text-primary"><button aria-label="View profile of dkundel-openai" rel="noopener noreferrer">Dominik Kundel<span class="ml-1 text-xs text-muted-foreground">(OpenAI)</span></button></span></div></div></div><div class="flex flex-row gap-x-0"><a href="https://github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-locally-ollama.md" target="_blank" rel="noopener noreferrer" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors flex gap-2 items-center"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">Open in GitHub</span></a></div></div></div></div><div id="content" class="w-full"><div class="w-full px-6 lg:px-8"><div class="grid grid-cols-1 lg:grid-cols-[clamp(12rem,20vw,18rem)_minmax(0,1fr)] gap-14"><div class="mx-auto w-full max-w-[clamp(60ch,72vw,90ch)] px-4 sm:px-8"><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>Want to get <a href="https://openai.com/open-models"><strong>OpenAI gpt-oss</strong></a> running on your own hardware? This guide will walk you through how to use <a href="https://ollama.ai">Ollama</a> to set up <strong>gpt-oss-20b</strong> or <strong>gpt-oss-120b</strong> locally, to chat with it offline, use it through an API, and even connect it to the Agents SDK.</p>
<p>Note that this guide is meant for consumer hardware, like running a model on a PC or Mac. For server applications with dedicated GPUs like NVIDIA’s H100s, <a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm">check out our vLLM guide</a>.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="pick-your-model"><a class="heading-link" href="#pick-your-model">Pick your model</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Ollama supports both model sizes of gpt-oss:</p>
<ul>
<li><strong><code>gpt-oss-20b</code></strong>
<ul>
<li>The smaller model</li>
<li>Best with <strong>≥16GB VRAM</strong> or <strong>unified memory</strong></li>
<li>Perfect for higher-end consumer GPUs or Apple Silicon Macs</li>
</ul>
</li>
<li><strong><code>gpt-oss-120b</code></strong>
<ul>
<li>Our larger full-sized model</li>
<li>Best with <strong>≥60GB VRAM</strong> or <strong>unified memory</strong></li>
<li>Ideal for multi-GPU or beefy workstation setup</li>
</ul>
</li>
</ul>
<p><strong>A couple of notes:</strong></p>
<ul>
<li>These models ship <strong>MXFP4 quantized</strong> out the box and there is currently no other quantization</li>
<li>You <em>can</em> offload to CPU if you’re short on VRAM, but expect it to run slower.</li>
</ul>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="quick-setup"><a class="heading-link" href="#quick-setup">Quick setup</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<ol>
<li><strong>Install Ollama</strong> → <a href="https://ollama.com/download">Get it here</a></li>
<li><strong>Pull the model you want:</strong></li>
</ol>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="shell" data-theme="default" style="display:grid"><span data-line=""><span style="color:#6A737D"># For 20B</span></span>
<span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">pull</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt-oss:20b</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># For 120B</span></span>
<span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">pull</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt-oss:120b</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="chat-with-gpt-oss"><a class="heading-link" href="#chat-with-gpt-oss">Chat with gpt-oss</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Ready to talk to the model? You can fire up a chat in the app or the terminal:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="shell" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">run</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt-oss:20b</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Ollama applies a <strong>chat template</strong> out of the box that mimics the <a href="https://cookbook.openai.com/articles/openai-harmony">OpenAI harmony format</a>. Type your message and start the conversation.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="use-the-api"><a class="heading-link" href="#use-the-api">Use the API</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Ollama exposes a <strong>Chat Completions-compatible API</strong>, so you can use the OpenAI SDK without changing much. Here’s a Python example:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> openai </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OpenAI</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">client </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OpenAI(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">base_url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:11434/v1"</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Local Ollama API</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">api_key</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"ollama"</span><span style="color:#E1E4E8">                       </span><span style="color:#6A737D"># Dummy key</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> client.chat.completions.create(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"gpt-oss:20b"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span></span>
<span data-line=""><span style="color:#E1E4E8">        {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"system"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"You are a helpful assistant."</span><span style="color:#E1E4E8">},</span></span>
<span data-line=""><span style="color:#E1E4E8">        {</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Explain what MXFP4 quantization is."</span><span style="color:#E1E4E8">}</span></span>
<span data-line=""><span style="color:#E1E4E8">    ]</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response.choices[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">].message.content)</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>If you’ve used the OpenAI SDK before, this will feel instantly familiar.</p>
<p>Alternatively, you can use the Ollama SDKs in <a href="https://github.com/ollama/ollama-python">Python</a> or <a href="https://github.com/ollama/ollama-js">JavaScript</a> directly.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="using-tools-function-calling"><a class="heading-link" href="#using-tools-function-calling">Using tools (function calling)</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Ollama can:</p>
<ul>
<li>Call functions</li>
<li>Use a <strong>built-in browser tool</strong> (in the app)</li>
</ul>
<p>Example of invoking a function via Chat Completions:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#E1E4E8">tools </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span data-line=""><span style="color:#E1E4E8">    {</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#9ECBFF">"type"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"function"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#9ECBFF">"function"</span><span style="color:#E1E4E8">: {</span></span>
<span data-line=""><span style="color:#E1E4E8">            </span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"get_weather"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">            </span><span style="color:#9ECBFF">"description"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Get current weather in a given city"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">            </span><span style="color:#9ECBFF">"parameters"</span><span style="color:#E1E4E8">: {</span></span>
<span data-line=""><span style="color:#E1E4E8">                </span><span style="color:#9ECBFF">"type"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"object"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">                </span><span style="color:#9ECBFF">"properties"</span><span style="color:#E1E4E8">: {</span><span style="color:#9ECBFF">"city"</span><span style="color:#E1E4E8">: {</span><span style="color:#9ECBFF">"type"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"string"</span><span style="color:#E1E4E8">}},</span></span>
<span data-line=""><span style="color:#E1E4E8">                </span><span style="color:#9ECBFF">"required"</span><span style="color:#E1E4E8">: [</span><span style="color:#9ECBFF">"city"</span><span style="color:#E1E4E8">]</span></span>
<span data-line=""><span style="color:#E1E4E8">            },</span></span>
<span data-line=""><span style="color:#E1E4E8">        },</span></span>
<span data-line=""><span style="color:#E1E4E8">    }</span></span>
<span data-line=""><span style="color:#E1E4E8">]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> client.chat.completions.create(</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"gpt-oss:20b"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"What's the weather in Berlin right now?"</span><span style="color:#E1E4E8">}],</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#FFAB70">tools</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tools</span></span>
<span data-line=""><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response.choices[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">].message)</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Since the models can perform tool calling as part of the chain-of-thought (CoT) it’s important for you to return the reasoning returned by the API back into a subsequent call to a tool call where you provide the answer until the model reaches a final answer.</p>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="responses-api-workarounds"><a class="heading-link" href="#responses-api-workarounds">Responses API workarounds</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Ollama doesn’t (yet) support the <strong>Responses API</strong> natively.</p>
<p>If you do want to use the Responses API you can use <a href="https://github.com/huggingface/responses.js"><strong>Hugging Face’s <code>Responses.js</code> proxy</strong></a> to convert Chat Completions to Responses API.</p>
<p>For basic use cases you can also <a href="https://github.com/openai/gpt-oss?tab=readme-ov-file#responses-api"><strong>run our example Python server with Ollama as the backend.</strong></a> This server is a basic example server and does not have the</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="shell" data-theme="default" style="display:grid"><span data-line=""><span style="color:#B392F0">pip</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">install</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt-oss</span></span>
<span data-line=""><span style="color:#B392F0">python</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">-m</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt_oss.responses_api.serve</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">\</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#79B8FF">--inference_backend=ollama</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">\</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#79B8FF">--checkpoint</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">gpt-oss:20b</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 class="
        group 
        relative 
        cursor-pointer
        scroll-mt-24 
      " id="agents-sdk-integration"><a class="heading-link" href="#agents-sdk-integration">Agents SDK integration</a><button class=" absolute  top-1/2  -translate-y-1/2 opacity-0 group-hover:opacity-100 transition-opacity duration-300 ease-in-out text-[var(--oai-green)] hover:text-[var(--oai-green-hover)] p-1 pl-3 pointer-events-none group-hover:pointer-events-auto " aria-label="Copy link to clipboard"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div></h2>
<p>Want to use gpt-oss with OpenAI’s <strong>Agents SDK</strong>?</p>
<p>Both Agents SDK enable you to override the OpenAI base client to point to Ollama using Chat Completions or your Responses.js proxy for your local models. Alternatively, you can use the built-in functionality to point the Agents SDK against third party models.</p>
<ul>
<li><strong>Python:</strong> Use <a href="https://openai.github.io/openai-agents-python/models/litellm/">LiteLLM</a> to proxy to Ollama through LiteLLM</li>
<li><strong>TypeScript:</strong> Use <a href="https://openai.github.io/openai-agents-js/extensions/ai-sdk/">AI SDK</a> with the <a href="https://ai-sdk.dev/providers/community-providers/ollama">ollama adapter</a></li>
</ul>
<p>Here’s a Python Agents SDK example using LiteLLM:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="py" data-theme="default" style="display:grid"><span data-line=""><span style="color:#F97583">import</span><span style="color:#E1E4E8"> asyncio</span></span>
<span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> agents </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> Agent, Runner, function_tool, set_tracing_disabled</span></span>
<span data-line=""><span style="color:#F97583">from</span><span style="color:#E1E4E8"> agents.extensions.models.litellm_model </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> LitellmModel</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">set_tracing_disabled(</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#B392F0">@function_tool</span></span>
<span data-line=""><span style="color:#F97583">def</span><span style="color:#E1E4E8"> </span><span style="color:#B392F0">get_weather</span><span style="color:#E1E4E8">(city: </span><span style="color:#79B8FF">str</span><span style="color:#E1E4E8">):</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"[debug] getting weather for </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">city</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#F97583">return</span><span style="color:#E1E4E8"> </span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"The weather in </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">city</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> is sunny."</span></span>
<span data-line=""> </span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">async</span><span style="color:#E1E4E8"> </span><span style="color:#F97583">def</span><span style="color:#E1E4E8"> </span><span style="color:#B392F0">main</span><span style="color:#E1E4E8">(model: </span><span style="color:#79B8FF">str</span><span style="color:#E1E4E8">, api_key: </span><span style="color:#79B8FF">str</span><span style="color:#E1E4E8">):</span></span>
<span data-line=""><span style="color:#E1E4E8">    agent </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Agent(</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"Assistant"</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#FFAB70">instructions</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"You only respond in haikus."</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">LitellmModel(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"ollama/gpt-oss:120b"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">api_key</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">api_key),</span></span>
<span data-line=""><span style="color:#E1E4E8">        </span><span style="color:#FFAB70">tools</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[get_weather],</span></span>
<span data-line=""><span style="color:#E1E4E8">    )</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E1E4E8">    result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> </span><span style="color:#F97583">await</span><span style="color:#E1E4E8"> Runner.run(agent, </span><span style="color:#9ECBFF">"What's the weather in Tokyo?"</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">    </span><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(result.final_output)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">if</span><span style="color:#E1E4E8"> </span><span style="color:#79B8FF">__name__</span><span style="color:#E1E4E8"> </span><span style="color:#F97583">==</span><span style="color:#E1E4E8"> </span><span style="color:#9ECBFF">"__main__"</span><span style="color:#E1E4E8">:</span></span>
<span data-line=""><span style="color:#E1E4E8">    asyncio.run(main())</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div></article></div></div></div></div></div></div></div><next-route-announcer style="position: absolute;"></next-route-announcer></article>