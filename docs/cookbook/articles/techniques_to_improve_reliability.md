---
lang: ru
translationOf: openai-cookbook
---

# Техники для повышения надежности

Когда GPT-3 не справляется с задачей, что делать?

- Искать лучший запрос (prompt), который даст более надежные ответы?
- Инвестировать в тысячи примеров для дообучения кастомной модели?
- Предположить, что модель не способна решить задачу, и перейти к следующей?

Простого ответа нет — всё зависит от ситуации. Однако если ваша задача связана с логическим рассуждением или сложностью, стоит попробовать техники из этой статьи, чтобы создавать более надежные и эффективные запросы.

## Почему GPT-3 ошибается в сложных задачах

Если вам нужно умножить 13 на 17, сразу ли ответ всплывёт у вас в голове? Для большинства — нет. Но это не значит, что люди не умеют умножать двузначные числа. Несколько секунд и лист бумаги — и можно посчитать, что 13 × 17 = 130 + 70 + 21 = 221.

Точно так же, если дать GPT-3 слишком сложную задачу, которую надо решить за время вычисления следующего токена, модель может выдумать неверный ответ. Однако, как и у человека, это не значит, что модель не способна решить задачу. При наличии времени и пространства для рассуждений она вполне может ответить верно.

Например, если задать `gpt-3.5-turbo-instruct` следующую задачу по математике, связанную с жонглированием шарами, он ответит неправильно:

&lt;&lt;&lt;CODE_0&gt;>>

&lt;&lt;&lt;CODE_1&gt;>>

Означает ли это, что GPT-3 не умеет решать простые задачи по математике? Нет. На самом деле, при запросе `Let's think step by step` модель надежно решает задачу:

&lt;&lt;&lt;CODE_2&gt;>>

&lt;&lt;&lt;CODE_3&gt;>>

Конечно, по одному примеру сложно судить, работает ли трюк `Let's think step by step` в целом или просто повезло. Но он действительно работает. На наборе задач по словесной математике трюк повысил процент решения с никуда не годных 18% до приличных 79%!

## Возможности модели зависят от контекста

Распространённая ошибка при работе с GPT-3 — считать, что возможности модели однородны во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит, она не умеет решать простую логику.

Однако на примере `Let's think step by step` видно, что видимые ошибки GPT-3 иногда можно исправить, задав более удачный запрос, который направит модель к правильному ответу.

## Как повысить надежность на сложных задачах

Дальше в статье представлены техники для повышения надежности больших языковых моделей в сложных задачах. Хотя часть техник специфична для определённых проблем, многие основаны на общих принципах, применимых к широкому спектру задач, например:

- Давать более чёткие инструкции
- Делить сложные задачи на простые подзадачи
- Структурировать инструкцию, чтобы удержать модель на задании
- Подсказывать модели объяснять решение перед ответом
- Запрашивать обоснования для разных вариантов и синтезировать их
- Генерировать множество ответов и выбирать лучший с помощью модели
- Дообучать кастомные модели для максимального качества

## Делите сложные задачи на простые

Один из способов дать модели больше времени и пространства для размышления — разбить задачу на более простые части.

Например, возьмём задачу с вопросом множественного выбора по тексту — игру Clue. При прямом вопросе `gpt-3.5-turbo-instruct` не может объединить подсказки 3 и 5 и ошибается:

&lt;&lt;&lt;CODE_4&gt;>>

&lt;&lt;&lt;CODE_5&gt;>>

Хотя из подсказок 3 и 5 следует, что полковник Горчица был единственным в обсерватории, а этот человек держал подсвечник, модель не объединяет эти факты в правильный ответ (a) Да.

Вместо прямого вопроса можно разбить задачу на три части:

- Сначала последовательно проанализировать каждую подсказку на предмет её возможной релевантности
- Потом объединить релевантные подсказки для рассуждений и поиска ответа
- Наконец, написать финальный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6&gt;>>

&lt;&lt;&lt;CODE_7&gt;>>

Давая модели больше времени и план рассуждения, она находит правильный ответ (a) Да.

Плюс деление сложных инструкций на подзадачи помогает удерживать модель сфокусированной на текущей подзадаче.

Например, если попросить `gpt-3.5-turbo-instruct` резюмировать текст на его оригинальном языке, модель может перейти на английский:

&lt;&lt;&lt;CODE_8&gt;>>

&lt;&lt;&lt;CODE_9&gt;>>

Если же сначала попросить модель определить язык текста, а потом сделать резюме, результат становится более надежным:

&lt;&lt;&lt;CODE_10&gt;>>

&lt;&lt;&lt;CODE_11&gt;>>

## Побуждайте модель объяснять перед ответом

Ещё одна мощная техника для повышения надежности — попросить модель последовательно рассуждать, а не сразу выдавать ответ. «Проговаривание вслух» помогает модели прийти к правильному выводу.

### Zero-shot

#### Метод

В 2022 году [Такеши Кодзима и соавторы](https://arxiv.org/abs/2205.11916) опубликовали простой способ заставить модель рассуждать — начинайте ответы с `Let's think step by step.` На рисунке 2 показан пример:

[![пример рассуждения zero-shot](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ от Takeshi Kojima и соавт. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применение трюка `Let's think step by step` к датасету MultiArith увеличило точность в 4 раза, с 18% до 79%!

[![пример рассуждения zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ от Takeshi Kojima и соавт. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк хорошо работает с математическими задачами, он не эффективен во всех случаях. Авторы отметили, что он полезен для многозадачных арифметических, символических, стратегических и других задач рассуждения. Не помогает с простыми математическими задачами или вопросами здравого смысла и, вероятно, малоэффективен для многих нерефлексивных задач.

[![пример рассуждения zero-shot](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ от Takeshi Kojima и соавт. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее в [полной статье](https://arxiv.org/abs/2205.11916).

При использовании этой техники в своих задачах не бойтесь экспериментировать с формулировками. `Let's think step by step` — общий вариант, но можно добиться лучшего результата, используя более строгие инструкции для вашего случая. Например, можно попробовать более структурированные варианты: `First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.` Или даже дать модели пример формата, чтобы помочь ей не сбиться:

&lt;&lt;&lt;CODE_12&gt;>>

&lt;&lt;&lt;CODE_13&gt;>>

### Few-shot примеры

#### Метод

Побуждение модели рассуждать можно делать разными способами. Один из них — показать несколько примеров («few-shot»), как изучали [Jason Wei и Denny Zhou с Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример prompt с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё примеры цепочек рассуждений от человеческих аннотаторов:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Заметим, что на плавающие груши уже указали)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

На школьных математических задачах этот подход утроил процент решения с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочки рассуждений улучшили результаты по таким задачам, как понимание спорта, отслеживание орла/решки при бросании монеты и объединение последних букв. Обычно для насыщения прироста нужно было не более 8 примеров.

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество примеров few-shot по сравнению с `Let's think step by step` в том, что вы легче можете задать формат, длину и стиль рассуждений, которые хотите от модели перед финальным ответом. Это полезно, если модель изначально рассуждает неверно или поверхностно.

### Fine-tuned (дообучение)

#### Метод

Для достижения наилучших результатов обычно требуется дообучать отдельную модель. Однако дообучение на объяснения может потребовать тысячи примеров, которые дорого создавать вручную.

В 2022 году Эрик Зеликман и Юхуа Ву и соавторы опубликовали умный метод создания датасета с объяснениями с помощью few-shot prompt. Идея: сначала генерировать варианты объяснений, оставляя только те, которые приводят к правильному ответу. Затем для неполных ответов повторять генерацию, но с правильным ответом в вопросе. Этот метод назвали STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ — Eric Zelikman и Yujuai Wu и др. (2022)](https://arxiv.org/abs/2203.14465)

Так можно объединить преимущества fine-tuning и chain-of-thought, не создавая тысячи примеров.

#### Результаты

На датасете Common Sense Q&A STaR превзошёл как chain-of-thought (73% > 37%), так и простое fine-tuning (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ — Eric Zelikman и Yujuai Wu и др. (2022)](https://arxiv.org/abs/2203.14465)

Подробнее в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot prompt для создания или модификации датасета для fine-tuning можно обобщить и на другие сценарии. Например, если у вас много неструктурированного текста, можно с помощью prompt вытащить из него структурированный набор данных, на котором потом дообучить модель.

## Расширения цепочек рассуждений

Опубликовано несколько расширений chain-of-thought prompt.

### Selection-inference prompting

#### Метод

Антония Кресвелл и соавторы предложили разделить один prompt для объяснений и ответов на части. Сначала prompt выбирает релевантные факты из текста («selection prompt»). Потом второй prompt делает вывод на основе этих фактов («inference prompt»). Эти шаги чередуются в цикле, генерируя серию рассуждений до финального ответа. Идея на рисунке:

[![selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На моделях с 7 миллиардами параметров selection-inference заметно повысил качество по задачам bAbi и Proof Writer, требующим длинных последовательностей рассуждений. Лучший результат получали, совмещая selection-inference и fine-tuning.

[![selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя улучшения были значительными для этих задач, они были выбраны из-за требуемых длинных последовательностей рассуждений. Для задач с менее сложными рассуждениями приросты могут быть меньше.

Основные уроки:
- Делите задачи на более простые подзадачи для повышения надежности.
- Для максимальных результатов часто нужна дообученная модель вместе с выбранным подходом.

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Архитектура достоверных рассуждений

Через несколько месяцев после публикации selection-inference авторы расширили методику в новой статье, добавив:

- способы определять, когда нужно останавливать или продолжать цикл selection-inference
- функцию ценности для поиска среди разных траекторий рассуждений
- уменьшение галлюцинаций фиктивных фактов с помощью дообучения модели работать с метками предложений (например, sen1), а не с самим текстом

#### Метод

В исходной технике selection-inference чередуются специальные promt’ы для выбора фактов и вывода из них, создавая цепочку рассуждений.

Авторы добавили две компоненты.

Во-первых, модель 'halter' после каждого шага inference оценивает, достаточно ли сделанных выводов для ответа. Если да — генерируется финальный ответ.

Преимущества halter:

- может остановить или продолжить процесс reasoning по необходимости
- если процесс не останавливается, ответ не выдается — что лучше, чем галлюцинация

[![достоверные рассуждения](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

[![достоверные рассуждения](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавлена функция ценности, оценивающая качество рассуждений и позволяющая искать лучшую траекторию среди множества. Это отражает идею повышения надежности: генерировать не один ответ, а много, затем выбирать лучший с помощью функции ценности или верификатора.

[![достоверные рассуждения](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Для уменьшения галлюцинаций модель дообучают работать с метками предложений, а не с самим текстом. Это уменьшает появление несуществующих фактов, отсутствующих в контексте.

[![достоверные рассуждения](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Оценка проводилась на ProofWriter (не показано) и EntailmentBankQA (показано). Эта методика значительно повысила точность, особенно на сложных задачах рассуждений.

![достоверные рассуждения](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, трюк с метками предложений практически полностью устранил галлюцинации!

![достоверные рассуждения](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

В статье показаны несколько полезных уроков по повышению надежности:

- Делите сложные задачи на более надежные подзадачи
- Получайте ответ по шагам, оценивая результат по мере движения вперёд
- Генерируйте множество вариантов и выбирайте лучший с помощью отдельной модели или функции
- Снижайте галлюцинации, ограничивая, что модель может говорить (например, работы с метками вместо текста)
- Максимизируйте качество дообучением на специализированных задачах

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Кроме того, chain-of-thought плохо работает с длинными цепочками рассуждений (где shine selection-inference), а особенно на коротких примерах с длинной задачей.

#### Метод

Least-to-most prompting — ещё один приём деления задач на подзадачи. Идея: вызвать подзадачу у модели запросом, например, `To solve {question}, we need to first solve: "`. Получив подзадачу, модель решает её, добавляет решение к исходному вопросу и повторяет, пока не получит финальный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ — Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

Применяя на задачах с длинными цепочками рассуждений и использовании `code-davinci-002` (оптимизированной для кода, но понимающей тексты), получили рост с 16% до 99.7%!

[
![Результаты least-to-most](/images/least-to-most_tab4.png)
![Результаты least-to-most на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most на DROP](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ — Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя приросты впечатляют, они актуальны для узких задач с длинными цепочками рассуждений.

Они подчеркивают общий принцип: повысить надежность, (а) разбивая задачи на подзадачи и (б) давая больше времени и пространства для решения.

Подробнее в [полной статье](https://arxiv.org/abs/2205.10625).

## Похожие идеи

### Maieutic prompting

#### Метод

В отличие от предыдущих подходов, которые максимизируют вероятность правильных ответов, другой метод — использовать GPT-3, чтобы построить дерево возможных объяснений (и правильных, и ошибочных), а затем анализировать их взаимосвязи, чтобы определить верный набор. Эту технику назвали maieutic prompting [Джехун Чжун и др., май 2022](https://arxiv.org/abs/2205.11822) (maieutic — от сократического метода задавания вопросов).

Метод сложен, и работает так:

- Сначала строится maieutic-дерево, где каждая вершина — утверждение, которое может быть истинным или ложным:
  - Начинается с МВО вопроса или утверждения «истина/ложь» (например, `War cannot have a tie`)
  - Для каждого варианта ответа модель генерирует объяснение с командой типа `War cannot have a tie? True, because`
  - Потом спрашивают модель с вопросом и объяснением, какой ответ она даст. Если при изменении объяснения (с префиксом `It is wrong to say that {explanation}`) ответ меняется на противоположный, объяснение считается «логически интегральным»
  - Если объяснение не интегрально, процесс повторяется рекурсивно, превращая объяснение в новое вопрос «истина/ложь» и генерируя новые объяснения
  - В итоге получается дерево объяснений, где каждый лист инвертирует ответ при инверсии объяснения
- Затем дерево переводится в граф отношений:
  - Для каждого узла вычисляется относительная уверенность модели в истинности (исходя из вероятности True при данном объяснении)
  - Для каждой пары узлов определяют, являются ли они взаимоисключающими или имплицированными
- В конце ищется наиболее согласованный набор убеждений и принимается за истину:
  - С использованием силы веры в каждый узел и логических отношений формулируется задача максимальной взвешенной выполнимости (MAX-SAT)
  - Решатель находит максимально самосогласованный набор убеждений, который и принимается

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ — Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![результаты Maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ — Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Несмотря на сложность, ограничение этого метода в том, что он применим только к вопросам с вариантами выбора.

Подробнее в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с ограниченным множеством ответов простой способ повысить надежность — получить несколько различных объяснений и ответов (с положительной температурой), а затем выбрать самый часто встречающийся финальный ответ.

[![метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ — Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Эта методика прибавила от 1 до 24 процентных пунктов на разных матем. и логических тестах. Ниже показаны результаты по модели Google's LaMDA; у модели Google PaLM исходно было выше, но прирост был меньше.

[![результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ — Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя метод прост, он дорог — 10 ответов повысит стоимость в 10 раз.

Также, как и многие техники здесь, применим только к задачам с ограниченным числом ответов. Для открытых задач (напр., сочинение стихов) неясно, как выбирать самый частый ответ.

Наконец, метод особенно полезен, если возможно несколько путей к ответу. Если путь один, то пользы может не быть. Крайний пример: если задача — сгенерировать один токен, взятие самого частого токена из 100 генераций не отличается от взятия токена с максимальной вероятностью (температура=0).

### Verifiers (верификаторы)

Ещё один способ повысить качество — обучить верификатор для оценки ответов генеративной модели. Если ответ отвергнут, сгенерировать другой. Часто оценить ответ проще, чем создать, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи OpenAI применили это к школьным задачам по математике, используя следующий алгоритм:

- Сначала дообучают модель на вопросах и решениях
- Для каждой задачи на тренинге генерируют 100 решений
- Каждое решение автоматически помечено как верное или нет по окончательному ответу
- По этим примерам doфинтюнят верификатор, классифицирующий решение как верное или нет
- На тесте модель генерирует 100 решений, а верификатор выбирает наилучшее

[![метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ — Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С 175B GPT-3 и 8000 примерами этот метод поднял точность школьной математики с ~33% до ~55%.

[![результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ — Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и self-consistency, метод дорогой — 100 решений увеличит затраты примерно в 100 раз.

## Теории надежности

Хотя описанные техники разнятся, все они направлены на повышение надежности в сложных задачах. В основном это достигается:

- разбиением ненадежных операций на более мелкие и надежные (например, selection-inference prompting)
- использованием нескольких шагов или взаимосвязей, чтобы сделать систему надежнее, чем любой компонент в отдельности (например, maieutic prompting)

### Вероятностные графовые модели

Такой подход к построению надежной системы из менее надежных компонентов напоминает вероятностное программирование, и многие методы из той области можно применять и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и соавторы рассматривают вышеописанные техники как вероятностные графовые модели:

#### Chain of thought prompting

[![графовая модель chain of thought](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ — David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Дообученный chain of thought / Self-taught reasoner

[![графовая модель fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ — David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ — David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Verifiers

[![графовая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ — David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя представление этих техник как графовых моделей может не сразу помочь решить конкретную проблему, рамки могут помочь в выборе, комбинировании и открытии новых методов.

## Итоговые мысли

Исследования больших языковых моделей активно развиваются и быстро меняются. Исследователи не только совершенствуют сами модели, но и лучше понимают, как их эффективно использовать. Чтобы показать темп: все статьи, использованные здесь, были опубликованы за последние 12 месяцев (на момент написания сентября 2022).

В дальнейшем ожидайте выход новых моделей и техник. Даже если конкретные методы из этой статьи устареют, общие принципы останутся частью арсенала опытного пользователя.

## Библиография

| Урок                                                                                                                            | Статья                                                                                                                                    | Дата     |
| ------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Делите сложные задачи на подзадачи (и показывайте промежуточные результаты пользователям)                                       | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт 2021 |
| Улучшайте выходы, генерируя много вариантов и выбирая лучший                                                                    | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Окт 2021 |
| В задачах рассуждений модели эффективнее, если рассуждают по шагам перед ответом                                               | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Янв 2022 |
| Можно улучшить работу по цепочке рассуждений, генерируя много объяснений и выбирая самый частый ответ                           | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Мар 2022 |
| Для дообучения шагового рассуждателя можно использовать только данные с вопросами и ответами с вариантами выбора               | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Мар 2022 |
| Метод рассуждений по шагам хорошо работает даже без примеров                                                                   | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Можно делать лучше, чередуя промты «selection» и «inference» для рассуждений                                                    | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных задач с рассуждениями улучшает работу решение задачи по частям                                                     | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022 |
| Можно анализировать и хорошие, и плохие объяснения, чтобы найти наиболее согласованный набор                                    | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Можно рассматривать эти техники как вероятностное программирование с ненадёжными компонентами                                   | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022 |
| Галлюцинации устраняются с помощью работы с метками предложений, а неверные ответы — с помощью «halter»                        | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Авг 2022 |