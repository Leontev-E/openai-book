---
lang: ru
translationOf: openai-cookbook
---

# Методы повышения надежности

Что делать, когда GPT-3 не справляется с задачей?

- Искать лучший промпт, который даст более надежные ответы?
- Вкладывать тысячи примеров для дообучения кастомной модели?
- Предположить, что модель не способна решить эту задачу, и двигаться дальше?

Простого ответа нет — это зависит от ситуации. Однако, если ваша задача требует логического рассуждения или обладает сложностью, рассмотрите применение техник из этой статьи для создания более надежных и эффективных промптов.

## Почему GPT-3 ошибается в сложных задачах

Если вас попросить умножить 13 на 17, сразу ли вы получите ответ в голове? Для большинства из нас — скорее нет. Но это не значит, что люди неспособны выполнять умножение двузначных чисел. Потратив несколько секунд и используя ручку с бумагой, можно легко посчитать, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 дать задачу слишком сложную для решения за время вычисления следующего токена, модель может придумать неправильный ответ. Однако, как и люди, это не означает, что модель не способна решить задачу. При наличии времени и пространства для рассуждения модель все еще может дать надежный ответ.

Например, если спросить `gpt-3.5-turbo-instruct` простую задачу по математике о жонглировании мячами, он ответит неверно:

&lt;&lt;&lt;FENCE_0>>>

&lt;&lt;&lt;FENCE_1>>>

Означает ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если задать моделью `Let's think step by step` правильный запрос, она решит задачу надежно:

&lt;&lt;&lt;FENCE_2>>>

&lt;&lt;&lt;FENCE_3>>>

Конечно, трудно судить по одному примеру, работает ли трюк `Let's think step by step` в общем случае или просто повезло на этой задаче. Но он действительно эффективен. На бенчмарке словесных математических задач трюк `Let's think step by step` значительно повысил точность GPT-3, с 18% (практически бесполезный) до 79% — очень хороший результат!

## Возможности модели зависят от контекста

При работе с GPT-3 распространена ошибка считать, что возможности модели фиксированы во всех контекстах. Например, если GPT-3 даёт неверный ответ на простой логический вопрос, значит, она неспособна решать простую логику.

Однако, как показывает пример `Let's think step by step`, кажущиеся ошибки GPT-3 иногда можно исправить улучшенным промптом, который помогает модели направить себя к правильному ответу.

## Как повысить надежность при решении сложных задач

Далее в статье описаны техники повышения надежности больших языковых моделей при работе со сложными задачами. Хотя некоторые методы ориентированы на конкретные типы задач, многие основаны на общих принципах, применимых к широкому спектру задач, например:

- Четко формулируйте инструкции
- Делите сложные задачи на более простые подзадачи
- Структурируйте инструкции, чтобы удерживать модель на цели
- Заставляйте модель объяснять ответ перед его выдачей
- Запрашивайте оправдания для множества возможных ответов и затем синтезируйте итог
- Генерируйте много вариантов и выбирайте лучший с помощью модели
- Дообучайте кастомные модели для максимальной производительности

## Деление сложных задач на более простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на более простые части.

Например, возьмём задачу, в которой нужно ответить на вопрос с вариантами (multiple choice) по тексту — в данном случае по игре Clue. При прямом вопросе `gpt-3.5-turbo-instruct` не может связать улики 3 и 5 вместе и отвечает неправильно:

&lt;&lt;&lt;FENCE_4>>>

&lt;&lt;&lt;FENCE_5>>>

Хотя улики 3 и 5 указывают, что в обсерватории был только полковник Горчица, а у того, кто был в обсерватории, был подсвечник, модель не объединяет эти данные в верный ответ (a) Да.

Вместо того чтобы спрашивать ответ напрямую, задачу можно разделить на три этапа:

- Сначала пройтись по уликам по порядку и оценить их релевантность
- Затем объединить релевантные улики для анализа и получения ответа на вопрос
- В конце написать итоговый ответ: (a), (b) или (c)

&lt;&lt;&lt;FENCE_6>>>

&lt;&lt;&lt;FENCE_7>>>

Дав модели больше времени и руководя её по плану рассуждений, она может найти правильный ответ (a) Да.

Кроме того, деление сложных инструкций на подзадачи помогает удерживать внимание модели на каждой части.

Например, если попросить `gpt-3.5-turbo-instruct` сделать реферат текста на исходном языке, она может перейти на английский:

&lt;&lt;&lt;FENCE_8>>>

&lt;&lt;&lt;FENCE_9>>>

Но если сначала попросить модель определить язык текста, а затем его резюмировать, надёжность возрастет:

&lt;&lt;&lt;FENCE_10>>>

&lt;&lt;&lt;FENCE_11>>>

## Подталкивайте модель объяснять перед ответом

Другой эффективный способ повысить надежность — попросить модель последовательно рассуждать перед выдачей окончательного ответа. «Говоря вслух» она гораздо чаще приходит к правильному выводу.

### Zero-shot

#### Метод

Опубликованный в 2022 году [Takeshi Kojima и соавт.](https://arxiv.org/abs/2205.11916), самый простой способ заставить модель рассуждать — просто добавить префикс `Let's think step by step.` перед ответом. На рисунке 2 показан пример:

[![пример zero-shot рассуждения](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применение этого трюка к набору задач MultiArith дало авторам увеличение точности в четыре раза — с 18% до 79%!

[![zero-shot рассуждения](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк `Let's think step by step` отлично работает на математических задачах, он не эффективен для всех типов задач. Авторы выяснили, что он особенно полезен для многошагового арифметического рассуждения, символических задач, стратегических и других задач с рассуждениями. Он не помогал с простыми задачами по математике или со здравым смыслом, и, вероятно, не подойдет многим нересонансным задачам.

[![zero-shot рассуждения](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.11916).

При использовании этого подхода для собственных задач не бойтесь экспериментировать с форматом инструкции. `Let's think step by step` — довольно общий, поэтому вы можете получить лучшие результаты с более строгими инструкциями, адаптированными под ваш случай. Например, можно попробовать более структурированные варианты, такие как `First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.`. Можно также подать пример формата, чтобы помогать модели не сбиваться, например:

&lt;&lt;&lt;FENCE_12>>>

&lt;&lt;&lt;FENCE_13>>>

### Few-shot примеры

#### Метод

Заставлять модель рассуждать можно по-разному. Один из способов — показать несколько примеров (few-shot), как исследовали [Jason Wei и Denny Zhou с Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022).](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Дополнительные цепочки рассуждений, написанные библиотекарями-специалистами:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022).](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание, что было высказано сомнение, плавают ли действительно груши)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При проверке на школьных задачах по математике, авторы зафиксировали троекратный рост успеха — с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022).](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математических задач, цепочки рассуждений улучшили результаты по вопросам спорта, отслеживанию подбрасывания монеты, конкатенации последних букв. Обычно не требовалось много примеров (меньше восьми), чтобы заморозить улучшения.

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022).](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Для подробностей прочитайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество approach few-shot относительно `Let's think step by step` в том, что вы можете точнее указать формат, длину и стиль рассуждений, которые хотите получить, прежде чем модель вынесет финальный ответ. Это полезно, если изначально модель рассуждает неверно или слишком поверхностно.

### Дообучение

#### Метод

В общем случае, чтобы добиться максимальной производительности, нужно дообучать кастомную модель. Но для дообучения на объяснениях потребуется тысячи аннотированных примеров, что дорого.

В 2022 году Eric Zelikman, Yuhuai Wu и др. предложили смелую стратегию — с помощью few-shot промпта сгенерировать датасет объяснений, подходящий для дообучения. Идея — использовать few-shot для генерации кандидатов объяснений и отбирать только те, что приводят к правильному ответу. Чтобы получить объяснения для неправильных ответов, повторять few-shot промт, при этом указывая правильные ответы как часть вопроса. Авторы назвали эту стратегию STaR (Self-taught Reasoner):

[![Процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Eric Zelikman, Yuhuai Wu и др. (2022).](https://arxiv.org/abs/2203.14465)

Эта техника объединяет плюсы дообучения и цепочек рассуждений, не требуя тысяч разметок.

#### Результаты

На датасете вопросов по здравому смыслу STaR превзошла цепочки рассуждений (73% > 37%) и просто дообучение (73% > 60%):

[![Результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Eric Zelikman, Yuhuai Wu и др., 2022.](https://arxiv.org/abs/2203.14465)

Подробнее — в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot промпта для расширения или изменения датасета дообучения — идея, применимая не только к объяснениям. Например, если у вас есть большие объемы неструктурированного текста, можно через промпт извлечь из них структурированный датасет, а затем дообучить модель на нем.

## Расширения цепочек рассуждений

Опубликовано несколько расширений chain-of-thought prompting.

### Selection-inference prompting

#### Метод

Опубликовано Antonia Creswell и др., одно из расширений — разбивать единый запрос на два: сначала запрос отбора релевантных фактов из текста («selection prompt»), затем запрос вывода заключения из выбранных фактов («inference prompt»). Эти два промпта чередуются по циклу, получая несколько шагов рассуждения и в итоге итоговый ответ. Идея иллюстрируется на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2205.09712)

#### Результаты

Для модели на 7 млрд параметров selection-inference prompting сильно улучшила результат по сравнению с chain-of-thought на задачах bAbi и Proof Writer (требующих длинных цепочек рассуждений). Лучший результат достигнут, когда selection-inference сочетается с дообучением.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя прирост на этих бенчмарках был заметным, они были выбраны за долгие цепочки рассуждений. На задачах без долговременных рассуждений эффект вероятно меньше.

Результаты иллюстрируют несколько общих правил работы с большими моделями. Во-первых, разбивание сложных задач на мелкие подзадачи повышает надежность и точность — мельче задача, меньше ошибок. Во-вторых, максимальный результат достигается комбинированием дообучения с выбранным подходом.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Архитектура достоверных рассуждений

Через несколько месяцев после публикации selection-inference prompting авторы расширили идею:

- Как понять, когда цикл selection-inference должен остановиться или продолжиться
- Добавить value-функцию для оценки нескольких веток рассуждений и выбора наилучшей
- Снизить галлюцинации «фейковых фактов», дообучая модель на рассуждениях о метках предложений (например, sen1), вместо генерации самих предложений

#### Метод

В оригинале selection-inference — это чередование специальных промптов «selection» и «inference» для выбора фактов и вывода из них, формируя цепочку рассуждений.

Расширение включает два компонента.

Во-первых, появляется модель 'halter', которая после каждого шага inference спрашивает, достаточно ли рассуждений для ответа. Если да, то генерируется итоговый ответ.

Преимущества halter:

- может останавливать или продолжать процесс selection-inference в зависимости от ситуации
- если процесс не остановился, будет отсутствовать ответ, что лучше, чем неправдоподобный (галлюцинаторный)

[![Достоверные рассуждения](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

[![Достоверные рассуждения](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

Во-вторых, добавлена value-функция для оценки качества шагов рассуждений и поиска по разным траекториям. Это повторяет идею повышения надежности: вместо одного ответа сгенерировать множество и выбрать лучший с помощью value-функции / дискриминатора / верификатора.

[![Достоверные рассуждения](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

Помимо этого, применён трюк для уменьшения галлюцинаций. Вместо генерации фактических предложений, обучается модель, работающая с метками предложений (например sen1). Это помогает предотвратить генерирование неуказанных в контексте ложных фактов.

[![Достоверные рассуждения](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы оценили методику на ProofWriter (не показан) и EntailmentBankQA (показан). Точность значительно возросла, особенно на сложных задачах рассуждений.

![Достоверные рассуждения](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

Более того, трюк с метками предложений практически полностью устранил галлюцинации!

![Достоверные рассуждения](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022).](https://arxiv.org/abs/2208.14271)

#### Выводы

Статья иллюстрирует ряд полезных уроков для повышения надежности больших языковых моделей:

- Делите сложные задачи на более надежные подзадачи
- Генерируйте ответ по шагам, оценивая его на каждом этапе
- Генерируйте множество возможных ответов и выбирайте наилучший с помощью другой модели или функции
- Снижайте галлюцинации, ограничивая, что модель может говорить (например, используя метки предложений вместо предложений)
- Максимизируйте производительность моделей через дообучение на специализированных задачах

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Кроме проблем с длинными цепочками рассуждений (где selection-inference особенно полезен), chain-of-thought сложно работать, когда примеры короткие, а задача — длинная.

#### Метод

Least-to-most prompting — техника, разбивающая рассуждения на более простые и надежные подзадачи. Идея — вызвать у модели подзадачу посредством промпта типа `To solve {question}, we need to first solve: "`. С этой подзадачей модель генерирует решение. Решение добавляется к исходному вопросу, и процесс повторяется, пока не будет получен финальный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou и др. (2022).](https://arxiv.org/abs/2205.10625)

#### Результаты

При применении к задачам с длинными цепочками рассуждений с помощью `code-davinci-002` (оптимизированного под код, но понимающего текст), авторы зафиксировали прирост с 16% до 99.7%!

[
![Least-to-most prompting результаты на задаче конкатенации последних букв](/images/least-to-most_tab4.png)
![Least-to-most prompting результаты на SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting результаты на числовых задачах DROP](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou и др. (2022).](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя прирост по least-to-most впечатляет, он измерен на узком наборе задач с длинными цепочками рассуждений.

Тем не менее, он показывает общие принципы: повышайте надежность, (a) деля задачу на подзадачи и (b) давая модели больше времени и пространства, чтобы решить задачу.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Майевтическое (maieutic) промптирование

#### Метод

В отличие от предыдущих техник, ориентированных на максимизацию вероятности правильных ответов, другая методика — использовать GPT-3 для генерации дерева возможных объяснений (правильных и неправильных), а потом анализировать их взаимосвязи и угадывать, какие из них истинны. Метод назван майевтическим промптированием [Jaehun Jung и др. в мае 2022](https://arxiv.org/abs/2205.11822) (майевтика — получение идей через вопросы, как в сократическом методе).

Суть метода:

- Сначала строится майевтическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинается с вопроса с вариантами или утверждения True/False (напр. `War cannot have a tie`)
  - Для каждого варианта ответа модель генерирует объяснение (с промптом типа `War cannot have a tie? True, because`)
  - Затем моделью проверяется, если обратное объяснение (с префиксом `It is wrong to say that {explanation}`) меняет ответ, значит объяснение считается «логически целостным»
  - Если объяснение не логически целостное, процесс рекурсивно повторяется: каждое объяснение становится вопросом True/False, для которого генерируются новые объяснения
  - В итоге получается дерево, листы которого — объяснения, инверсия которых меняет ответ
- Далее дерево преобразуется в граф отношений:
  - Для каждого узла считается мера доверия модели (выведенная из вероятности получить ответ `True` при данном объяснении)
  - Для каждой пары узлов модель определяет, являются ли они подразумеваемыми друг другом или противоречащими
- В конце находится наиболее согласованное множество убеждений, принимаемых за true:
  - Проблема формулируется как задача взвешенной максимальной удовлетворимости (MAX-SAT)
  - Решателем находится самое самосогласованное множество, принимаемое за правдивое

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung и др. (2022).](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung и др. (2022).](https://arxiv.org/abs/2205.11822)

#### Выводы

Несмотря на сложность, один из минусов — применение только к вопросам с выбором ответа.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность

#### Метод

Для задач с дискретным множеством ответов простой способ повысить надежность — сэмплировать множество объяснений и ответов (используя положительную температуру), а потом выбрать наиболее часто встречающийся итоговый ответ.

[![Метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang и др. (2022).](https://arxiv.org/abs/2203.11171)

#### Результаты

Этот метод повысил точность на 1–24 пункта процента на ряде бенчмарков по математике и рассуждениям. (На рисунке показаны результаты модели LaMDA Google; на более крупной PaLM базовые результаты выше, но улучшения несколько меньше.)

[![Результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang и др. (2022).](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя метод прост в реализации, он может быть дорогим. Генерация 10 ответов увеличит стоимость в 10 раз.

Кроме того, как и многие техники, он применим только к задачам с ограниченным множеством ответов. Для открытых задач, где каждый ответ уникален (например, написание поэмы) непонятно, что значит выбрать самый распространённый ответ.

Наконец, метод полезен, когда есть множество путей или формулировок для ответа; если путь один, метод может не помочь. Крайний пример: если задача — сгенерировать одиночный токен, то выбор наиболее частого токена из 100 генераций эквивалентен идентификации токена с наивысшей вероятностью (достижимой при temperature=0).

### Верификаторы

Другой способ повысить производительность — обучить модель-верификатор для оценки ответов основной модели. Если выходная версия отвергается, можно повторно генерировать ответ, пока не получим приемлемый результат. Часто проще оценить готовый ответ, чем создать ответ, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи из OpenAI применили этот метод для школьных задач по математике:

- Сначала дообучали модель на вопросах и решениях
- Для каждой задачи в тренировочном наборе сгенерировали 100 решений
- Каждое решение автоматически маркировали как правильное или неправильное по итоговому ответу
- Используя эти решения, с метками правильности, обучили верификатор классифицировать корректность решения для вопроса
- На этапе теста генеративная модель порождает 100 решений для каждой задачи, а итоговый ответ выбирается по максимальному баллу верификатора

[![Метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe и др. (2021).](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 175B параметров и 8000 примеров модель достигла прироста точности с ~33% до ~55% на школьной математике.

[![Результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe и др. (2021).](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и при самосогласованности, метод может быть дорогим, т.к. генерация 100 решений на задачу увеличивает стоимость примерно в 100 раз.

## Теории надежности

Хотя методы выше различаются по подходу, их цель — повысить надежность на сложных задачах. В основном они делают это через:

- разбиение ненадежных операций на более надежные (например, selection-inference prompting)
- использование нескольких шагов или отношений для повышения общей надежности системы выше, чем у любой отдельной части (например, maieutic prompting)

### Вероятностные графовые модели

Парадигма сборки надежной системы из ненадежных компонентов напоминает вероятностное программирование, и многие техники из того поля применимы и здесь.

В статье _Language Model Cascades_ Дэвид Доан и др. интерпретируют описанные техники в терминах вероятностных графовых моделей:

#### Цепочки рассуждений (chain of thought prompting)

[![графовая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022).](https://arxiv.org/abs/2207.10342)

#### Дообучение цепочек рассуждений / Self-taught reasoner

[![графовая модель fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022).](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022).](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![графовая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022).](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формализация техник в виде вероятностных моделей сразу может быть не очень полезна для решения конкретных задач, этот подход может помочь при выборе, комбинировании и разработке новых техник.

## Итоговые мысли

Исследования больших языковых моделей активно ведутся и быстро развиваются. Наука не только улучшает модели, но и углубляет понимание лучших способов их использования. Чтобы оценить темп, все приведённые статьи опубликованы за последние 12 месяцев (по состоянию на сентябрь 2022).

Ожидайте появления моделей и методов лучше, чем описанные здесь. Даже если конкретные техники устареют, общие принципы вряд ли потеряют актуальность и останутся частью арсенала экспертов.

## Библиография

| Урок                                                                                                                           | Статья                                                                                                                                 | Дата      |
| ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- | --------- |
| Делите сложные задачи на подзадачи (учитывайте возможность показывать промежуточные результаты пользователям)                 | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Можно улучшить результат, сгенерировав много кандидатов и выбрав лучший                                                          | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                    | Октябрь 2021 |
| В задачах рассуждений модели лучше работают, рассуждая шаг за шагом перед ответом                                              | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                             | Январь 2022 |
| Шаговые цепочки рассуждений улучшаются от генерации множества объяснений и выбора самого популярного ответа                   | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                           | Март 2022 |
| Дообучить пошагового рассуждателя можно только по вопросам с множественным выбором                                            | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                      | Март 2022 |
| Метод пошагового рассуждения работает хорошо даже без примеров                                                               | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                     | Май 2022 |
| Можно улучшить рассуждение, чередуя «промпт отбора» и «промпт вывода»                                                          | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)         | Май 2022 |
| Для длинных цепочек рассуждений улучшение даёт последовательное разбиение задачи на части                                       | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                        | Май 2022 |
| Модель может анализировать верные и ложные объяснения для выбора наиболее согласованного их набора                            | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                    | Май 2022 |
| При изложении техник полезно представлять системы как вероятностные каскады из ненадежных компонентов                           | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                           | Июль 2022 |
| Галлюцинации устраняются с помощью меток предложений, а «halter» помогает уменьшить ложные ответы                             | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                    | Август 2022 |