---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, если GPT-3 не справляется с задачей?

- Искать лучший запрос, который будет выдавать более надежные ответы?
- Вложиться в тысячи примеров для тонкой настройки кастомной модели?
- Предположить, что модель неспособна решить задачу, и перейти к следующему?

Однозначного ответа нет — всё зависит от ситуации. Однако, если ваша задача включает логическое рассуждение или сложность, рассмотрите техники из этой статьи, чтобы создать более надежные и эффективные запросы.

## Почему GPT-3 ошибается на сложных задачах

Если бы вас попросили умножить 13 на 17, ответ сразу пришёл бы в голову? Для большинства, вероятно, нет. Но это не значит, что люди не умеют умножать двузначные числа. С несколькими секундами и листом бумаги легко понять, что 13 × 17 = 130 + 70 + 21 = 221.

Так же и GPT-3: если дать ей слишком сложную задачу, чтобы решить её за время вычисления следующего токена, она может выдумать неверный ответ. Но, как и люди, это не значит, что модель неспособна решить задачу. Если дать время и пространство для рассуждений, модель может дать надежный ответ.

Например, если вы зададите &lt;&lt;&lt;INL_0>>> следующую задачу про жонглирование мячами, он ответит неверно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет. На самом деле, если задать модели &lt;&lt;&lt;INL_1>>>, она надежно решит задачу:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, из одного примера сложно понять, работает ли этот трюк &lt;&lt;&lt;INL_2>>> вообще или просто повезло именно с этой задачей. Но он действительно работает. На бенчмарке задач по математическим головоломкам трюк &lt;&lt;&lt;INL_3>>> значительно повысил эффективность GPT-3 — с бесполезных 18% до приличных 79%!

## Возможности модели зависят от контекста

При работе с GPT-3 частая концептуальная ошибка — считать, что возможности модели одинаковы во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит она не способна решать простую логику.

Однако, как показывает пример &lt;&lt;&lt;INL_4>>>, очевидные ошибки GPT-3 можно исправить, улучшив запрос, чтобы модель смогла сама направить себя к правильному ответу.

## Как повысить надежность на сложных задачах

Остальная часть статьи делится техниками повышения надежности больших языковых моделей для сложных задач. Несмотря на то, что некоторые техники специфичны для определённых типов задач, многие строятся на общих принципах, применимых к широкому спектру задач, например:

- Давать более четкие инструкции
- Разбивать сложные задачи на простые подзадачи
- Структурировать инструкции, чтобы модель оставалась сфокусированной
- Просить модель объяснять перед тем, как отвечать
- Запрашивать обоснования для многих возможных ответов, а затем синтезировать их
- Генерировать много вариантов ответов и выбирать лучший с помощью модели
- Тонко настраивать кастомные модели для максимальной производительности

## Разбивайте сложные задачи на простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на упрощённые части.

Например, рассмотрим задачу с выбором ответа по тексту — в данном случае игра Clue. При прямом вопросе &lt;&lt;&lt;INL_5>>> не может связать подсказки 3 и 5 и отвечает неправильно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 показывают, что полковник Мустанг был единственным человеком в обсерватории и что тот человек держал свечу, модель не объединяет эти факты в правильный ответ (a) — Да.

Вместо прямого запроса ответа мы можем разбить задачу на три этапа:

- Сначала пройти по подсказкам и определить, какие из них потенциально релевантны
- Затем объединить релевантные подсказки для рассуждения над ответом
- И, наконец, записать окончательный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Давая модели больше времени и пространства для размышлений и направляя её по плану рассуждений, она может определить правильный ответ (a) — Да.

Дополнительный плюс разбивки сложных инструкций на подзадачи — это помогает удержать внимание модели на каждой из них.

Например, если мы попросим &lt;&lt;&lt;INL_6>>> резюмировать текст на его оригинальном языке, модель может случайно переключиться на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Однако, если сначала попросить её определить язык текста, а затем резюмировать, модель становится надежнее:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просите модель объяснять перед ответом

Ещё один мощный способ повысить надежность ответов — заставить модель рассуждать постепенно, а не сразу выдавать финальный ответ. «Размышляя вслух», модель гораздо чаще приходит к правильному решению.

### Zero-shot

#### Метод

Опубликованная [Такеши Кодзимой и соавторами в 2022 году](https://arxiv.org/abs/2205.11916), самая простая техника заставить модель рассуждать — просто дописывать ответы фразой &lt;&lt;&lt;INL_7>>>. На рисунке 2 показан пример:

[![пример zero-shot рассуждения](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применив этот простой трюк к датасету задач MultiArith, авторы обнаружили, что точность увеличилась в 4 раза — с 18% до 79%!

[![результаты zero-shot рассуждения](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк &lt;&lt;&lt;INL_8>>> хорошо работает для математических задач, он не универсален. Авторы выяснили, что он лучше всего помогает при многошаговых арифметических задачах, символических рассуждениях, стратегических задачах и других задачах на рассуждение. Он не помогал при простых арифметических задачах или вопросах здравого смысла и, предположительно, не будет полезен в ряде нерассуждающих задач.

[![результаты zero-shot рассуждения](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.11916).

Если вы применяете эту технику к своим задачам, не бойтесь экспериментировать с инструкциями. &lt;&lt;&lt;INL_9>>> — довольно общая, поэтому вы можете добиться лучшей производительности, создавая более строгие инструкции, настроенные под ваш кейс. Например, можно попробовать более структурированные варианты вроде &lt;&lt;&lt;INL_10>>>. Можно даже дать модели пример правильного формата, чтобы удерживать её в нужном русле, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Few-shot примеры

#### Метод

Подсказка модели с рассуждениями может быть оформлена по-разному. Один из способов — показать несколько примеров ('few-shot'), как изучили [Джейсон Вэй и Денни Чжоу и соавторы из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot запроса с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Больше примеров цепочек рассуждений от разметчиков:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Отметим, что вопрос о том, плавают ли груши, вызвал споры)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

Тестировать на школьных математических задачах показало, что цепочки рассуждений увеличили процент решения задач в 3 раза — с 18% до 57%.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Помимо математики, цепочки рассуждений улучшили производительность на вопросах, связанных со спортом, отслеживанием монеты и конкатенацией последней буквы. Обычно, чтобы достичь максимума, нужно не много примеров — около 8.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество few-shot подхода по сравнению с &lt;&lt;&lt;INL_11>>> в том, что вы можете гибко указать формат, длину и стиль рассуждений, которые хотите получить до получения окончательного ответа. Это особенно полезно, если модель изначально рассуждает не так, или недостаточно глубоко.

### Тонкая настройка

#### Метод

Чтобы добиться максимальной производительности, обычно требуется тонкая настройка модели. Но тонкая настройка на объяснениях требует тысяч снятых объяснений — это дорого.

В 2022 году Эрик Зеликман и Юхуай Ву с соавторами предложили хитрую процедуру, которая использует few-shot запрос для генерации датасета объяснений для тонкой настройки модели. Идея: генерировать объяснения с few-shot запросом, оставляя те, которые дают правильный ответ. Для неправильных ответов затем повторять генерацию с правильными ответами, включёнными в вопрос. Они назвали эту процедуру STaR (Self-taught Reasoner — самообучающийся рассуждатель):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Такой подход объединяет преимущества тонкой настройки с преимуществами цепочек рассуждений без необходимости писать тысячи объяснений.

#### Результаты

Применив метод к датасету вопросов здравого смысла, авторы получили, что STaR превосходит цепочки рассуждений по отдельности (73% > 37%) и тонкую настройку по отдельности (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot запроса для расширения или изменения датасета тонкой настройки — идея, применимая не только к созданию объяснений. Например, если у вас есть большой объём неструктурированного текста, можно с помощью запросов извлечь структурированный датасет для дальнейшей тонкой настройки.

## Расширения цепочек рассуждений

Опубликовано несколько расширений техники цепочек рассуждений.

### Selection-inference prompting (выбор-применение)

#### Метод

Опубликованная Антонией Кресвелл и соавторами, эта техника разбивает единый запрос для генерации объяснений и ответов на части. Сначала запрос выбирает релевантные факты из текста ('selection prompt'). Затем второй запрос выводит заключение на основе выбранных фактов ('inference prompt'). Эти два запроса чередуются, позволяя генерировать несколько шагов рассуждений и находить окончательный ответ. Авторы иллюстрируют идею на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

Применённая к модели с 7 млрд параметров, техника существенно улучшила результаты на задачах bAbi и Proof Writer (требующих длинных последовательностей рассуждений). Лучшие результаты получены при комбинировании selection-inference с тонкой настройкой.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя улучшения на этих бенчмарках значительны, их выбрали специально для задач с длинными последовательностями рассуждений. Для задач без многих шагов рассуждений приросты, вероятно, будут меньше.

Общие уроки:

- Разбивайте сложные задачи на более мелкие — так меньше ошибок
- Для максимума производительности комбинируйте тонкую настройку с выбранным подходом

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Спустя несколько месяцев после публикации selection-inference авторы расширили метод в отдельной статье, предложив:

- как определять, когда цикл selection-inference нужно остановить или продолжить 
- добавление функции ценности для поиска по разным путям рассуждений 
- снижение галлюцинаций, заставляя модель работать с метками предложений (напр., sen1) вместо написания предложений целиком

#### Метод

В оригинальной технике чередуются запросы для выбора фактов и вывода заключений, создавая цепочку рассуждений.

В расширении добавляют два компонента.

Первое — модель 'halter', которая после каждого шага рассуждений решает, достаточно ли информации для ответа. Если да — генерируется финальный ответ.

Преимущества halter:

- может прекратить или продолжить процесс selection-inference
- если процесс никогда не остановится, ответа не будет (что лучше чем выдуманный)

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Второе — функция ценности, которая оценивает качество рассуждений и ищет лучшие логические цепочки. Это отражает общую тему — вместо одного ответа сгенерировать множество и выбрать лучший с помощью дополнительной модели.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, чтобы уменьшить галлюцинации, вместо генерации фактических предложений модель тонко настраивают для работы с метками предложений (например, sen1), что помогает избежать вымышленных фактов, отсутствующих в контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Оценка на ProofWriter (не показано) и EntailmentBankQA (показано) показала значительный рост точности, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

Их трюк с метками предложений практически полностью избавил от галлюцинаций!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Статья даёт несколько полезных уроков для повышения надежности больших языковых моделей:

- Разбивайте сложные задачи на более надежные подзадачи
- Стройте ответы поэтапно с оценкой на каждом шаге
- Генерируйте много вариантов и выбирайте лучшие при помощи другой модели или функции
- Снижайте галлюцинации, ограничивая то, что модель может говорить (например, метками предложений)
- Максимизируйте производительность моделей тонкой настройкой на специализированных задачах

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Помимо плохой работы на длинных цепочках рассуждений (где эффективность selection-inference на высоте), цепочки рассуждений могут плохо справляться, когда примеры короткие, но задача большая.

#### Метод

Least-to-most prompting — ещё одна техника, которая разбивает задачу на более простые подзадачи. Идея — сначала запросить у модели подзадачу, например, с помощью &lt;&lt;&lt;INL_12>>>. Получив подзадачу, модель генерирует решение. Затем решение добавляется к исходному вопросу, и процесс повторяется, пока не получится окончательный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На бенчмарках с длинными цепочками рассуждений и использованием &lt;&lt;&lt;INL_13>>> (оптимизированного для кода, но понимающего текст) прирост составил до 16% -> 99.7%!

[
![Результаты least-to-most prompting по задаче с конкатенацией последней буквы](/images/least-to-most_tab4.png)
![Результаты least-to-most prompting на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most prompting по числовым задачам DROP](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя приросты впечатляют, они измерены на узком наборе задач с длинными цепочками рассуждений.

Тем не менее, они подчёркивают главную идею: повысить надежность, (а) разбив сложные задачи на подзадачи и (б) дав модели больше времени и пространства на решение.

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Маевтические подсказки (Maieutic prompting)

#### Метод

В отличие от предыдущих техник, которые пытаются максимизировать вероятность правильных ответов, другой подход — использовать GPT-3 для построения дерева возможных объяснений (правильных и неправильных), а затем анализировать их взаимосвязи, чтобы определить, какое множество верно. Этот метод назвали маевтическим подсказыванием (от греческого софистического метода задавания вопросов), как предложили [Джэхун Джунг и др. в мае 2022](https://arxiv.org/abs/2205.11822).

Метод сложен, работает так:

- Сначала строится маевтическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинаем с множественного выбора или утверждения правда/ложь (например, &lt;&lt;&lt;INL_14>>>)
  - Для каждого варианта ответа модель генерирует объяснение (пример запроса &lt;&lt;&lt;INL_15>>>)
  - Затем по вопросу и объяснению модель генерирует ответ. Если реверс объяснения (с префиксом &lt;&lt;&lt;INL_16>>>) меняет ответ, объяснение считается «логически цельным»
  - Если объяснение не цельно, процесс повторяется рекурсивно, превращая объяснение в новый вопрос правда/ложь и генерируя объяснения для него
  - В итоге получается дерево объяснений, где у каждого листа реверс объяснения меняет ответ модели
- Затем строится граф отношений:
  - Для каждого узла вычисляют относительную уверенность модели (по вероятности получить ответ &lt;&lt;&lt;INL_17>>> при данном объяснении)
  - Для пары узлов модель определяет, связаны ли они (подтверждение/противоречие)
- После этого ищут максимально согласованный набор убеждений через задачу максимальной удовлетворимости с весами (MAX-SAT)
- Решатель находит максимально самосогласованный набор убеждений, принимаемый за истинные

[
![Маевтическое подсказываниe](/images/maieutic_fig2.png)
![Маевтическое подсказываниe](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты маевтического подсказывани](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Несмотря на сложность, минус этого метода — он применяется только к мультивыборочным вопросам.

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность

#### Метод

Для задач с дискретным набором ответов простой способ повысить надежность — создать множество объяснений и ответов с помощью выборки (с ненулевой температурой) и выбрать наиболее встречающийся финальный ответ.

[![Метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Техника повысила точность от 1 до 24 процентных пунктов на ряде бенчмарков по математике и рассуждениям. (На графиках — результаты на модели Google LaMDA; для более крупной модели PaLM базовый уровень был выше, но прирост немного меньше.)

[![Результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Несмотря на простоту, способ может быть затратным — 10 ответов увеличат расходы в 10 раз.

Он применим только к задачам с ограниченным набором ответов. В творческих задачах (напр. написание стихотворения) непонятно, что значит выбрать самый частотный ответ.

Этот метод наиболее полезен, когда есть несколько путей или формулировок решить задачу; если путь единственный, метод не поможет. Крайний пример: если задача — сгенерировать один токен, то выбор наиболее частого токена из 100 генераций — то же, что и выбор с максимальной вероятностью (температура=0).

### Верификаторы

Другой важный метод повышения производительности — обучить модель-верификатор/дискриминатор, который оценивает ответы главной модели. Если ответ отвергается, можно сгенерировать заново. Часто оценить ответ проще, чем его создать, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи OpenAI применили этот метод к задачам школьной математики, так:

- Сначала модель тонко настраивают на вопросы и решения
- Для каждой задачи генерируют 100 решений
- Решения автоматически размечают как правильные или неправильные, исходя из правильности финального ответа
- На размеченных данных обучают модель-верификатор классифицировать решения
- На тесте главная модель генерирует 100 решений, и выбирается решение с максимальным баллом у верификатора

[![Метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 175B и 8000 примерами точность школьной математики выросла примерно с 33% до 55%.

[![Результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и метод самосогласованности, этот способ довольно затратен — генерация, например, 100 решений на задачу увеличит расходы примерно в 100 раз.

## Теории надежности

Несмотря на разные подходы, все вышеперечисленные техники направлены на повышение надежности в сложных задачах. В основном они делают это через:

- разбиение ненадежных операций на более надежные подоперации (например, selection-inference prompting)
- использование множества шагов или отношений, чтобы надёжность системы превосходила надёжность отдельных компонентов (например, maieutic prompting)

### Вероятностные графовые модели

Парадигма построения надежной системы из ненадежных компонентов напоминает вероятностное программирование, и многие методы анализа из той области применимы и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и соавторы рассматривают техники выше через призму вероятностных графовых моделей:

#### Цепочки рассуждений

[![графовая модель цепочек рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Тонко настроенные цепочки рассуждений / Self-taught reasoner

[![графовая модель тонко настроенных цепочек](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![графовая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_, David Dohan и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формализация в виде вероятностных графовых моделей не всегда прямоприменима к конкретным задачам, она может помочь при отборе, комбинировании и поиске новых подходов.

## Заключение

Исследования больших языковых моделей идут очень активно и быстро развиваются. Учёные не только улучшают модели, но и лучше понимают, как их эффективно использовать. Обратите внимание — все упомянутые статьи опубликованы за последние 12 месяцев (по состоянию на сентябрь 2022).

В будущем ожидайте появления новых, лучших моделей и техник. Даже если конкретные здесь описанные методы будут забыты или заменены, их общие принципы, вероятно, останутся важной частью арсенала любого эксперта.

## Библиография

| Урок                                                                                                                     | Статья                                                                                                                                    | Дата       |
| ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------- |
| Разбивайте сложные задачи на подзадачи (рассмотрите возможность показать промежуточные результаты пользователям)          | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Улучшают результаты генерация множества кандидатов и последующий выбор лучшего                                          | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Октябрь 2021 |
| На задачах рассуждений модели работают лучше, когда рассуждают поэтапно до ответа                                         | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Январь 2022 |
| Повышают качество рассуждений генерация множества объяснено-ответных вариантов и выбор наиболее популярного              | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Март 2022  |
| Для тонкой настройки рассуждателя можно использовать только данные с вопросами и ответами множественного выбора          | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Март 2022  |
| Метод пошаговых рассуждений работает отлично даже без примеров                                                           | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022   |
| Лучше рассуждать, чередуя prompts для выбора фактов и их вывода (selection + inference)                                   | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022   |
| Для длинных задач с рассуждениями улучшает производительность разбиение задачи на части с incremental решением          | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022   |
| Модель можно заставить анализировать как правильные, так и ложные объяснения, чтобы определить наиболее согласованный набор | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022   |
| Эти техники можно описать в терминах вероятностного программирования, где система построена из ненадежных компонентов      | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022  |
| Можно устранить галлюцинации, используя метки предложений и режим 'halter'                                                | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Август 2022 |