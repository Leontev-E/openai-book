---
lang: ru
translationOf: openai-cookbook
---

# Техники для повышения надежности

Что делать, когда GPT-3 не справляется с задачей?

- Искать лучший запрос (prompt), который будет выдавать более надежные ответы?
- Вкладывать средства в тысячи примеров для дообучения кастомной модели?
- Считать, что модель неспособна выполнить задачу, и переходить к следующей?

Однозначного ответа нет — всё зависит от ситуации. Однако, если ваша задача связана с логическим рассуждением или сложностью, рассмотрите применение техник из этой статьи для создания более надежных и эффективных запросов.

## Почему GPT-3 ошибается в сложных задачах

Если вас попросят умножить 13 на 17, сразу ли ответ придет вам в голову? Для большинства из нас — скорее нет. Но это не значит, что люди неспособны делать умножение двузначных чисел. Немного времени и лист бумаги — и вы поймете, что 13 x 17 = 130 + 70 + 21 = 221.

Точно так же, если GPT-3 дать слишком сложную задачу для решения за время предсказания следующего токена, он может выдумать неверный ответ. Тем не менее, как и человек, модель не обязательно неспособна решить задачу. Имея немного больше времени и пространства для рассуждений, модель может ответить корректно.

Например, если попросить &lt;&lt;&lt;INL_0>>> решить следующую математическую задачу о жонглировании мячами, ответ будет неправильным:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Означает ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если подать на вход модели &lt;&lt;&lt;INL_1>>>, задача решается корректно:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно судить, работает ли этот &lt;&lt;&lt;INL_2>>> приём в целом или модель просто повезло на конкретной задаче. Но это действительно работает. На наборе задач по словесной математике, &lt;&lt;&lt;INL_3>>> увеличил процент правильных решений GPT-3 с 18% — практически бесполезного результата — до 79% — приличного показателя!

## Способности модели зависят от контекста

При работе с GPT-3 распространённая концептуальная ошибка — считать, что его способности фиксированы во всех контекстах. Например, если GPT-3 не справляется с простым логическим вопросом, значит он неспособен к решению таких задач.

Однако пример &lt;&lt;&lt;INL_4>>> показывает, что видимые ошибки GPT-3 можно исправить, подобрав лучший запрос, который поможет модели направить её рассуждения к правильному ответу.

## Как повысить надежность в сложных задачах

Далее в статье приведены техники повышения надежности больших языковых моделей в сложных задачах. Некоторые из них специфичны для определённых типов проблем, но многие основаны на общих принципах, применимых к разным задачам, например:

- Давать более чёткие инструкции
- Делить сложные задачи на более простые подпункты
- Структурировать инструкции, чтобы удерживать модель в рамках задачи
- Предлагать модели объяснять ход рассуждений перед ответом
- Запрашивать обоснования для разных вариантов ответа, а затем обобщать
- Генерировать множество ответов и выбирать лучший с помощью модели
- Дообучать кастомные модели для повышения производительности

## Деление сложных задач на более простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на более простые части.

Например, возьмём задачу с выбором ответа по тексту — например, игра Clue. Если спросить напрямую, &lt;&lt;&lt;INL_5>>> не сможет связать подсказки 3 и 5 и ответит неправильно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 показывают, что в обсерватории был только полковник Мастард и у этого человека был канделябр, модель не объединяет эти факты для правильного ответа «(a) Да».

Но вместо прямого запроса можно разбить задачу на три части:

- Сначала пройтись по подсказкам по одной и определить, может ли каждая быть релевантной
- Затем объединить релевантные подсказки и рассуждать о правильном ответе
- И, наконец, написать окончательный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Дав модели больше времени и направляя её по плану рассуждения, она может найти правильный ответ (a) Да.

Ещё одно преимущество деления сложных инструкций на подпункты — это помогает удержать фокус модели на каждом этапе.

Например, если попросить &lt;&lt;&lt;INL_6>>> резюмировать текст на исходном языке, модель иногда переходит на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Если же сначала попросить модель определить язык текста, а потом уже резюмировать его, надёжность ответа увеличивается:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просьба к модели объяснять ход рассуждений перед ответом

Ещё одна мощная техника для повышения надежности — заставить модель постепенно рассуждать над ответом, а не сразу выдавать итог. «Проговаривание» мысли вслух повышает шансы получить правильный результат.

### Зеро-шот (без примеров)

#### Метод

Опубликованный [Такеши Кодзимой и соавторами в 2022 году](https://arxiv.org/abs/2205.11916) самый простой способ вызвать у модели рассуждения — просто попытаться начать ответ с &lt;&lt;&lt;INL_7>>>. На рисунке 2 приведён пример:

[![пример рассуждений в зеро-шот](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применение этой простой методики к датасету MultiArith увеличило точность в четыре раза — с 18% до 79%:

[![результаты зеро-шот рассуждений](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Последствия

Хотя этот &lt;&lt;&lt;INL_8>>> прием хорошо работает на математике, он менее эффективен в других задачах. Авторы отметили, что он особенно подходит для многошаговых арифметических и символических задач, а также стратегий и других задач с рассуждениями. Он не помогал с простыми математическими задачами или вопросами здравого смысла и, предположительно, не подойдет для многих других задач без рассуждений.

[![результаты зеро-шот рассуждений](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы: Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее — читайте [полную статью](https://arxiv.org/abs/2205.11916).

Если вы применяете эту технику в своих задачах, не бойтесь экспериментировать с инструкциями. &lt;&lt;&lt;INL_9>>> довольно общая, так что возможно, что более строгие форматы, адаптированные под конкретный кейс, дадут лучший результат. Можно попробовать более структурированные варианты, например &lt;&lt;&lt;INL_10>>>. Можно даже привести пример формата, чтобы помочь модели оставаться в рамках, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Несколько примеров (few-shot)

#### Метод

Способ стимулировать рассуждения — привести несколько примеров («few-shot»). Этот подход изучали [Джейсон Вэй и Дэнни Чжоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Ниже пример few-shot с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй, Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё примеры цепочек рассуждений, созданных людьми:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй, Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Заметим, что вызывает сомнения факт, что груши действительно плавают)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При проверке на задачах школьной математики такой подход с цепочкой рассуждений утроил количество правильных решений — с 18% до 57%.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй, Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, эта техника улучшила ответы по спортивным задачам, отслеживанию подбрасывания монеты и составлению слов из последних букв. В большинстве случаев для достижения эффекта требовалось немного примеров (обычно меньше восьми).

[![результаты цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй, Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее — читайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Последствия

Одним из преимуществ few-shot по сравнению с &lt;&lt;&lt;INL_11>>> является более лёгкий контроль формата, длины и стиля рассуждений, которые вы ожидаете от модели перед итоговым ответом. Это особенно полезно, когда модель изначально рассуждает неправильно или недостаточно глубоко.

### Дообученные модели

#### Метод

Чтобы добиться максимальной производительности на задаче, обычно требуется дообучение кастомной модели. Однако обучение на объяснениях — затратный процесс, требующий тысяч примеров объяснений.

В 2022 году Эрик Зеликман, Юхуай Ву и соавторы предложили хитрый способ генерировать датасет объяснений с помощью few-shot prompt. Идея в том, чтобы сгенерировать кандидатов на объяснение, отобрать только те, которые приводят к правильному ответу, а для неправильных ответов повторять генерацию, теперь с правильным ответом в качестве части вопроса. Процедуру назвали STaR (Self-taught Reasoner):

[![Процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман, Юхуай Ву и др. (2022)](https://arxiv.org/abs/2203.14465)

Так можно сочетать выгоды fine-tuning и цепочек рассуждений без необходимости писать тысячи объяснений вручную.

#### Результаты

При применении к набору данных Common Sense Q&A, STaR превзошёл и цепочку рассуждений (73% > 37%), и fine-tuning без объяснений (73% > 60%):

[![Результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман, Юхуай Ву и др. (2022)](https://arxiv.org/abs/2203.14465)

Подробнее — см. [полную статью](https://arxiv.org/abs/2203.14465).

#### Последствия

Использование few-shot prompt для расширения или корректировки датасета для fine-tuning — идея, которую можно применять и в других задачах. Например, из большого массива неструктурированного текста можно с помощью prompt сформировать структурированные данные для дальнейшего обучения.

## Расширения цепочек рассуждений

Появилось несколько расширений техники chain-of-thought.

### Selection-inference prompting

#### Метод

Предложенное Антонией Кресвелл с соавторами, расширение развивает идею цепочки рассуждений, разбивая её на отдельные части. Сначала prompt выбирает релевантные факты из текста ("selection prompt"), затем второй prompt делает выводы на основании выбранных данных ("inference prompt"). Их чередуют в цикле, чтобы пройти множество шагов и прийти к итоговому выводу. В иллюстрации на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 млрд параметров авторы показали значительный прирост к chain-of-thought на тестах bAbi и Proof Writer (требующих много шагов рассуждений). Лучшие результаты достигались в комбинации selection-inference с fine-tuning.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Последствия

Хотя прирост на этих тестах был большим, сами бенчмарки были выбраны именно из-за их требовательности к длинным цепочкам рассуждений. На задачах без заставляющих думать много шагов эффективность вряд ли так высока.

Результаты показывают несколько важных уроков:

- Делить сложные задачи на более простые — отличный способ повысить надежность и качество, чем более атомарна задача, тем меньше вероятность ошибки.
- Для максимального качества часто лучше сочетать fine-tuning с выбранным подходом.

Для подробнее — читайте [полную статью](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Через несколько месяцев после публикации selection-inference, авторы расширили метод в последующей работе с новыми идеями:

- когда нужно остановить или продолжать цикл selection-inference
- введение value function для поиска среди разных цепочек рассуждений
- уменьшение галлюцинаций несуществующих фактов с помощью fine-tuning модели, обученной рассуждать не над самими предложениями, а над их метками (например, sen1) вместо текста

#### Метод

В оригинальной технике selection-inference промты спец-названия "selection" и "inference" чередуются для выбора фактов и вывода на их основе, формируя цепочку рассуждений.

Авторы добавляют два новых компонента.

Во-первых, «halter» модель после каждого шага делает оценку — достаточно ли текущих рассуждений, чтобы ответить на вопрос. Если да — модель формирует итоговый ответ.

Преимущества halter:

- может решать, продолжать ли цепочку или остановить процесс
- если цикл не прерывается никогда — лучше получить отсутствие ответа, а не ошибочный

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавляется value function, которая оценивает качество рассуждений и помогает искать среди разных логических цепочек. Это отражает общее правило повышения надёжности — генерировать множество ответов и выбирать лучший с помощью модели-дискриминатора или функции оценки.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, для уменьшения галлюцинаций, вместо вывода фактических предложений модель обучают работать с их метками (например, sen1), что препятствует созданию ложных фактов, отсутствующих в контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Технику оценили на двух тестах: ProofWriter (не показан) и [EntailmentBankQA](https://allenai.org/data/entailmentbank) (показан). Результаты значительно улучшились, особенно для сложных задач рассуждения.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Их трюк с метками предложений фактически устранил галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Последствия

Эта работа даёт несколько практических советов по повышению надежности больших моделей:

- Разбивайте сложные задачи на более простые надежные
- Стройте ответ пошагово, оценивая процесс
- Генерируйте много ответов и используйте модель/функцию для выбора лучшего
- Снижайте галлюцинации, ограничивая формат вывода (например, метками вместо текста)
- Добивайтесь максимума производительности через тонкую настройку

Подробнее — [полная статья](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Помимо проблем с длинными цепочками рассуждений (где эффективен selection-inference), chain-of-thought бывает проблемным при коротких примерах, но длинной задаче.

#### Метод

Least-to-most prompting — метод, делящий рассуждения на надёжные подпункты. Идея — сначала вывести подзадачу с помощью запроса вроде &lt;&lt;&lt;INL_12>>>, затем решить её. Решение добавляется к исходному вопросу, после чего процесс повторяется, пока не будет получен окончательный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

Применённый к длинным цепочкам рассуждений на моделях &lt;&lt;&lt;INL_13>>> (оптимизированных для кода, но понимающих и текст) приносил приросты от 16% до 99.7%!

[
![Результаты Least-to-most на задаче last-letter-concatenation](/images/least-to-most_tab4.png)
![Результаты Least-to-most на SCAN](/images/least-to-most_tab9.png)
![Результаты Least-to-most на DROP (числовые задачи)](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Последствия

Хотя результаты впечатляющие, они получены на узком наборе задач, требующих длинных цепочек рассуждений.

Тем не менее, они подтверждают общий принцип: повышайте надежность, (а) разбивая задачи на подпункты и (б) давая модели больше времени на решение.

Подробнее — [полная статья](https://arxiv.org/abs/2205.10625).

## Сопутствующие идеи

### Maieutic prompting

#### Метод

В отличие от предыдущих методов, которые пытаются максимизировать вероятность правильного ответа, другой подход — использовать GPT-3 для построения дерева возможных объяснений (как правильных, так и ошибочных), а затем анализировать их взаимоотношения и выбирать наиболее вероятный набор. Этот метод получил название maieutic prompting от [Джэхуна Чжунга и соавторов (май 2022)](https://arxiv.org/abs/2205.11822) (maieutic — связанное с сократовским методом постановки вопросов для выявления идей).

Метод сложен и работает так:

- Сначала строится maieutic-дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинается с вопроса множественного выбора или утверждения «верно/ложно» (например, &lt;&lt;&lt;INL_14>>>)
  - Для каждого варианта ответа генерируется объяснение (с помощью prompt, например &lt;&lt;&lt;INL_15>>>)
  - Потом через prompt с вопросом и объяснением модель должна выдать ответ. Если инвертирование объяснения (с префиксом &lt;&lt;&lt;INL_16>>>) меняет ответ модели, объяснение считается «логически целостным»
  - Если объяснение не логически целостное, процесс повторяется рекурсивно: каждое объяснение превращается в True/False вопрос, для которого генерируются новые объяснения
  - В результате получается дерево объяснений, где каждая листовая вершина обладает свойством изменения ответа при инверсии объяснения
- Во-вторых, дерево превращается в граф взаимосвязей:
  - Для каждого узла вычисляется относительная уверенность модели (по вероятности ответа &lt;&lt;&lt;INL_17>>> при данном объяснении)
  - Для каждой пары узлов определяется, влечёт ли один другой или противоречит ему
- В-третьих, находят самосогласованный набор убеждений:
  - Задача сводится к взвешенной максимальной выполнимости (MAX-SAT)
  - Используется решатель для поиска максимально консистентного множества, которое принимается за истину

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Чжунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты Maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Чжунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Последствия

Несмотря на сложность, ограничение метода в том, что он подходит только для вопросов с вариантами выбора.

Подробнее — [полная статья](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с дискретным набором ответов простой способ повысить надежность — сэмплировать несколько объяснений и ответов (используя положительную температуру), а затем выбрать наиболее частый итоговый ответ.

[![Метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ванг и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Этот приём увеличил точность от 1 до 24 процентных пунктов на различных математических и логических задачах. (На графике результаты для модели LaMDA от Google; у более крупной PaLM исходные показатели были выше, прирост чуть меньше.)

[![Результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ванг и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Последствия

Хотя приём простой, он дорог в вычислениях — создание 10 ответов увеличит стоимость примерно в 10 раз.

Кроме того, как и многие подобные методы, он применим только к задачам с фиксированным набором ответов. Для открытых задач с уникальными ответами (например, написание стихотворения) вопрос о «наиболее частом ответе» неочевиден.

И наконец, в тех случаях, когда есть несколько путей или формулировок для ответа, приём будет полезен. Если путь один, эффект незначителен. Например, если задача — выдать один токен, тогда выбор самого частого ответа из 100 генераций равносилен выбору токена с наибольшей вероятностью (что можно получить за один проход с температурой 0).

### Верификаторы (верификаторы)

Ещё один важный метод повышения качества — обучать модель-дискриминатор для оценки генераций основной модели. Если дискриминатор отклоняет результат, производят повторный запуск генерации. Обычно проще оценить ответ, чем сгенерировать его, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи OpenAI применили этот метод к школьным задачам по математике по такой схеме:

- Сначала модель дообучили на вопросах и решениях
- Для каждого примера в обучающем наборе сгенерировали 100 решений
- Каждое решение автоматически пометили как правильное или неправильное по итоговому ответу
- С помощью этих данных дообучили верификатор для классификации задачи и решения на правильные/неправильные
- При тестировании генеративная модель создаёт 100 решений, а финальным ответом выбирается тот, что получил высший балл от верификатора

[![Метод верификаторов](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коуб и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 на 175 млрд параметров и 8000 обучающих примеров, этот приём поднял точность по задачам с ~33% до ~55%.

[![Результаты верификаторов](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коуб и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Последствия

Подобно self-consistency, метод может быть дорогим: генерация 100 решений увеличивает стоимость примерно в 100 раз.

## Теории надежности

Хотя методы выше разные по своей реализации, у всех одинаковая цель — повысить надежность в сложных задачах. В основном это достигается:

- разбиением ненадежных операций на более надёжные (например, selection-inference prompting)
- использованием нескольких шагов и взаимосвязей, чтобы повысить общую надежность системы выше, чем у отдельных компонентов (например, maieutic prompting)

### Вероятностные графовые модели

Построение надежной системы из менее надежных компонентов напоминает вероятностное программирование. Многие приемы из этой области можно применить и тут.

В работе _Language Model Cascades_ Дэвид Доган и соавторы интерпретируют перечисленные техники в терминах вероятностных графовых моделей:

#### Цепочка рассуждений (chain-of-thought)

[![Вероятностная модель chain-of-thought](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Доган и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Дообученная цепочка рассуждений / Self-taught reasoner

[![Вероятностная модель fine-tuned chain-of-thought](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Доган и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![Вероятностная модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Доган и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![Вероятностная модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Доган и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Последствия

Хотя формализация в виде графовых моделей может и не помочь напрямую решить задачу, она может быть полезна для выбора, объединения и разработки новых методов.

## Заключительные мысли

Исследование больших языковых моделей — очень активная и быстро меняющаяся область. Исследователи не только улучшают модели, но и углубляют понимание того, как лучше применять их. Все приведённые выше статьи были опубликованы за последние 12 месяцев (по состоянию на сентябрь 2022 года).

В будущем ожидайте появление новых моделей и техник. Даже если конкретные методы здесь окажутся устаревшими, общие принципы останутся ключевыми инструментами профессионалов.

## Библиография

| Урок                                                                                                                           | Статья                                                                                                                                  | Дата     |
| ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Делите сложные задачи на более простые подпункты (и подумайте о показе промежуточных результатов пользователям)                | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Улучшайте результат, генерируя много кандидатов и выбирая лучший                                                                | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Октябрь 2021 |
| В задачах рассуждений модели лучше отвечают, рассуждая по шагам                                                                 | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Январь 2022 |
| Повышайте качество поэтапных рассуждений, генерируя много объяснений-ответов и выбирая наиболее популярный                       | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Март 2022 |
| Для обучения модель по поэтапным рассуждениям можно использовать только вопросы с вариантами выбора                              | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Март 2022 |
| Метод поэтапного рассуждения работает отлично даже без примеров                                                                   | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Эффективнее поэтапных рассуждений — поочерёдный вызов «selection» и «inference» prompts                                           | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных задач рассуждений поэтапное решение разбивкой на части сработает лучше                                                | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022 |
| Пускайте модель анализировать и хорошие, и ошибочные объяснения, чтобы выбирать самый согласованный набор                        | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Рассматривайте эти методы как вероятностное программирование, где система построена из ненадёжных компонентов                       | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022 |
| Борьба с галлюцинациями с помощью меток предложений и контроль остановки цикла рассуждений с помощью 'halter' prompt           | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Август 2022 |