---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, когда GPT-3 не справляется с задачей?

- Поискать лучший запрос, который вызывает более надежные ответы?
- Вложиться в тысячи примеров для дообучения кастомной модели?
- Предположить, что модель неспособна решить задачу, и перейти к другой?

Простого ответа нет — всё зависит от ситуации. Однако, если ваша задача связана с логическим рассуждением или сложностью, стоит попробовать техники из этой статьи, чтобы создавать более надежные и эффективные запросы.

## Почему GPT-3 ошибается в сложных задачах

Если вас попросят умножить 13 на 17, сразу ли вы получите ответ? Для большинства — скорее нет. Это не значит, что люди не умеют умножать двухзначные числа. Потратив несколько секунд с бумажкой и карандашом, можно понять, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если задать GPT-3 слишком сложную задачу, чтобы она успела ответить за время вычисления следующего токена, модель может придумать неверный ответ. Однако, как и у людей, это не обязательно означает, что модель не способна решить задачу. Получив немного времени и пространства на рассуждения, она всё ещё может ответить правильно.

Например, если задать `gpt-3.5-turbo-instruct` следующую задачу по математике о жонглировании мячами, она ответит неправильно:

&lt;&lt;&lt;FENCE_0>>>

&lt;&lt;&lt;FENCE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если подать модели `Let's think step by step`, она решает её правильно:

&lt;&lt;&lt;FENCE_2>>>

&lt;&lt;&lt;FENCE_3>>>

Конечно, по одному примеру сложно понять, работает ли трюк `Let's think step by step` вообще или просто повезло в этом конкретном случае. Но он действительно работает. На бенчмарке задач со словами и математикой трюк `Let's think step by step` значительно повысил точность GPT-3 с бесполезных 18% до приличных 79%!

## Возможности модели зависят от контекста

При обучении работе с GPT-3 часто совершается концептуальная ошибка — считать, что её возможности фиксированы во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит она неспособна решать простую логику.

Но как показывает пример `Let's think step by step`, кажущиеся ошибки GPT-3 иногда можно исправить более удачным запросом, который помогает модели направить себя к правильному ответу.

## Как повысить надежность на сложных задачах

Далее в статье описаны техники улучшения надежности больших языковых моделей на сложных задачах. Некоторые техники подходят для конкретных типов проблем, но многие основаны на общих принципах, применимых к широкому кругу задач, например:

- Давать более четкие инструкции
- Делить сложные задачи на простые подзадачи
- Выстраивать инструкцию так, чтобы модель оставалась в нужном русле
- Просить модель объяснить до того, как ответить
- Запрашивать обоснования для разных ответов и затем их синтезировать
- Генерировать много вариантов ответа и выбирать лучший с помощью модели
- Дообучать кастомные модели для максимальной производительности

## Делите сложные задачи на простые

Один из способов дать модели больше времени и пространства для размышлений — разбивать задачи на более простые части.

Например, рассмотрим задачу, где модели задают вопрос с вариантами ответа о тексте — здесь, о игре «Клуэдо». При прямом вопросе `gpt-3.5-turbo-instruct` не может объединить подсказки 3 и 5 и отвечает неправильно:

&lt;&lt;&lt;FENCE_4>>>

&lt;&lt;&lt;FENCE_5>>>

Хотя подсказки 3 и 5 показывают, что полковник Горчица был единственным человеком в обсерватории и у этого человека был подсвечник, модель не соединяет эти данные в правильный ответ (a) Да.

Вместо прямого вопроса можно разбить задачу на три части:

- Сначала пройти по подсказкам по одной и оценить, насколько каждая релевантна
- Затем объединить релевантные подсказки и вывести ответ на вопрос
- Наконец, записать итоговый ответ: (a), (b) или (c)

&lt;&lt;&lt;FENCE_6>>>

&lt;&lt;&lt;FENCE_7>>>

Так, давая модели больше времени и пространства на размышления и направляя её планом рассуждений, она может найти правильный ответ — (a) Да.

Дополнительный плюс разбивки сложных инструкций на мелкие подзадачи — помощь модели в концентрации на каждой из них.

Например, если попросить `gpt-3.5-turbo-instruct` резюмировать текст на его исходном языке, модель может переключиться на английский:

&lt;&lt;&lt;FENCE_8>>>

&lt;&lt;&lt;FENCE_9>>>

Однако, если сначала попросить модель определить язык текста, а потом резюмировать, результаты становятся более надежными:

&lt;&lt;&lt;FENCE_10>>>

&lt;&lt;&lt;FENCE_11>>>

## Просите модель объяснять до ответа

Ещё одна сильная техника повышения надежности — попросить модель постепенно рассуждать над ответом вместо мгновенного перехода к финальному. «Говоря вслух», модель с большей вероятностью придёт к правильному выводу.

### Zero-shot

#### Метод

Опубликовано [Такеши Кодзимой и др. в 2022 году](https://arxiv.org/abs/2205.11916), самый простой способ заставить модель рассуждать — просто добавить к ответам `Let's think step by step.`. На рисунке 2 показан пример:

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применяя трюк к математическому датасету MultiArith, авторы обнаружили, что `Let's think step by step` увеличивает точность в 4 раза — с 18% до 79%!

[![результаты zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Следствия

Хотя `Let's think step by step` работает отлично для задач по математике, он не подходит для всех задач. Авторы выяснили, что он полезен для многошагового арифметического и символического рассуждения, стратегических и прочих логических задач. Не помогал в простых задачах по математике и вопросах здравого смысла, и, скорее всего, не будет полезен для многих нерешающих задач.

[![таблица zero-shot](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.11916).

Если применяете эту технику, не бойтесь экспериментировать с форматированием инструкций. `Let's think step by step` довольно общий, возможно, в вашем случае лучше подойдут более строгие инструкции, например, `First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.`. Мо­жно даже привести модель пример формата, чтобы она не сбивалась, например:

&lt;&lt;&lt;FENCE_12>>>

&lt;&lt;&lt;FENCE_13>>>

### Few-shot примеры

#### Метод

Побуждение модели порассуждать можно делать разными способами. Один из них — показать несколько примеров ('few-shot'), как исследовали [Джейсон Вэй и Дэнни Чжоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Пример few-shot chain-of-thought запроса:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Несколько демонстраций цепочек рассуждений, написанных людьми:

[![пример chain of thought](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание, что подлинность утверждения о плавающих грушах ставится под сомнение)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

На школьных математических задачах цепочки рассуждений увеличили процент решений с 18% до 57%.

[![результаты chain of thought](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочка рассуждений улучшила работу на вопросах спорта, отслеживания подбрасывания монеты и слияния последних букв. При этом для максимального эффекта нужно было не больше 8 примеров.

[![результаты chain of thought](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Читайте больше в [полной статье](https://arxiv.org/abs/2201.11903).

#### Следствия

Преимущество few-shot перед `Let's think step by step` в том, что вы можете легче задать формат, длину и стиль рассуждений, который хотите получить до окончательного ответа. Это особенно полезно, если модель изначально рассуждает неправильно или поверхностно.

### Fine-tuned (дообучение)

#### Метод

В целом, чтобы добиться максимума на задаче, нужно дообучать кастомную модель. Но для дообучения через объяснения могут понадобиться тысячи пояснительных примеров, которые дорого создавать.

В 2022 году Эрик Зеликман, Юхуа Ву и др. предложили способ использовать few-shot запрос для генерации датасета с объяснениями, который пригодится для дообучения. Идея — при помощи few-shot запроса создавать варианты объяснений, оставлять только те, что приводят к правильному ответу. Для неправильных ответов повторять запрос, предоставляя правильный ответ в вопросе. Эта методика получила название STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуа Ву (2022)](https://arxiv.org/abs/2203.14465)

Так можно совместить плюсы fine-tune и цепочек рассуждений, избегая необходимости писать тысячи примеров.

#### Результаты

При применении к датасету вопросов здравого смысла STaR превзошёл как обычные цепочки рассуждений (73% > 37%), так и простое дообучение (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуа Ву (2022)](https://arxiv.org/abs/2203.14465)

Подробнее в [полной статье](https://arxiv.org/abs/2203.14465).

#### Следствия

Использование few-shot запроса для расширения или изменения датасета fine-tune — идея, которую можно обобщить. Если есть много неструктурированного текста, можно извлечь из него структурированный датасет с помощью запроса, а затем дообучить модель.

## Расширения цепочек рассуждений

Были предложены несколько расширений техники chain-of-thought prompting.

### Selection-inference prompting

#### Метод

Предложено Антонией Кресвелл и соавторами, идея — разбить единый запрос на две части: сначала запрос выбирает релевантные факты из текста ('selection prompt'), затем другой запрос делает выводы на базе выбранных фактов ('inference prompt'). Эти запросы чередуются в цикле, позволяя пройти несколько шагов рассуждений и получить итоговый ответ. Иллюстрация:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 миллиардами параметров selection-inference prompting существенно улучшил результаты по задачам bAbi и Proof Writer, требующим длинных последовательностей рассуждений. Лучший результат достигался сочетанием selection-inference и fine-tune.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Следствия

Хотя прирост больших, эти бенчмарки созданы специально для задач с длинными цепочками рассуждений. Для задач с короткими цепочками выигрыш будет меньше.

Основные выводы: разбивать сложную задачу на мелкие — хороший способ повысить надежность и производительность; и для максимума стоит использовать fine-tune вместе с выбранным подходом.

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Через несколько месяцев после публикации selection-inference техника была расширена, с идеями:

- как понять, когда цикл selection-inference остановить или продолжить
- как добавить value-функцию для поиска среди множественных траекторий рассуждений
- как снизить галлюцинации фиктивных фактов, дообучая модель для работы с метками предложений (например, sen1), а не с самим текстом

#### Метод

В исходной selection-inference технике поочерёдно применяются специализированные запросы 'selection' и 'inference' для выбора фактов и построения выводов, составляющих цепочку рассуждений.

Авторы добавляют два новых компонента.

Во-первых, «halter»-модель, которая после каждого шага рассуждения определяет, достаточно ли сделанных выводов для ответа. Если да — генерирует итоговый ответ.

Преимущества halter'а:

- может решать, когда процесс рассуждения остановить или продолжать
- если процесс не останавливается, нет ответа — что лучше, чем ошибочный

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавлена value-функция для оценки качества рассуждений и поиска среди разных вариантов. Это отражает общий принцип повышения надежности — вместо одного ответа создавать множество вариантов и отбирать лучший с помощью другой модели или функции.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, авторы применяют трюк для снижения галлюцинаций. Вместо того чтобы просить модель записывать фактические предложения, они дообучают модель работать с метками предложений (например, sen1), что помогает избежать выдумки фактов, не упомянутых в контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Оценивали технику на ProofWriter (не показано) и [EntailmentBankQA](https://allenai.org/data/entailmentbank) (показано). Техника существенно повышала точность, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Плюс манипуляция метками предложений практически устранила галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Следствия

В работе представлены полезные выводы для повышения надежности:

- Делите сложные задачи на более мелкие, надежные подзадачи
- Генерируйте ответ пошагово, оценивая его на каждом этапе
- Создавайте много вариантов ответов и выбирайте лучший с помощью другой модели или функции
- Снижайте галлюцинации, ограничивая то, что модель может сказать (например, используйте метки предложений)
- Максимизируйте производительность моделей, дообучая их под специальные задачи

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Цепочки рассуждений плохо работают с длинными цепочками рассуждений (где помогает selection-inference) и особенно с короткими примерами, но длинной задачей.

#### Метод

Least-to-most prompting — ещё один способ разбивать задачи на более мелкие, надежные. Идея в том, чтобы запросить у модели сначала подзадачу (например `To solve {question}, we need to first solve: "`), а затем решить её и добавить решение к вопросу, повторяя, пока не будет получен окончательный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На бенчмарках с длинными цепочками рассуждений и моделью `code-davinci-002` (оптимизированной для кода, но понимающей текст) были зафиксированы приросты с 16% до 99.7%!

[
![Least-to-most prompting на task last-letter-concatenation](/images/least-to-most_tab4.png)
![Least-to-most prompting на SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Следствия

Хотя приросты впечатляют, они получены на узком наборе задач с длинными цепочками. Тем не менее, это иллюстрирует главную идею: повысить надежность, (а) разбивая сложные задачи на подзадачи, и (б) давая модели больше времени и пространства для решения.

Подробнее в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Maieutic prompting

#### Метод

В отличие от предыдущих техник, которые стараются максимизировать вероятность правильного ответа, другой подход — попросить GPT-3 построить дерево возможных объяснений (правильных и неправильных), затем проанализировать их взаимосвязи, чтобы определить самый правильный набор. Такой метод назвали маиутическим prompting-ом [Джэхун Чжун и др. в мае 2022](https://arxiv.org/abs/2205.11822) (маиутика — метод Сократа с вопросами для выявления идей).

Метод сложен и работает так:

- Сначала строится маиутическое дерево, где каждая вершина — утверждение, которое может быть истинным или ложным:
  - Начинается с вопроса с вариантами или утверждения True/False (например, `War cannot have a tie`)
  - Для каждого варианта с помощью модели генерируется объяснение (запрос подобен `War cannot have a tie? True, because`)
  - Затем модель запрашивается с вопросом и объяснением, и подготовлен ответ. Если инвертирование объяснения (с префиксом `It is wrong to say that {explanation}`) инвертирует ответ — объяснение считается «логически целостным»
  - Если объяснение не логически целостно, процедура повторяется рекурсивно, превращая объяснение в новый True/False вопрос и генерируя новые объяснения
  - В итоге получается дерево объяснений, где у каждого листа выполнение условия: инверсия объяснения инвертирует ответ модели
- Второй этап — превращение дерева в граф связей:
  - Для каждой вершины вычисляется вероятность веры модели в неё (из вероятности правильного ответа `True` для данного объяснения)
  - Для каждой пары узлов модель определяет, являются ли они взаимными следствиями или противоречиями
- Третий этап — поиск максимально согласованного набора верований:
  - Задача формулируется как взвешенная задача максимальной удовлетворимости (MAX-SAT)
  - Решатель ищет самый самосогласованный набор верований, который принимается за истину

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Чжун и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты Maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Чжун и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Следствия

Метод сложен и, похоже, применим лишь к вопросам с вариантами ответов.

Подробнее в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с дискретным набором ответов простой способ повысить надежность — взять несколько объяснений и ответов от модели (с положительной температурой), а потом выбрать самый частый ответ.

[![метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Данный приём повысил точность на 1–24 процентных пункта на ряде задач по математике и рассуждениям. (На нижеприведённом графике — результаты модели Google LaMDA; у более крупной Google PaLM исходный уровень выше, но прирост чуть меньше.)

[![Результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Следствия

Хотя метод простой, он затратный. Создание 10 вариантов ответов увеличит расходы в 10 раз.

Также, как и многие техники, он подходит только для задач с ограниченным набором ответов. Для открытых задач (например, написание стихотворения), где ответ уникален, непонятно, что значит выбрать «самый частый».

Наконец, этот метод полезен, когда есть несколько вариантов или формулировок ответа; если путь один — он не поможет. Экстремальный пример: при генерации одиночного токена выбор самого частого из 100 вариантов равносилен выбору токена с наивысшей вероятностью (который можно получить одним прогоном при temperature=0).

### Verifiers

Другой ключевой метод — обучить модель-дискриминатор или проверяющую модель, которая оценивает выходные данные основной генеративной модели. Если дискриминатор отвергает ответ, генеративную модель запускают заново для другого варианта. Во многих случаях оценить ответ проще, чем создать, что объясняет эффективность этого подхода.

#### Метод

В 2021 году исследователи OpenAI применили технику к школьным задачам по математике, использовав такой подход:

- Сначала дообучили модель на паре вопросы-решения
- Для каждой задачи в обучающем наборе сгенерировали 100 решений
- Каждое из них автоматически отмечалось как верное или ошибочное по правильности итогового ответа
- Обучили модель-проверяющую классифицировать решения на правильные и неправильные
- На тесте генеративная модель создаёт 100 решений, и выбирается ответ с наивысшей оценкой от проверяющей модели

[![метод verifier](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 на 175 млрд параметров и 8 000 примеров обучение проверяющей модели подняло точность в школьной математике с ~33% до ~55%.

[![Результаты verifier](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Следствия

Как и self-consistency, этот метод дорогой — 100 решений на задачу увеличивают расходы примерно в 100 раз.

## Теории надежности

Хотя описанные техники разнятся по подходам, всех их объединяет цель — повысить надежность на сложных задачах. Обычно они:

- разбивают ненадежные операции на более надежные (например, selection-inference prompting)
- используют множество шагов или связей, чтобы сделать систему надежнее каждого компонента по отдельности (например, maieutic prompting)

### Вероятностные графовые модели

Подход разбиения системы из ненадежных компонентов на более надежные напоминает вероятностное программирование, и анализы этого поля применимы и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и др. трактуют вышеописанные техники в парадигме вероятностных графовых моделей:

#### Цепочка рассуждений (chain of thought prompting)

[![графовая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Fine-tuned chain of thought prompting / Self-taught reasoner

[![графовая модель fine-tuned chain of thought](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Verifiers

[![графовая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Следствия

Хотя формулировка техник в виде графовых моделей может не помочь прямо решить конкретную задачу, этот подход может помочь выбрать, комбинировать и придумать новые техники.

## Итоговые мысли

Исследования больших языковых моделей идут очень активно и быстро развиваются. Не только модели улучшаются, но и растёт понимание, как их лучше использовать. Чтобы осознать скорость развития, все статьи, упомянутые выше, опубликованы за последние 12 месяцев (на момент написания — сентябрь 2022).

В будущем ждите более мощных моделей и лучших техник. Даже если эта подборка утратит актуальность, общие принципы в ней останутся ключевыми элементами арсенала опытного пользователя.

## Библиография

| Урок                                                                                                                         | Статья                                                                                                                                   | Дата     |
| ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Делите сложные задачи на простые подзадачи (подумайте о выводе промежуточных результатов для пользователей)                  | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Можно улучшить результат, создав много кандидатов и выбрав лучший                                                               | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Октябрь 2021 |
| Для задач рассуждений модели работают лучше, когда они рассуждают шаг за шагом до ответа                                         | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Январь 2022 |
| Трёхкратное повышение решения задач достигается генерацией множества объяснительных ответов и выбором самого популярного ответа | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Март 2022 |
| Можно дообучить шаг-за-шагом рассуждающую модель, используя только данные с вопросами и ответами с вариантами                   | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Март 2022 |
| Метод рассуждений шаг за шагом отлично работает без примеров                                                                   | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Можно улучшить рассуждения, чередуя запросы «выбери» и «сделай вывод»                                                           | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных задач можно улучшить рассуждения, разбивая проблему на части и решая их поэтапно                                     | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022 |
| Можно заставить модель анализировать и правильные, и ложные объяснения, чтобы найти наиболее последовательный их набор          | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Все эти техники можно представить в терминах вероятностного программирования, где система — набор ненадежных компонентов        | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022 |
| Можно устранить галлюцинации с помощью меток предложений и уменьшить ошибки с помощью «halter»-промпта                          | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Август 2022 |