---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, если GPT-3 не справляется с задачей?

- Поискать лучший промпт, который вызывает более надежные ответы?
- Вложиться в тысячи примеров для тонкой настройки собственной модели?
- Предположить, что модель не способна решить задачу, и перейти к следующей?

Простого ответа нет — всё зависит от ситуации. Однако если ваша задача включает логическое рассуждение или сложность, рассмотрите возможность использования техник из этой статьи для создания более надежных и высокоэффективных промптов.

## Почему GPT-3 не справляется с комплексными задачами

Если вас попросят перемножить 13 на 17, сразу ли у вас возникнет ответ? Для большинства из нас — скорее нет. Однако это не значит, что люди неспособны считать умножение двухзначных чисел. Секунда с листом бумаги — и нетрудно понять, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если дать GPT-3 слишком сложную задачу, которую нельзя выполнить за время вычисления следующего токена, модель может "сболтнуть" неверный ответ. Но, как и у людей, это не обязательно означает, что модель неспособна решить эту задачу. При наличии некоторого времени и пространства для рассуждения она все еще может ответить надежно.

Например, если задать &lt;&lt;&lt;INL_0>>> следующую задачу по математике про жонглирование мячами, он ответит неправильно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Означает ли это, что GPT-3 не справляется с простыми математическими задачами? Нет, наоборот, оказывается, что, подготовив модель с помощью &lt;&lt;&lt;INL_1>>>, она решит задачу надежно:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно утверждать, что трюк &lt;&lt;&lt;INL_2>>> работает всегда или просто повезло на конкретной задаче. Но он действительно работает. На эталоне задач на слова с арифметикой трюк &lt;&lt;&lt;INL_3>>> значительно повысил процент решений GPT-3 с бесполезных 18% до достойных 79%!

## Способности модели зависят от контекста

При работе с GPT-3 распространенной ошибкой является представление, что ее способности фиксированы для всех контекстов. Например, если GPT-3 ошибается в простом логическом вопросе, значит, она неспособна к простой логике.

Но, как показано на примере &lt;&lt;&lt;INL_4>>>, видимые неудачи GPT-3 иногда исправляются лучшим промптом, который помогает модели направить себя к правильному ответу.

## Как повысить надежность на сложных задачах

Далее в статье представлены техники для повышения надежности больших языковых моделей на сложных задачах. Хотя некоторые техники специфичны для определенных типов задач, многие основаны на общих принципах, применимых к широкому кругу задач, например:

- Давать более понятные инструкции
- Разбивать сложные задачи на более простые подзадачи
- Структурировать инструкцию, чтобы удержать модель на задаче
- Предлагать модели объяснять перед ответом
- Просить обоснования множества возможных ответов, а затем синтезировать
- Генерировать множество вариантов, а затем попросить модель выбрать лучший
- Тонко настраивать кастомные модели для максимальной производительности

## Разбивайте сложные задачи на простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на более простые части.

Например, рассмотрим задачу, в которой модели задают вопрос с несколькими вариантами ответов по тексту — в данном случае игрой Clue. При прямом вопросе &lt;&lt;&lt;INL_5>>> не может связать вместе подсказки 3 и 5 и дает неправильный ответ:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 устанавливают, что полковник Мустанг был единственным в обсерватории и что у человека в обсерватории был подсвечник, модель не соединяет эти факты в правильный ответ (а) Да.

Однако вместо прямого запроса ответа мы можем разбить задачу на три части:

- Сначала пройтись по подсказкам по одной и проверить, может ли каждая быть релевантной
- Затем объединить соответствующие подсказки для рассуждения и ответа на вопрос
- Наконец, написать итоговый ответ: (а), (б) или (в)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Давая модели больше времени и пространства для размышлений и направляя её по плану рассуждения, она справляется и находит правильный ответ (а) Да.

Еще одно преимущество разбиения сложных инструкций на более мелкие подзадачи — это помогает модели сосредоточиться на каждой отдельной подзадаче.

Например, если попросить &lt;&lt;&lt;INL_6>>> сделать резюме текста на исходном языке, модель может вернуться к английскому:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Однако если сначала попросить ее определить язык текста, а затем резюмировать, надежность повышается:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просите модель объяснять, прежде чем отвечать

Еще одна мощная техника повышения надежности — заставить модель постепенно рассуждать, а не сразу выдавать финальный ответ. При "голосе вслух" вероятность получить правильный ответ значительно повышается.

### Zero-shot

#### Метод

Опубликованная [Такеши Кодзима и соавторами в 2022 году](https://arxiv.org/abs/2205.11916), самый простой способ заставить модель рассуждать — это просто добавлять в начале ответов &lt;&lt;&lt;INL_7>>>. На рисунке 2 приведен пример:

[![пример размышлений zero-shot](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применив этот трюк к набору задач MultiArith, авторы обнаружили, что точность &lt;&lt;&lt;INL_8>>> выросла в 4 раза — с 18% до 79%!

[![пример размышлений zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк &lt;&lt;&lt;INL_9>>> хорошо работает на математических задачах, он не универсален. Авторы отметили, что он больше помогает в многошаговых арифметических задачах, задачах символического рассуждения, стратегических и других задачах рассуждений. Он не помогает с простыми арифметическими задачами или вопросами здравого смысла и, предположительно, не поможет с многими немыслями задачами.

[![пример размышлений zero-shot](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее — в [полной статье](https://arxiv.org/abs/2205.11916).

Если вы применяете эту технику к собственным задачам, не бойтесь экспериментировать с настройкой инструкции. &lt;&lt;&lt;INL_10>>> довольно общий, поэтому вы можете добиться лучшей производительности при более жестком формате, адаптированном к вашим кейсам. Например, можно попробовать более структурированные варианты вроде &lt;&lt;&lt;INL_11>>>. Также можно дать модели пример формата, чтобы помочь ей придерживаться правильного пути, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Few-shot примеры

#### Метод

Заставлять модель рассуждать можно многими способами. Один из них — демонстрация с помощью нескольких примеров («few-shot»), как изучали [Джейсон Вэй и Денни Джоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Дополнительные человеческие демонстрации цепочек рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Отметим, что ставится под вопрос, плавают ли груши на самом деле)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При проверке на школьных математических задачах авторы обнаружили, что цепочка рассуждений увеличила количество решений в 3 раза — с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Помимо математических задач, цепочка рассуждений повысила эффективность по вопросам из спорта, отслеживанию результатов подбрасывания монетки и конкатенации последних букв. В большинстве случаев для насыщения прироста производительности было достаточно нескольких примеров (около 8 или меньше).

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее — в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Одним из преимуществ few-shot подхода по сравнению с &lt;&lt;&lt;INL_12>>> является возможность точнее задать формат, длину и стиль рассуждений, которые вы хотите получить до финального ответа. Это особенно полезно, когда модель не рассуждает в нужной манере или недостаточно глубоко.

### Тонкая настройка

#### Метод

Для достижения максимальной производительности в общем случае нужно тонко настраивать собственную модель. Однако тонкая настройка с использованием объяснений может потребовать тысячи примеров, что дорого.

В 2022 году Эрик Зеликман, Юхуаи Ву и другие предложили умный метод, используя few-shot промпты для генерации датасета объяснений для тонкой настройки модели. Идея — сгенерировать кандидаты объяснений с few-shot промптом и оставить только те, что ведут к правильному ответу. Для неправильных ответов повторять промпт, теперь включая правильные ответы в вопрос. Авторы назвали процедуру STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

С этой техникой можно сочетать преимущества тонкой настройки и цепочек рассуждений без необходимости писать тысячи объяснений.

#### Результаты

При применении к датасету вопросов здравого смысла STaR превзошел как цепочку рассуждений (73% > 37%), так и тонкую настройку (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Подробнее — в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Идея использовать few-shot для расширения или модификации тренировочного датасета тонкой настройки универсальна. Например, имея большой неструктурированный текст, можно использовать промпт для выделения структурированных данных, а затем тонко настроить модель на них.

## Расширения цепочки рассуждений

Опубликовано несколько расширений цепочки рассуждений.

### Selection-inference prompting

#### Метод

Опубликованный Антонией Кресвелл и соавторами, один из подходов — разделить генерацию объяснений и ответов на части. Сначала промпт выбирает релевантный набор фактов из текста ("selection prompt"). Затем второй промпт делает вывод на основе выбранных фактов ("inference prompt"). Эти промпты чередуются для создания многошаговых рассуждений и конечного ответа. Авторы иллюстрируют идею на рисунке:

[![selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 млрд параметров выборочно-последовательный промпт показал серьезное улучшение относительно стандартной цепочки рассуждений на задачах bAbi и Proof Writer (требующих длинных цепочек дум). Лучший результат достигался с комбинированием selection-inference prompting и тонкой настройки.

[![selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя прирост на этих эталонах большой, они выбраны именно за требование длинных цепочек рассуждений. На задачах без множества шагов эффект меньше.

В целом, итоговые уроки:

- Делить сложные задачи на более мелкие улучшает надежность и результат
- Максимальная производительность часто достигается сочетанием тонкой настройки и выбранного подхода

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Спустя несколько месяцев после публикации selection-inference prompting авторы расширили технику:

- добавили способ определять, когда остановить цикл выбора и вывода
- добавили value-функцию для поиска среди нескольких путей рассуждений
- уменьшили галлюцинации выдуманных фактов с помощью тонкой настройки модели рассуждать о метках предложений (например, sen1), а не записывать предложения напрямую

#### Метод

В оригинальной технике prompt'ы выбора и вывода чередуются, создавая цепочку рассуждений.

Теперь добавлены две компоненты.

Во-первых, «halter» — модель, которая после каждого шага вывода решает, достаточно ли рассуждений для ответа. Если да, генерируется финальный ответ.

Преимущества halter:

- может останавливать или продолжать процесс выбора-вывода
- если процесс не останавливается, ответа нет — что лучше, чем галлюцинация

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, value-функция, оценивающая качество рассуждений и осуществляющая поиск среди множества путей. Это отражает общую идею: вместо одного ответа создать множество, а затем выбрать лучший с помощью функции оценки/дискриминатора/верификатора.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, для сокращения галлюцинаций фактов модель дообучается работать с метками предложений, а не с самими предложениями, что предотвращает "вымывку" из области промпта выдуманных фактов.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Технику протестировали на ProofWriter (не показан) и [EntailmentBankQA](https://allenai.org/data/entailmentbank) (показан). Точность возросла существенно, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

При этом трюк с метками предложений свел галлюцинации практически к нулю!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Статья дает несколько полезных советов по повышению надежности больших языковых моделей:

- Делите сложные задачи на более надежные подзадачи  
- Генерируйте ответ по шагам, оценивая качество по ходу  
- Создавайте много вариантов и выбирайте лучший с помощью другой модели или функции  
- Сокращайте галлюцинации, ограничивая то, что модель может говорить (например, метками предложений вместо предложений)  
- Добивайтесь максимума производительности с помощью тонкой настройки на специализированных задачах

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Цепочки рассуждений особенно плохо работают с длинными цепочками (где selection-inference хорош), а также когда примеры короткие, а задача — длинная.

#### Метод

Least-to-most prompting — ещё одна техника разбивки задач на более мелкие и надежные подзадачи. Идея — получить подзадачу, запросив модель чем-то вроде &lt;&lt;&lt;INL_13>>>, а затем, имея подзадачу, сгенерировать решение. Решение добавляется к исходному вопросу, и процесс повторяется, пока не получен итоговый ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На сложных задачах с длинной цепочкой рассуждений и &lt;&lt;&lt;INL_14>>> (оптимизированным для кода, но понимающим текст) было зафиксировано улучшение с 16% до 99,7%!

[
![Least-to-most prompting результаты по задаче конкатенации последних букв](/images/least-to-most_tab4.png)
![Least-to-most prompting результаты по SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting результаты по числовому рассуждению DROP](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя результаты очень впечатляют, они получены на очень узком наборе задач, требующих длинных цепочек рассуждений.

Тем не менее, они повторяют общую идею: повысить надежность, (а) разбивая задачи на подзадачи и (б) давая модели больше времени и пространства для выработки ответа.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Маиевтическое промптинг

#### Метод

В отличие от предыдущих техник, которые пытаются максимизировать вероятность правильных ответов, другой подход — заставить GPT-3 создать дерево возможных объяснений (включая как правильные, так и неправильные), а затем проанализировать их взаимосвязи, чтобы угадать правильный набор объяснений. Название «maieutic prompting» предложили [Джехун Чжун и соавторы в мае 2022](https://arxiv.org/abs/2205.11822) (маиевтика — метод Сократа, когда задаются вопросы для выявления идей).

Метод сложный, и работает так:

- Сначала строим маиевтическое дерево, в котором каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинаем с вопроса с несколькими вариантами или утверждения «истина/ложь» (например &lt;&lt;&lt;INL_15>>>)
  - Для каждого варианта ответа используем модель, чтобы сгенерировать соответствующее объяснение (с промптом типа &lt;&lt;&lt;INL_16>>>)
  - Затем промптим модель с вопросом и объяснением, прося дать ответ. Если инверсия объяснения (с префиксом типа &lt;&lt;&lt;INL_17>>>) меняет ответ на противоположный, объяснение считается «логически цельным»  
  - Если объяснение не цельное, повторяем процесс рекурсивно, превращая каждое объяснение в вопрос Истина/Ложь и генерируя дальше объяснения для новых вопросов  
  - В конце имеем дерево объяснений, где каждая ветвь имеет свойство, что переворот объяснения инвертирует ответ модели  
- Второй шаг — превращаем дерево в граф отношений:
  - Для каждого узла рассчитываем относительную веру модели (по вероятности получить &lt;&lt;&lt;INL_18>>> при данном объяснении)
  - Для каждой пары узлов определяем в модели, являются ли они взаимонтекающими (выплывающими) или противоречащими  
- Третий шаг — ищем наиболее согласованный набор верований и считаем их истинными:
  - Используя силу веры и логические связи, формулируем задачу как задачу максимальной удовлетворимости с весами (MAX-SAT)  
  - Используем решение для нахождения максимально самосогласованного набора верований, принимая их за истинные  

[
![Маиевтическое промптинг](/images/maieutic_fig2.png)
![Маиевтическое промптинг](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты маиевтического промптинга](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Кроме сложности, ограничение метода в том, что он, похоже, применим только к вопросам с несколькими вариантами.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с дискретным набором ответов простой способ повысить надежность — сэмплировать множество объяснений и ответов из модели (с положительной температурой), а затем выбрать финальный ответ, который встречается чаще всего.

[![метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Эта техника повысила точность от 1 до 24 процентных пунктов на ряде эталонов для математики и рассуждений. (Ниже показаны результаты модели LaMDA от Google; при использовании более масштабной PaLM базовые показатели были выше, а прирост — чуть скромнее.)

[![результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя техника проста, она может быть дорогой: генерация 10 ответов увеличит ваши затраты в 10 раз.

Кроме того, как и многие техники, она применима только к задачам с ограниченным набором ответов. Для открытых задач с уникальными ответами (например, написание стихотворения) непонятно, что значит выбрать самый частый ответ.

И наконец, техника наиболее полезна, если есть несколько путей или формулировок ответа. Если путь только один, польза минимальна. Например, если задача — сгенерировать один токен, то выбор самого частого токена из 100 генераций ничем не лучше выбора токена с максимальным логарифмом вероятности (что можно получить одним запросом с temperature=0).

### Верификаторы

Еще одна ключевая техника повышения производительности — обучить верификатор или дискриминатор, который оценивает выходы основной генеративной модели. Если дискриминатор отклоняет ответ, можно сгенерировать заново. Во многих случаях судить ответ проще, чем создавать, что объясняет силу этого метода.

#### Метод

В 2021 году исследователи OpenAI применили эту технику к школьным задачам по математике, таким образом:

- Сначала тонко настроили модель на вопросы и решения
- Для каждой задачи из тренировочного набора сгенерировали 100 решений
- Каждое решение автоматически размечалось как правильное или неправильное в зависимости от правильности итогового ответа
- Использовали эти размеченные решения для дообучения модели-верификатора, который классифицировал правильность решения к экзаменационному вопросу
- При тестировании генеративная модель создавала 100 решений для каждой задачи, а финальным ответом выбрано решение с максимальным баллом от верификатора

[![метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С моделью GPT-3 175B и 8 тыс. обучающих примеров эта техника существенно подняла точность в школьной математике с ~33% до ~55%.

[![результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и self-consistency, техника может быть дорогой: генерация 100 решений на одну задачу увеличит затраты примерно в 100 раз.

## Теории надежности

Несмотря на разнообразие подходов, все техники объединяет цель — повысить надежность на сложных задачах. В основном это достигается путём:

- разбиения ненадежных операций на более надежные подзадачи (например, selection-inference prompting)  
- использования нескольких шагов и цепочек отношений для повышения надежности системы сверх надежности отдельных компонентов (например, maieutic prompting)

### Вероятностные графовые модели

Данный подход по сборке надежной системы из менее надежных компонентов напоминает вероятностное программирование; многие методы анализа из той области применимы и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и соавторы рассматривают вышеупомянутые техники в парадигме вероятностных графовых моделей:

#### Цепочка рассуждений

[![графовая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Тонко настроенная цепочка рассуждений / Self-taught reasoner

[![графовая модель тонко настроенной цепочки рассуждений](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![графовая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формализация в виде вероятностных графовых моделей не обязательно поможет сразу решать конкретные задачи, этот фреймворк может быть полезен для выбора, объединения и открытия новых техник.

## Заключение

Исследования в области больших языковых моделей очень активны и быстро развиваются. Исследователи не только продолжают улучшать модели, но и лучше понимают, как их эффективно использовать. Чтобы подчеркнуть скорость развития, все упомянутые статьи опубликованы за последние 12 месяцев (на момент написания в сентябре 2022).

В будущем ожидайте появления более совершенных моделей и техник. Даже если конкретные техники здесь будут заменены будущими практиками, общие принципы останутся важной частью инструментария любого эксперта.

## Библиография

| Урок                                                                                                                         | Статья                                                                                                                                 | Дата     |
| ---------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на подзадачи (и рассматривайте возможность показывать промежуточные результаты пользователям)       | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт. 2021 |
| Можно улучшить результат, сгенерировав много кандидатов и выбрав лучший                                                       | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                      | Окт. 2021 |
| На задачах рассуждений модели лучше отвечают, рассуждая по шагам                                                            | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                               | Янв. 2022 |
| Можно улучшить рассуждения по шагам, сгенерировав много объяснений и выбрав самый популярный ответ                           | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                             | Мар. 2022 |
| Для тонкой настройки пошагового рассуждателя достаточно использовать данные с вопросами и вариантами ответов               | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                        | Мар. 2022 |
| Пошаговая рассуждения эффективна даже без примеров                                                                          | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                       | Май 2022 |
| Можно улучшить пошаговое рассуждение, чередуя промпты «выбора» и «вывода»                                                    | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)           | Май 2022 |
| Для длинных задач рассуждений можно улучшить результат, решая задачи поэтапно, решая части                                   | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                        | Май 2022 |
| Модель может анализировать хорошие и плохие объяснения, чтобы найти наиболее согласованный набор                             | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                    | Май 2022 |
| Эти техники можно рассматривать с точки зрения вероятностного программирования, представленного системами из ненадежных компонентов | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                             | Июль 2022 |
| Можно устранить галлюцинации с помощью меток предложений и уменьшить ошибочные ответы с помощью промпта 'halter'            | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                    | Авг. 2022 |