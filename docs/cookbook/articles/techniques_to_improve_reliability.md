---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надёжности

Что делать, когда GPT-3 не справляется с задачей?

- Искать лучший промпт, который даст более надёжные ответы?
- Инвестировать в тысячи примеров, чтобы дообучить кастомную модель?
- Предположить, что модель не способна выполнить задачу, и перейти к другой?

Простого ответа нет — всё зависит от ситуации. Однако, если задача требует логического рассуждения или имеет сложную структуру, стоит попробовать техники из этой статьи, чтобы создать более надёжные и высокоэффективные промпты.

## Почему GPT-3 не справляется со сложными задачами

Если вас попросят умножить 13 на 17, сразу придёт ли вам ответ? Большинству людей — скорее нет. Но это не значит, что люди не могут умножать двузначные числа. Несколько секунд и лист бумаги — и можно легко вычислить, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если поставить GPT-3 задачу слишком сложную, чтобы решить её за время генерации следующего токена, модель может сгенерировать неправильный ответ. Но, как и у людей, это не означает, что модель не способна решить задачу. При наличии времени и возможности рассуждать модель всё же может дать правильный ответ.

Например, если задать &lt;&lt;&lt;INL_0>>> следующую задачу по математике о жонглировании мячами, он отвечает неправильно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые задачи по математике? Нет; на самом деле, если предложить модели &lt;&lt;&lt;INL_1>>>, она решит задачу надёжнее:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно сказать, работает ли этот &lt;&lt;&lt;INL_2>>> приём вообще или просто повезло в этот раз. Но он действительно работает. На тесте математических задач на слова &lt;&lt;&lt;INL_3>>> повысил точность GPT-3 с 18% (практически бесполезно) до 79% — очень впечатляющий результат!

## Возможности модели зависят от контекста

Одной из частых ошибок при работе с GPT-3 является предположение, что его способности фиксированы во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит он не умеет решать простую логику.

Но как показывает пример &lt;&lt;&lt;INL_4>>>, очевидные ошибки GPT-3 иногда устраняются более продуманным промптом, который помогает модели переходить к правильному результату.

## Как повысить надёжность на сложных задачах

Далее в статье рассказываются техники повышения надёжности больших языковых моделей для сложных задач. Хотя некоторые техники специфичны для определённых типов задач, многие основаны на общих принципах применимых к широкому спектру:

- Давать более чёткие инструкции  
- Делить сложные задачи на более простые подзадачи  
- Структурировать инструкцию, чтобы удерживать модель в пределах задачи  
- Просить модель объяснить решение перед ответом  
- Запрашивать обоснования множества возможных ответов и затем синтезировать  
- Генерировать много вариантов и просить модель выбрать лучший  
- Дообучать кастомные модели для максимальной эффективности  

## Делите сложные задачи на более простые

Чтобы дать модели больше времени и пространства для размышлений, разбивайте задачи на более простые части.

Например, рассмотрим задачу, где нужно ответить на вопрос с несколькими вариантами ответа, основанную на тексте — в данном случае, игре Clue. При прямом запросе &lt;&lt;&lt;INL_5>>> не может связать подсказки 3 и 5 и отвечает неправильно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 указывают, что полковник Мастерд был единственным человеком в обсерватории и у него был подсвечник, модель не объединяет их в правильный ответ (a) Да.

Вместо прямого вопроса можно разделить задачу на три этапа:

- Сначала пройти по подсказкам по очереди и определить, могут ли они быть важны  
- Затем объединить релевантные подсказки и логически вывести ответ на вопрос  
- В конце написать окончательный ответ: (a), (b) или (c)  

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Давая модели больше времени и направляя ход рассуждений, она смогла найти правильный ответ (a) Да.

Ещё один плюс разбиения инструкций на мелкие подзадачи — это помогает модели сосредоточиться на каждой отдельной части.

Например, если попросить &lt;&lt;&lt;INL_6>>> сделать краткое содержание текста на исходном языке, модель может переключиться обратно на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Зато если сначала попросить модель определить язык текста, а потом сделать краткое содержание, она будет работать надёжнее:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просите модель объяснять перед ответом

Ещё один мощный приём — заставить модель постепенно рассуждать вслух, а не сразу выдавать ответ. Такой «проговаривание» увеличивает вероятность правильного решения.

### Zero-shot

#### Метод

Опубликованный [Такеши Кодзимой и соавторами в 2022 году](https://arxiv.org/abs/2205.11916) простой способ заставить модель рассуждать — это просто добавлять в начало ответов &lt;&lt;&lt;INL_7>>>. На рисунке 2 показан пример:

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022)](https://arxiv.org/abs/2205.11916)

#### Результаты

Применив этот простой приём к набору задач MultiArith по математике, авторы выявили, что &lt;&lt;&lt;INL_8>>> увеличил точность в четыре раза — с 18% до 79%!

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022)](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя &lt;&lt;&lt;INL_9>>> хорошо работает на арифметических задачах с несколькими шагами, он неэффективен для всех задач. Авторы обнаружили, что он полезен для многоступенчатых арифметических задач, задач символического рассуждения, стратегий и других задач на рассуждение. Он не помог простым математическим задачам или задачам с повседневным здравым смыслом и, вероятно, не поможет во многих других нерешающих задачах.

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кодзима и др. (2022)](https://arxiv.org/abs/2205.11916)

Подробнее в [полной статье](https://arxiv.org/abs/2205.11916).

Если применяете этот приём к своим задачам, не бойтесь экспериментировать с формулировкой инструкции. &lt;&lt;&lt;INL_10>>> довольно общая, поэтому вы можете добиться лучшего результата, создав более строгий формат, заточенный под вашу задачу. Например, можно попробовать более структурированные варианты, как &lt;&lt;&lt;INL_11>>>. А можно даже дать модели пример формата, чтобы она не сбивалась с курса, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Few-shot примеры

#### Метод

Побуждение модели рассуждать можно задавать по-разному. Один из способов — показать несколько примеров («few-shot»), что изучали [Джейсон Вэй и Дэнни Чжоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё демонстрации цепочек рассуждений, составленных людьми:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Заметьте, что было поставлено под сомнение, действительно ли груши плавают)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При тестировании на школьных задачах по математике авторы обнаружили, что цепочка рассуждений повысила точность решения с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочка рассуждений повысила эффективность ответов по задачам, связанным со спортом, отслеживанием бросков монеты и объединением последних букв. Большинство эффектов достигается с небольшим количеством примеров (около 8 или меньше).

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу и др. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество few-shot подхода по сравнению с &lt;&lt;&lt;INL_12>>> в том, что вы можете точнее задавать формат, длину и стиль рассуждений, которые хотите получить от модели перед окончательным ответом. Это особенно полезно, когда изначально модель рассуждает недостаточно глубоко или неправильно.

### Дообученная модель

#### Метод

Чтобы добиться максимальной эффективности в задаче, обычно нужно дообучать кастомную модель. Но для обучения модели с объяснениями требуется тысячи примеров, которые долго и дорого собирать.

В 2022 году Эрик Зеликман и Юхуаи Ву и соавторы опубликовали хитрый метод использования few-shot промптов для генерации набора данных объяснений, пригодных для дообучения модели. Идея в том, чтобы использовать few-shot промпт для генерации кандидатов на объяснения, и отбирать лишь те, которые приводят к правильному ответу. Чтобы получить дополнительные объяснения для неправильных ответов, запускают ещё раз few-shot промт, но уже с правильным ответом, включённым в вопрос. Этот метод назвали STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуаи Ву и др. (2022)](https://arxiv.org/abs/2203.14465)

С помощью этого метода вы объединяете преимущества fine-tuning с преимуществами chain-of-thought, не тратясь на тысячи объяснений.

#### Результаты

Применяя метод к датасету вопросов на здравый смысл, авторы обнаружили, что STaR превзошёл как одни цепочки рассуждений (73% > 37%), так и чистый fine-tuning (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуаи Ву и др. (2022)](https://arxiv.org/abs/2203.14465)

Подробнее в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot промпта для расширения или уточнения обучающего датасета – идея применимая не только к объяснениям. Если у вас много неструктурированного текста, можно попробовать выдавать модели промпты, чтобы она превращала их в структурированный датасет, и затем дообучать модель на нём.

## Расширения цепочки рассуждений

Опубликовано несколько расширений техники chain-of-thought.

### Selection-inference prompting

#### Метод

Опубликовано Антонией Кресвелл и др., одно из расширений техники цепочки рассуждений — разделение промпта на меньшие части. Сначала промпт выбирает релевантный поднабор фактов из текста («selection prompt»). Потом второй промпт делает вывод на основании выбранных фактов («inference prompt»). Эти промпты чередуются, чтобы генерировать несколько шагов логики и в итоге получить ответ. Иллюстрация идеи:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

При работе с моделью 7B параметров авторы обнаружили, что selection-inference substantially улучшает результаты по сравнению с chain-of-thought на задачах bAbi и Proof Writer (где требуются длинные цепочки рассуждений). Лучший результат был при сочетании selection-inference и дообучения.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя улучшения на этих бенчмарках были значительны, их специально выбирали из-за требований к длинным цепочкам рассуждений. На задачах с небольшим количеством логических шагов эффекты могут быть меньше.

Общие уроки:  
- Деление сложных задач на более мелкие повышает надёжность и качество — чем атомарнее задача, тем меньше ошибок  
- Максимальная эффективность обычно достигается путём сочетания fine-tuning и выбранного подхода  

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Несколько месяцев спустя после публикации selection-inference, авторы выпустили продолжение с идеями:

- когда останавливать или продолжать цикл selection-inference  
- добавить value function для поиска лучшего из множества путей рассуждений  
- уменьшить галлюцинации, дообучив модель работать с метками предложений (например, sen1), а не писать сами предложения  

#### Метод

В изначальной selection-inference подходе чередуются специальные «selection» и «inference» промпты для выбора фактов и вывода из них, создавая цепочку рассуждений.

Авторы расширили приём двумя компонентами.

Сначала ввели модель «halter», которая после каждого шага вывода спрашивает, достаточно ли сделанных выводов для ответа. Если да — генерируется финальный ответ.

Преимущества halter:  
- может останавливать или продолжать процесс selection-inference по необходимости  
- если процесс не останавливается, ответа не будет, что лучше, чем ложный ответ  

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Далее добавили value function — она оценивает качество шагов рассуждений и помогает искать лучший из множества вариантов. Это отражает общий принцип повышения надёжности: вместо одного ответа создавать множество, а затем выбирать лучший с помощью функции оценки / дискриминатора / верификатора.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, авторы применили трюк, чтобы уменьшить галлюцинации — дообучили модель на метках предложений, а не на самих предложениях, что помогает предотвратить появление «фейковых» фактов, не содержащихся в исходном контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы оценили методику на задачах ProofWriter (не показано) и EntailmentBankQA (показано). Методика существенно повышала точность, особенно на сложных задачах рассуждения.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Трюк с метками предложений практически полностью устранил галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Статья демонстрирует ряд полезных уроков по повышению надёжности моделей:

- Делите сложные задачи на более надёжные подзадачи  
- Генерируйте ответ поэтапно, оценивая его на ходу  
- Генерируйте множество ответов и используйте другую модель или функцию для выбора лучших  
- Снижайте галлюцинации, ограничивая, что может говорить модель (например, метками предложений вместо целых предложений)  
- Максимизируйте эффективность моделей через дообучение на специализированные задачи  

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Помимо слабых результатов на длинных цепочках рассуждений (где сильна selection-inference), chain-of-thought особенно плохо справляется, когда примеры короткие, а задача длинная.

#### Метод

Least-to-most prompting — другая техника разбиения задач рассуждений на более простые и надёжные подзадачи. Идея — попросить модель вывести подзадачу с помощью промпта вроде &lt;&lt;&lt;INL_13>>>. Затем, имея эту подзадачу, модель генерирует решение. Решение добавляется к исходному вопросу, и процесс повторяется, пока не получим финальный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На бенчмарках с длинными рассуждениями с использованием &lt;&lt;&lt;INL_14>>> (оптимизированного для кода, но понимающего и текст) улучшения доходили с 16% до 99.7%!

[
![Результаты least-to-most prompting на last-letter-concatenation](/images/least-to-most_tab4.png)
![Результаты least-to-most prompting на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most prompting на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя достижения впечатляют, они получены на очень ограниченном наборе задач, которые требуют длинных цепочек рассуждений.

Тем не менее они демонстрируют общую идею: повысить надёжность, (а) разбивая задачи на подзадачи и (б) давая модели больше времени и пространства для решения.

Подробнее в [полной статье](https://arxiv.org/abs/2205.10625).

## Родственные идеи

### Maieutic prompting

#### Метод

В отличие от предыдущих техник, максимизирующих вероятность правильного ответа, другой подход — использовать GPT-3 для генерации дерева возможных объяснений (как правильных, так и неправильных), а затем анализировать их взаимосвязи, чтобы найти наиболее правдоподобный набор. Этот метод назвали maieutic prompting [Джаехун Чжунг и др. в мае 2022](https://arxiv.org/abs/2205.11822) (maieutic — относящийся к сократическому методу вопросов).

Метод сложен и работает так:

- Сначала строится maieutic дерево, где каждый узел — утверждение, которое может быть истинным или ложным:  
  - Начинают с задачи с множественным выбором или с истинно/ложного утверждения (например, &lt;&lt;&lt;INL_15>>>)  
  - Для каждого варианта ответа используют модель, чтобы сгенерировать соответствующее объяснение (с помощью промпта вида &lt;&lt;&lt;INL_16>>>)  
  - Затем модель получает вопрос с объяснением и выдаёт ответ. Если обращение объяснения (с префиксом &lt;&lt;&lt;INL_17>>>) меняет ответ на противоположный — объяснение считается «логически целостным»  
  - Если объяснение не логически целостно, процесс повторяется рекурсивно, превращая объяснение в новый вопрос с True/False и генерируя новые объяснения  
  - В итоге получается дерево объяснений, в котором каждое «лицо» вызывает смену ответа при обращении объяснения  
- Затем дерево преобразуется в граф связей:  
  - Для каждого узла вычисляют относительную уверенность модели (на основе вероятности ответа &lt;&lt;&lt;INL_18>>> при данном объяснении)  
  - Для пар узлов определяется, подразумевает ли один другой или противоречит ему  
- Наконец, ищется наиболее согласованный набор убеждений:  
  - Используя силу уверенности и логические связи, формулируют задачу как задачу взвешенной максимальной выполнимости (MAX-SAT)  
  - С помощью решателя находят максимально согласованный набор убеждений и принимают их как истину  

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхьюн Чжунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхьюн Чжунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Помимо сложности, ограничение метода в том, что он подходит только для вопросов в формате множественного выбора.

Подробнее в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность (Self-consistency)

#### Метод

Для задач с дискретным набором ответов простой способ повысить надёжность — сгенерировать несколько объяснений и ответов (при положительной температуре), а затем выбрать ответ, который встречается чаще всего.

[![Метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Этот метод поднял точность от 1 до 24 процентных пунктов на наборе математических и логических задач. (На рисунках результаты модели LaMDA от Google; на более крупной модели PaLM базовые результаты были выше, а улучшения — чуть меньше.)

[![Результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя метод прост, он может быть дорогим: генерация 10 ответов увеличит затраты в 10 раз.

Также, как и у многих подходов, метод применим только к задачам с ограниченным набором ответов. Для открытых задач, где каждый ответ уникален (например, написание поэмы), непонятно, что значит выбирать самый частый ответ.

Ещё он полезен, когда возможных путей получения ответа несколько; если путём всего один, метод может не помочь. Крайний пример: если ответ — это один токен, то выбор наиболее частого токена из 100 генераций будет равносилен выбору токена с максимальной вероятностью из одной генерации при temperature=0.

### Верификаторы

Ещё один важный способ повысить качество работы — обучить модель-верификатор (дискриминатор), которая оценивает ответы основной генеративной модели. Если верификатор отклоняет результат, генерируют заново. Часто проще оценить ответ, чем создать его, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи OpenAI применили этот метод к школьным математическим задачам, используя такую схему:

- Сначала дообучили модель на вопросах и решениях  
- Для каждой задачи сгенерировали 100 решений  
- Каждое из решений автоматически маркировали как правильное или неправильное, исходя из правильности финального ответа  
- Дообучили верификатор на этом наборе решений для классификации правильных и неправильных  
- На тесте генерируют 100 решений и выбирают с лучшим баллом по верификатору  

[![Метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С моделью GPT-3 на 175B параметров и 8,000 обучающих примеров эта техника подняла точность с ~33% до ~55%.

[![Результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и self-consistency, этот подход может быть дорогим, поскольку 100 решений на задачу увеличивают затраты примерно в 100 раз.

## Теории надёжности

Хотя техники различаются, они все стремятся повысить надёжность в сложных задачах. Главным образом, они:

- разбивают ненадёжные операции на более простые и надёжные (например, selection-inference prompting)  
- используют множественные шаги или взаимосвязи, чтобы система была надёжнее любого отдельного компонента (например, maieutic prompting)  

### Вероятностные графовые модели

Этот подход к построению надёжной системы из менее надёжных компонентов напоминает вероятностное программирование, и многие методы анализа из этой области можно применить и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и др. рассматривают приведённые техники в терминах вероятностных графовых моделей:

#### Chain of thought prompting

[![графовая модель chain of thought prompting](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Fine-tuned chain of thought prompting / Self-taught reasoner

[![графовая модель fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Verifiers

[![графовая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя изложение техник как графовых моделей может быть не сразу полезным для решения конкретной задачи, этот подход помогает выбирать, комбинировать и открывать новые методы.

## Заключительные мысли

Исследования больших языковых моделей идут очень активно и быстро меняются. Учёные не только продолжают улучшать модели, но и лучше понимают, как правильно их использовать. Чтобы подчеркнуть скорость, все статьи, приведённые выше, опубликованы в последние 12 месяцев (на момент написания в сентябре 2022).

В будущем ожидайте новые, более совершенные модели и техники. Даже если конкретные подходы здесь утратят актуальность, общие принципы скорее всего останутся ключевой частью инструментария экспертов.

## Библиография

| Урок                                                                                                                          | Статья                                                                                                                                  | Дата      |
| ------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- | --------- |
| Делите сложные задачи на подзадачи (и рассматривайте возможность показывать промежуточные результаты пользователю)             | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Можно улучшать результаты, генерируя много кандидатов и выбирая лучший                                                        | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Октябрь 2021 |
| При задачах рассуждений модели лучше работают, когда рассуждают по шагам перед ответом                                          | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Январь 2022 |
| Можно улучшить пошаговое рассуждение генерируя много объяснений-ответов и выбирая самый популярный                             | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Март 2022 |
| Для дообучения пошагового решателя можно использовать только данные с вопросами и ответами множественного выбора              | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Март 2022 |
| Метод пошагового рассуждения работает отлично даже без примеров                                                                | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022  |
| Можно улучшить пошаговое рассуждение, чередуя промпты «selection» и «inference»                                                 | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022  |
| Для длинных рассуждений можно улучшить результаты, решая задачу пошагово, разбивая её на части                                   | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022  |
| Модель можно заставить анализировать хорошие и плохие объяснения, чтобы найти наиболее согласованное объяснение                | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022  |
| Можно рассматривать эти техники через призму вероятностного программирования, где система состоит из надёжных и ненадёжных компонентов | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022 |
| Улучшение надёжности достигается через метки предложений и использование промптов с остановкой ("halter")                      | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Август 2022 |