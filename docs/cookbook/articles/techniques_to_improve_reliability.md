---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, если GPT-3 не справляется с задачей?

- Искать лучший запрос, который вызовет более надежные ответы?
- Инвестировать в тысячи примеров для дообучения кастомной модели?
- Предположить, что модель неспособна решить задачу, и двигаться дальше?

Простого ответа нет — всё зависит от ситуации. Однако, если ваша задача связана с логическим рассуждением или высокой сложностью, попробуйте техники из этой статьи, чтобы создавать более надежные и эффективные запросы.

## Почему GPT-3 ошибается на сложных задачах

Если вас попросят умножить 13 на 17, сразу ли вам придёт ответ? Для большинства из нас — скорее нет. Но это не значит, что мы не способны на умножение двузначных чисел. С несколькими секундами и листом бумаги вполне реально вычислить, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 получить задачу слишком сложную, чтобы она могла выполнить её за время генерации следующего токена, модель может выдать неверный результат. Но, как и у людей, это не означает полной невозможности решения. С дополнительным временем и пространством для рассуждений модель может ответить корректно.

Например, если задать &lt;&lt;&lt;INL_0>>> такую математическую задачу о жонглировании мячами, он ответит неправильно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если использовать подсказку &lt;&lt;&lt;INL_1>>>, модель решает задачу корректно:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно судить, насколько общим является этот метод &lt;&lt;&lt;INL_2>>> — случайность это или действительно эффективно. Но он реально работает. На наборе задач по математике с текстовым описанием трюк &lt;&lt;&lt;INL_3>>> поднял эффективность GPT-3 с 18% (ненадёжно) до 79% (прилично)!

## Возможности модели зависят от контекста

Одна распространённая ошибка при работе с GPT-3 — считать, что способности модели одинаковы во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит он не умеет решать простую логику.

Но, как показывает пример &lt;&lt;&lt;INL_4>>>, на первый взгляд "ошибки" GPT-3 порой решаются улучшением запроса, который помогает модели корректно сориентироваться и выдать правильный результат.

## Как повысить надежность на сложных задачах

Далее в статье описаны техники повышения надежности больших языковых моделей при решении сложных задач. Некоторые методы ориентированы на определённые типы проблем, но многие построены на общих принципах и применимы в широком спектре задач, например:

- Давать более чёткие инструкции
- Разбивать сложные задачи на более простые подзадачи
- Структурировать инструкции, чтобы модель не сбивалась с темы
- Запрашивать объяснение перед конечным ответом
- Просить обоснования для нескольких вариантов ответа, а затем синтезировать их
- Генерировать множество вариантов и отбирать лучший с помощью модели
- Дообучать кастомные модели для максимальной производительности

## Разбивайте сложные задачи на более простые

Чтобы дать модели больше времени и пространства для размышлений, разбейте задачу на более простые части.

Например, возьмём вопрос с несколькими вариантами ответа по тексту — в данном случае игра Clue. При прямом запросе &lt;&lt;&lt;INL_5>>> не может совместить подсказки 3 и 5 и даёт неправильный ответ:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Подсказки 3 и 5 утверждают, что полковник Горчичный был единственным в обсерватории и у него был подсвечник, но модель не объединяет это в правильный ответ (a) Да.

Вместо того, чтобы спрашивать ответ напрямую, можно разбить задачу на три шага:

- Сначала пройтись по каждой подсказке и определить, релевантна ли она
- Затем объединить релевантные подсказки для рассуждения над ответом
- Наконец, написать окончательный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Дав больше времени на рассуждения и направляя модель по плану, она находит правильный ответ (a) Да.

Ещё одно преимущество разбиения сложных инструкций — помощь модели оставаться сосредоточенной на конкретной подзадаче.

Например, если попросить &lt;&lt;&lt;INL_6>>> сделать резюме текста на исходном языке, модель может перейти обратно на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Если же сначала попросить определить язык текста, а потом сделать резюме, надежность работы повышается:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Подсказывайте модели объяснять перед ответом

Другой мощный способ повысить надежность ответов — попросить модель постепенно рассуждать, а не сразу выдавать окончательный ответ. Размышляя "вслух", модель гораздо чаще приходит к правильному решению.

### Метод нулевого примера (zero-shot)

#### Метод

Опубликованный [Такеши Кожимой и др. в 2022](https://arxiv.org/abs/2205.11916) самый простой способ заставить модель рассуждать — просто добавлять &lt;&lt;&lt;INL_7>>> перед ответами. Пример приведён на рисунке 2:

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кожима и др. (2022)](https://arxiv.org/abs/2205.11916)

#### Результаты

Применяя этот простой приём к датасету MultiArith, авторы обнаружили, что &lt;&lt;&lt;INL_8>>> увеличивает точность в четыре раза — с 18% до 79%!

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кожима и др. (2022)](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя &lt;&lt;&lt;INL_9>>> хорошо работает для математических задач, он не эффективен для всех типов задач. Авторы отметили, что этот метод помогает в задачах с арифметикой в несколько шагов, символическом логическом рассуждении, стратегиях и других задачах требующих рассуждений. Для простых математических примеров и вопросов здравого смысла он не помогает, и предположительно не будет полезен для многих нерешающих задач.

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Такеши Кожима и др. (2022)](https://arxiv.org/abs/2205.11916)

Подробности см. в [полной статье](https://arxiv.org/abs/2205.11916).

Если вы применяете данный подход, не бойтесь экспериментировать с формулировкой инструкции. &lt;&lt;&lt;INL_10>>> — довольно универсальна, и вы можете добиться лучшей производительности, если использовать более строгие форматы, адаптированные под ваш сценарий. Например, можно попробовать более структурированные варианты вроде &lt;&lt;&lt;INL_11>>>. А можно и показать модели пример, чтобы она не сбивалась с пути, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Метод с несколькими примерами (few-shot)

#### Метод

Подсказки с демонстрацией рассуждений через несколько примеров ("few-shot") изучали [Джейсон Вэй и Дэнни Чжоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример запроса с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Другие примеры цепочек рассуждений, написанных разметчиками:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Примечание: есть споры, действительно ли груши плавают)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

На школьных математических задачах подсказки с цепочкой рассуждений увеличили процент решений в три раза — с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, этот метод повысил качество ответов на вопросы по спортивному пониманию, отслеживанию подбрасывания монеты и соединению последних букв. Обычно требуется меньше 8 примеров, чтобы достигнуть предела повышения производительности.

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вэй и Дэнни Чжоу (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее — в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество few-shot подхода перед &lt;&lt;&lt;INL_12>>> в том, что вы можете задавать формат, длину и стиль рассуждений, которые хотите получить до финального ответа. Это полезно, когда модель изначально не рассуждает в нужном стиле или с нужной глубиной.

### Дообученные модели

#### Метод

Для максимальной производительности обычно требуется дообучение кастомной модели. Но для обучения моделей на объяснениях могут понадобиться тысячи примеров, что дорого.

В 2022 Эрик Зеликман и Юхуай Ву придумали процедуру генерации набора данных объяснений с использованием few-shot подсказок, которые затем сохраняются, если дают правильный ответ. Для неудачных примеров процедура повторяется, но в вопросе указывается правильный ответ. Они назвали метод STaR (Self-taught Reasoner):

[![Процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуай Ву (2022)](https://arxiv.org/abs/2203.14465)

Так можно объединить преимущества дообучения с цепочками рассуждений без необходимости писать тысячи объяснений вручную.

#### Результаты

На датасете Common Sense Q&A STaR превзошёл как только цепочку рассуждений (73% > 37%), так и только дообучение (73% > 60%):

[![Результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхуай Ву (2022)](https://arxiv.org/abs/2203.14465)

Подробнее — в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot подсказок для расширения набора данных для дообучения можно применять и для других задач. Например, если у вас много неструктурированного текста, вы можете сгенерировать структурированный набор данных и дообучить модель для работы с ним.

## Расширения цепочки рассуждений

Были предложены различные расширения техники цепочек рассуждений.

### Подсказки с выбором и выводом (selection-inference prompting)

#### Метод

Опубликованный Антонией Кресвелл и др., один из подходов — разбивать один запрос на объяснения и ответы на две части. Сначала подсказка выбирает релевантные факты из текста ("selection prompt"), потом вторая подсказка делает вывод на основе выбранных фактов ("inference prompt"). Эти запросы чередуются, создавая цепочку рассуждений для окончательного ответа. Авторы показывают идею на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 млрд параметров это существенно улучшило результаты в задачах bAbi и Proof Writer, где требуется длинная цепочка рассуждений. Лучший результат был получен при сочетании selection-inference и дообучения.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя рост точности на этих бенчмарках значителен, они выбраны именно за длинные последовательные задачи. Для задач с небольшим числом шагов рассуждений эффект будет меньше.

Выводы для работы с языковыми моделями:

- Разбивайте сложные задачи на простые — так повышается надежность и точность.
- Для максимума производительности часто нужно сочетать дообучение с выбранным подходом.

Подробности — в [полной статье](https://arxiv.org/abs/2205.09712).

### Архитектура "надежных рассуждений" (faithful reasoning)

Через несколько месяцев после публикации selection-inference авторы расширили технику, предложив:

- способ решить, когда остановить цикл выбора и вывода
- функцию оценки для поиска по разным путям рассуждений
- уменьшение "галлюцинаций" фейковых фактов через дообучение модели на ярлыках предложений (например, sen1), а не тексте предложения

#### Метод

В оригинальной технике чередуются специальные подсказки “selection” и “inference” для выбора фактов и вывода из них, создавая цепочку рассуждений.

Расширение добавляет две вещи.

Во-первых, модель “halter”, которая после каждого шага оценки отвечает, достаточно ли уже выводов для ответа. Если да — формируется итоговый ответ.

Преимущества модели halter:

- Она может останавливать или продолжать процесс выбора-вывода.
- Если процесс не останавливается, не будет ответа, что лучше, чем "галлюцинарованный" ответ.

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавлена функция оценки качества рассуждений, которая помогает искать лучшие из множества вариантов. Это типично для повышения надежности — генерировать много ответов и использовать вспомогательную функцию или модель, чтобы выбрать лучший.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, чтобы уменьшить галлюцинации, модель обучали работать с ярлыками предложений, а не с их текстом.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы протестировали метод на задачах ProofWriter и [EntailmentBankQA](https://allenai.org/data/entailmentbank). Точность выросла значительно, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)

Их хитрость с ярлыками предложений практически устранила галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)

#### Выводы

В статье показаны полезные уроки для повышения надежности моделей:

- Делите сложные задачи на более простые, надежные подзадачи
- Генерируйте ответ поэтапно, оценивая его на каждом шаге
- Генерируйте много вариантов и выбирайте лучший с помощью дополнительной модели или функции
- Уменьшайте галлюцинацию, ограничивая, что модель может говорить (например, ярлыками предложений)
- Максимально улучшайте модели дообучением на специализированных задачах

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Подсказки от меньшего к большему (least-to-most prompting)

Кроме проблем длинных цепочек рассуждений (где хорошо работает selection-inference), цепочки рассуждений плохо работают, когда примеры короткие, а задача длинная.

#### Метод

Least-to-most prompting — ещё одна техника разбивки рассуждений на более простые задачи. Идея — попросить модель сформулировать подзадачу, например с помощью &lt;&lt;&lt;INL_13>>>. Затем с этой подзадачей модель генерирует решение, добавляет его к исходному вопросу и повторяет процесс, пока не получит окончательный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

Используя &lt;&lt;&lt;INL_14>>> (оптимизированный для кода, но также понимающий текст), авторы получили рост точности с 16% до 99.7% на задачах с длинными цепочками рассуждений.

[
![Least-to-most prompting на last-letter-concatenation](/images/least-to-most_tab4.png)
![Least-to-most prompting на SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя результаты впечатляют, они касаются узкого круга задач с длинными цепочками рассуждений.

Тем не менее, они иллюстрируют общую идею: повышайте надежность, (а) разбивая сложные задачи на подзадачи и (б) давая модели больше времени и пространства для решения.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Маиютетические подсказки (maieutic prompting)

#### Метод

В отличие от предыдущих техник, ориентированных на максимизацию вероятности правильного ответа, другой подход — сгенерировать дерево возможных объяснений (верных и неверных), проанализировать взаимосвязи и оценить, какие наборы являются корректными. Термин "maieutic prompting" ввели [Джэхун Джунг и др. в мае 2022](https://arxiv.org/abs/2205.11822) (маиютетика — отношение к сократическому методу вопросов для выявления идей).

Метод такой:

- Сначала создаётся маиютетическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинается с вопроса с несколькими вариантами или утверждения ИЛИ/ИЛИ (например &lt;&lt;&lt;INL_15>>>)
  - Для каждого варианта ответа модель генерирует объяснение (с помощью подсказки &lt;&lt;&lt;INL_16>>>)
  - Модель проверяется, меняется ли ответ при отрицании объяснения (например, с префиксом &lt;&lt;&lt;INL_17>>>); если меняется, объяснение считается "логически цельным"
  - Если объяснение не логически цельно, процесс повторяется рекурсивно: объяснение превращается в вопрос Истина/Ложь, для которого генерируются новые объяснения
  - В итоге формируется дерево, где листы обладают свойством, что их отрицание меняет ответ модели
- Затем дерево переводится в граф отношений:
  - Для каждого узла считается относительная вера модели (на основе вероятности ответа &lt;&lt;&lt;INL_18>>> при данном объяснении)
  - Для пар узлов модель определяет, подразумевают ли они друг друга или противоречат
- Наконец, вычисляется наиболее согласованный набор утверждений:
  - Формулируется задача максимальной выполнимости с весами (MAX-SAT)
  - Решатель выбирает наиболее самосогласованный набор, который принимается за истину

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Джунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Maieutic prompting результаты](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Джунг и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Несмотря на сложность, один из недостатков метода — возможное применение только к задачам с вариантами выбора.

Подробности — в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность (self-consistency)

#### Метод

Для задач с конечным набором вариантов ответов простой способ повысить надежность — сгенерировать несколько вариантов объяснения и ответа с помощью модели (используя положительную температуру выборки), а потом выбрать самый популярный ответ.

[![Метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Данная техника повысила точность на 1–24 процентных пункта на различных математических и логических датасетах. Внизу графики для модели LaMDA; с большей моделью PaLM базовое качество было выше, но рост меньше.

[![Результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэжи Ван и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя техника проста, она может быть дорогой: 10 генераций ответов «увеличат» расходы в 10 раз.

Кроме того, она применима только к задачам с ограниченным числом ответов. Для открытых задач с уникальными ответами (например, написание стихов) неясно, как выбрать самый частый ответ.

Также метод полезен, когда есть несколько путей к ответу; если путь единственный, техника бесполезна. Например, если ответ — один токен, выбор самого частого среди 100 генераций не отличается от выбора токена с максимальной вероятностью (которую можно получить с одним запросом при temperature=0).

### Верификаторы (verifiers)

Другой важный способ — обучить модель-проверяющего или дискриминатора оценивать ответы генеративной модели. Если проверка не пройдена, повторяем генерацию. Часто оценить ответ проще, чем придумать, что объясняет эффективность подхода.

#### Метод

В 2021 году исследователи OpenAI применили такой подход к школьной математике:

- Дообучили модель на вопросах и решениях
- Для каждого вопроса сгенерировали 100 решений
- Автоматически метили каждое как правильное или нет, по правильности итогового ответа
- Дообучили верификатор на этом наборе помеченных решений
- При тесте сгенерировали 100 решений, выбрали ответ с наивысшей оценкой верификатора

[![Метод верификаторов](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

Используя GPT-3 с 175 млрд параметров и 8000 примеров обучения, метод поднял точность с ~33% до ~55%.

[![Результаты верификаторов](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Коббе и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и с самосогласованностью, расходы могут вырасти примерно в 100 раз из-за генерации множества вариантов.

## Теории надежности

Хотя методы различны, общая цель — повысить надежность решения сложных проблем. В основном это достигается:

- разбиением ненадежных операций на более простые и надёжные (например, selection-inference prompting)
- использованием нескольких этапов и взаимосвязей, чтобы надежность системы превосходила надёжность каждого элемента (например, maieutic prompting)

### Вероятностные графовые модели

Идея построения надежной системы из менее надежных частей напоминает вероятностное программирование, и методы этого направления можно применять и тут.

В статье _Language Model Cascades_ Дэвид Дохан и соавторы анализируют перечисленные методы в рамках вероятностных графовых моделей:

#### Цепочки рассуждений (Chain of thought prompting)

[![графическая модель цепочек рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Дообученные цепочки рассуждений / Self-taught reasoner

[![графическая модель fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графическая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы (Verifiers)

[![графическая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формулировка в виде графовых моделей может не помочь сразу решить задачу, эта структура может помочь в выборе, комбинации и разработке новых методов.

## Заключительные мысли

Исследования в области больших языковых моделей развиваются очень быстро. Учёные не только улучшают модели, но и всё лучше понимают, как их использовать. Все рассмотренные статьи вышли за последние 12 месяцев (по состоянию на сентябрь 2022).

В будущем появятся новые модели и методы. Даже если конкретные техники сменятся, общие принципы останутся важными инструментами профессионалов.

## Библиография

| Урок                                                                                                                                | Статья                                                                                                                                 | Дата     |
| ----------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на подзадачи (и подумайте о выводах промежуточных результатов для пользователей)                         | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Можно улучшить результат, сгенерировав много кандидатов, а затем выбрав лучший                                                        | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                       | Октябрь 2021 |
| Для задач рассуждений модели лучше отвечают, если они рассуждают шаг за шагом                                                        | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                | Январь 2022 |
| Можно улучшить пошаговые рассуждения, генерируя много вариантов объяснений и ответов и выбирая самый популярный                     | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                              | Март 2022 |
| Для дообучения пошагового рассуждателя можно использовать только данные с вопросами и вариантами ответов                            | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                         | Март 2022 |
| Метод пошаговых рассуждений работает отлично даже без примеров                                                                      | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                        | Май 2022 |
| Можно улучшить пошаговые рассуждения, чередуя запросы выбора и вывода                                                                 | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)            | Май 2022 |
| Для длинных задач с рассуждениями можно улучшить работу, решая подзадачи поэтапно                                                     | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                           | Май 2022 |
| Можно заставить модель анализировать хорошие и плохие объяснения, чтобы выявить наиболее согласованные из них                       | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                       | Май 2022 |
| Эти техники можно рассматривать через призму вероятностного программирования, где система состоит из ненадежных компонентов          | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                              | Июль 2022 |
| Можно избавиться от галлюцинаций с помощью ярлыков предложений и снизить неправильные ответы с помощью запроса "остановщика"         | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                       | Август 2022 |