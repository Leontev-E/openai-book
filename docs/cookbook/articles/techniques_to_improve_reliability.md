---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, если GPT-3 не справляется с задачей?

- Искать более удачный запрос, который даст более надежные ответы?
- Инвестировать в тысячи примеров для тонкой донастройки собственной модели?
- Предположить, что модель просто неспособна решить задачу, и переключиться на другое?

Простого ответа нет — всё зависит от ситуации. Однако, если ваша задача требует логического рассуждения или включает сложность, попробуйте применить техники из этой статьи для создания более надежных и эффективных запросов.

## Почему GPT-3 ошибается при решении сложных задач

Если вас попросят умножить 13 на 17, мгновенно придет ли ответ в голову? Для большинства — скорее нет. Но это не значит, что человек не умеет умножать двузначные числа. За несколько секунд и с помощью листа бумаги легко посчитать, что 13 x 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 получить слишком сложную задачу, чтобы решить её за время вычисления следующего токена, он может сгенерировать неверное предположение. Однако, как и человек, это не значит, что модель не способна решить задачу. Если дать ей время и возможность рассуждать, она всё равно может выдать надежный ответ.

Например, если задать &lt;&lt;&lt;INL_0>>> такую задачу по математике про жонглирование мячами, ответ будет неверным:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если подать модели подсказку &lt;&lt;&lt;INL_1>>>, она решит задачу надежно:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно понять, работает ли трюк &lt;&lt;&lt;INL_2>>> в общем случае, или просто повезло на этой задаче. Но он действительно работает. На тестовой выборке задач по словесной математике трюк &lt;&lt;&lt;INL_3>>> значительно увеличил точность GPT-3 — с никчемных 18% до приличных 79%!

## Возможности модели зависят от контекста

При работе с GPT-3 часто совершается концептуальная ошибка — считать, что его возможности фиксированы во всех контекстах. Например, если GPT-3 ошибается с простым логическим вопросом, значит модель неспособна решать простую логику.

Но, как показывает пример &lt;&lt;&lt;INL_4>>>, кажущиеся ошибки GPT-3 иногда можно исправить, предоставив ему более удачный запрос, который помогает модели откорректировать собственный ответ.

## Как повысить надежность при решении сложных задач

В остальной части статьи описываются техники повышения надежности больших языковых моделей при решении сложных задач. Хотя некоторые методы подходят для конкретных типов задач, многие основаны на общих принципах, применимых во множестве случаев, например:

- Давать более четкие инструкции
- Разбивать сложные задачи на более простые подзадачи
- Структурировать инструкции, чтобы удерживать модель на нужной теме
- Побуждать модель сначала объяснить рассуждения, а затем отвечать
- Запрашивать обоснования для множества вариантов ответа и затем синтезировать их
- Генерировать много вариантов ответа и выбирать лучший с помощью модели
- Тонко настраивать пользовательские модели для максимальной производительности

## Разбивайте сложные задачи на более простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на более простые части.

Например, рассмотрим задачу с выбором ответа из нескольких вариантов по тексту — в нашем случае, игре Clue. При прямом вопросе &lt;&lt;&lt;INL_5>>> не может связать подсказки 3 и 5 и отвечает неправильно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 показывают, что полковник Мастард был единственным человеком в обсерватории, и у этого человека был подсвечник, модель не соединяет их и не выдает правильный ответ (а) Да.

Вместо того чтобы просить ответ напрямую, можно разбить задачу на три части:

- Сначала пройти по подсказкам одну за другой и оценить, может ли подсказка быть релевантной
- Затем связать релевантные подсказки для вывода ответа на вопрос
- Наконец, написать итоговый ответ: (а), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Давая модели больше времени и направляя через план рассуждений, она находит правильный ответ — (а) Да.

Кроме того, разбиение сложных инструкций на подзадачи помогает удерживать модель сосредоточенной на каждой из них.

Например, если попросить &lt;&lt;&lt;INL_6>>> резюмировать текст на его исходном языке, модель может неожиданно перейти на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Однако, если перед резюмированием сначала определить язык текста, модель становится более надежной:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Побуждайте модель объяснять перед ответом

Еще одна мощная техника повышения надежности — заставить модель шаг за шагом рассуждать ответ, вместо того чтобы сразу выдавать итог.

### Нулевой шаг (zero-shot)

#### Метод

Опубликованный [Тakeshi Kojima и соавторами в 2022 году](https://arxiv.org/abs/2205.11916), самый простой способ заставить модель рассуждать — просто добавить к ответам &lt;&lt;&lt;INL_7>>>. На рисунке 2 показан пример:

[![пример рассуждений zero-shot](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применяя этот простой прием к набору задач MultiArith, авторы обнаружили, что &lt;&lt;&lt;INL_8>>> увеличил точность в 4 раза — с 18% до 79%!

[![пример рассуждений zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк &lt;&lt;&lt;INL_9>>> хорошо работает для математических задач, он не эффективен для всех задач. Авторы отметили, что он особенно полезен для многошаговых арифметических задач, задач на символические рассуждения, стратегий и прочих логических задач. Для простых арифметических задач или вопросов здравого смысла он почти не помогает.

[![пример рассуждений zero-shot](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ авторы Takeshi Kojima и др. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.11916).

Если вы применяете этот прием к своим задачам, не бойтесь экспериментировать с настройкой инструкции. &lt;&lt;&lt;INL_10>>> довольно универсальна, но лучше могут показать себя более строгие форматы, настроенные под ваш кейс. Например, попробуйте более структурированные варианты, как &lt;&lt;&lt;INL_11>>>. Можно даже дать модели пример формата, чтобы направить её, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Примеры с обучением на небольшом количестве (few-shot)

#### Метод

Побуждение модели рассуждать можно сделать разными способами. Один из них — продемонстрировать несколько примеров («few-shot»), как это исследовали [Jason Wei и Denny Zhou из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Еще примеры рассуждений, написанных человеком для обучения модели:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Примечание: сомнения появились насчет того, плавали ли по-настоящему груши)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При тестировании на школьных задачах по математике, этот прием увеличил точность решения втрое — с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочка рассуждений повысила результаты по вопросам понимания спорта, отслеживания подбрасывания монеты и конкатенации последних букв. В большинстве случаев нужно было немного примеров (обычно меньше 8), чтобы получить эффект.

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее читайте в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество few-shot-подхода перед техникой &lt;&lt;&lt;INL_12>>> в том, что вы легче можете задавать формат, длину и стиль рассуждений, которые хотите увидеть, прежде чем модель даст итоговый ответ. Это особенно полезно, если модель изначально рассуждает неверно или поверхностно.

### Тонкая настройка

#### Метод

Чтобы максимизировать производительность на задаче, обычно нужна тонкая настройка собственной модели. Однако обучение на объяснениях вызывает необходимость в тысячах примеров объяснений, создание которых дорогостоящее.

В 2022 году Эрик Зеликман и Юхуаи Ву опубликовали умный метод генерации набора данных объяснений с помощью few-shot подсказки, который затем можно использовать для тонкой настройки модели. Идея в следующем:

- Использовать few-shot подсказку для генерации кандидатов объяснений и сохранять только те, которые приводят к правильному ответу.
- Для некоторых неправильных ответов повторять few-shot генерацию, но с правильными ответами, вписанными в вопрос.
- Эта процедура названа STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ авторы Eric Zelikman и Yujuai Wu (2022)](https://arxiv.org/abs/2203.14465)

Так вы сочетаете преимущества тонкой настройки и цепочек рассуждений без необходимости писать тысячи объяснений вручную.

#### Результаты

Применяя метод к датасету вопросов здравого смысла, авторы обнаружили, что STaR превзошел как одиночное использование цепочек рассуждений (73% против 37%), так и тонкую настройку без объяснений (73% против 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ авторы Eric Zelikman и Yujuai Wu (2022)](https://arxiv.org/abs/2203.14465)

Подробнее читайте в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot подсказки для расширения или модификации датасета тонкой настройки — идея, которую можно применять не только к написанию объяснений. Например, если у вас есть большой объем неструктурированного текста, можно с помощью модели извлечь из него структурированный датасет, а затем тонко настроить модель на этот датасет.

## Расширения цепочек рассуждений

Было опубликовано несколько расширений метода цепочек рассуждений.

### Selection-inference prompting

#### Метод

Опубликованная Антонией Кресвелл и соавторами, одна из расширенных техник разбивает один запрос на объяснения и ответы на две части. Сначала одна подсказка выбирает релевантные факты из текста («selection prompt»). Затем другая делает выводы на основе выбранных фактов («inference prompt»). Эти подсказки чередуются в цикле, что помогает создавать несколько шагов рассуждений и, в конечном итоге, приходить к ответу. Авторы иллюстрируют идею таким образом:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

При применении к модели с 7 млрд параметров авторы обнаружили, что selection-inference prompting существенно улучшает результаты по задачам bAbi и Proof Writer (которые требуют длинных цепочек рассуждений). Лучших результатов они добились, сочетая selection-inference с тонкой настройкой.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя прирост по этим бенчмаркам большой, они выбраны именно из-за требования длинных цепочек рассуждений. В задачах с короткими цепочками эффект, вероятно, будет меньше.

Общие уроки для работы с большими языковыми моделями таковы: во-первых, разбивать сложные задачи на более атомарные — отличный способ повысить надежность и точность; чем проще подзадача, тем меньше шанс ошибки. Во-вторых, максимальную производительность часто дают комбинации с тонкой настройкой.

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.09712).

### Архитектура «Faithful reasoning»

Через несколько месяцев после публикации selection-inference, авторы развили идею в последующей работе, предложив:

- определять, когда цикл selection-inference должен остановиться или продолжаться
- добавлять функцию ценности для поиска по нескольким путям рассуждения
- снижать галлюцинации фейковых фактов с помощью тонкой настройки модели, которая рассуждает не о самих предложениях, а об их метках (e.g., sen1)

#### Метод

В оригинальном методе selection-inference чередуются специализированные подсказки выбора и вывода для выделения фактов и вывода заключений, создавая цепочку рассуждений.

Авторы добавляют два компонента.

Во-первых, «halter» — модель, которая после каждого шага вывода спрашивает себя, достаточно ли сделанных выводов для ответа на вопрос. Если да — генерируется итоговый ответ.

Плюсы halter:

- он указывает процессу selection-inference, когда прекратить или продолжить;
- если процесс так и не остановится, ответа не будет, что лучше, чем получить ошибочный ответ.

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавляется функция ценности, оценивающая качество шагов рассуждений и позволяющая искать лучшие траектории рассуждения. Это отражает распространенную идею повышения надежности — вместо одного ответа генерировать множество и выбирать лучший при помощи функции ценности / дискриминатора / верификатора.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, авторы применяют трюк для снижения галлюцинаций — вместо просьбы писать фактические предложения они тонко настраивают модель работать с метками предложений (например, sen1). Это помогает снизить выдумывание несущетсвующих фактов.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Метод оценивался на бенчмарках ProofWriter и [EntailmentBankQA](https://allenai.org/data/entailmentbank) (показанном ниже). Он существенно повысил точность, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, трюк с метками предложений почти полностью устранил галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и др. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Эта работа дает несколько полезных уроков по повышению надежности моделей:

- Делите сложные задачи на более простые и надежные подзадачи
- Генерируйте ответ шаг за шагом, проверяя его на каждом этапе
- Генерируйте множество ответов и используйте модель или функцию для выбора лучших
- Снижайте галлюцинации, ограничивая то, что модель может произносить (например, метками предложений вместо текста)
- Максимизируйте производительность, тонко настраивая модель под специальные задачи

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Помимо плохой работы с длинными цепочками рассуждений (где сильна selection-inference), цепочки рассуждений часто слабо работают, если примеры короткие, а задача — длинная.

#### Метод

Least-to-most prompting — еще одна техника разбивки задач на более простые. Идея — вызвать подзадачу с помощью подсказки вроде &lt;&lt;&lt;INL_13>>>, затем с помощью этой подзадачи сгенерировать решение. Решение добавляется к исходному вопросу, и процесс повторяется, пока не получается итоговый ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

При применении к бенчмаркам с длинными цепочками рассуждений с помощью &lt;&lt;&lt;INL_14>>> (оптимизированная для кода, но понимает и текст) авторы зафиксировали прирост от 16% до 99.7%!

[
![Результаты least-to-most на задаче last-letter-concatenation](/images/least-to-most_tab4.png)
![Результаты least-to-most на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most на DROP (числовое рассуждение)](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Denny Zhou и др. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя приросты выдают впечатляющие результаты, они измерены на узком наборе задач с длинными цепочками рассуждений.

Тем не менее они иллюстрируют общую идею: повысить надежность через (a) разбиение задачи на подзадачи и (b) предоставление времени и пространства для рассуждений.

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Маевтическое побуждение

#### Метод

В отличие от предыдущих методов, которые стремятся максимизировать вероятность правильных ответов, другой подход — сгенерировать дерево возможных объяснений (как правильных, так и неправильных), проанализировать их взаимосвязи и угадать, какая совокупность правильна. Этот метод назвали maieutic prompting, предложенный [Jaehun Jung и др. в мае 2022](https://arxiv.org/abs/2205.11822) (maieutic — метод сократического задавания вопросов для вызова идей).

Метод сложен и работает так:

- Сначала строится маевтическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинают с вопроса с множественным выбором или утверждения «истина/ложь» (напр., &lt;&lt;&lt;INL_15>>>)
  - Для каждого варианта ответа модель генерирует объяснение (с помощью подсказки вроде &lt;&lt;&lt;INL_16>>>)
  - Затем модель получает вопрос и объяснение и выдает ответ. Если инверсия объяснения (с префиксом типа &lt;&lt;&lt;INL_17>>>) инвертирует ответ, объяснение считается «логически цельным».
  - Если объяснение не цельно, процесс повторяется рекурсивно: каждое объяснение превращается в вопрос «истина/ложь» и для него генерируются свои объяснения.
  - В итоге получается дерево объяснений, где каждая «листва» обладает свойством, что переворот объяснения меняет модельный ответ.
- Далее дерево преобразуют в граф отношений:
  - Для каждого узла вычисляется относительная вера модели в утверждение (на основе вероятности ответа &lt;&lt;&lt;INL_18>>> при данном объяснении)
  - Для каждой пары узлов модель определяет, подразумевают ли они друг друга или противоречат
- Наконец, ищется наиболее последовательный набор убеждений, принимаемый за истинный:
  - Задача формулируется как задача максимизации удовлетворения с весами (MAX-SAT) с учетом силы веры и логических связей
  - Решателем ищут наиболее самосогласованный набор убеждений, принимаемый за истинный

[
![Маевтическое побуждение](/images/maieutic_fig2.png)
![Маевтическое побуждение](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты маевтического побуждения](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Jaehun Jung и др. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Помимо сложности, метод ограничен тем, что применим только к вопросам с вариантами ответов.

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность (Self-consistency)

#### Метод

Для задач с ограниченным набором ответов простой способ повысить надежность — сгенерировать несколько объяснений и ответов с температурой выше нуля, затем выбрать наиболее часто встречающийся итоговый ответ.

[![Метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Метод повышал точность от 1 до 24 процентных пунктов на различных наборах задач по математике и рассуждениям. (На графике ниже результаты модели LaMDA от Google, а для более крупной модели PaLM базовые показатели были выше, а рост — чуть меньше.)

[![Результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Xuezhi Wang и др. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя метод прост в реализации, он может быть дорогим. Генерация 10 ответов увеличит затраты примерно в 10 раз.

Кроме того, как и многие методы, он годится лишь для задач с ограниченным множеством ответов. Для открытых задач с уникальными ответами (например, написание стихов) не понятно, как выбирать «наиболее частый» ответ.

И, наконец, техника полезна там, где есть несколько путей или формулировок решения; если только один путь, эта методика не поможет. Например, если задача — сгенерировать один токен, то выбор самого частого из 100 вариантов не отличается от выбора токена с максимальной логарифмической вероятностью (что можно сделать с одним запуском и температурой 0).

### Верификаторы

Еще один важный метод повышения качества — обучить верификатор или дискриминатор для оценки результатов основной генеративной модели. Если верификатор отвергает ответ, можно повторно запросить генерацию, пока не получим подходящий ответ. Во многих случаях оценить правильность проще, чем создать ответ — отсюда сила этого метода.

#### Метод

В 2021 году исследователи OpenAI применили этот подход к задачам школьной математики, используя такой алгоритм:

- Сначала модель тонко настраивается на парах «вопрос — решение»
- Для каждого вопроса набора тренировочных данных генерируется 100 ответов
- Каждое решение метится автоматически как правильное или неправильное по итоговому ответу
- На этих помеченных данных обучают верификатор, который классифицирует, правильный ли ответ
- В тесте генеративная модель создаёт 100 решений, и выбирается то, которое получила наивысший балл от верификатора

[![Метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С моделью GPT-3 на 175 млрд параметров и 8 тысячами тренировочных примеров метод сильно увеличил точность школьной математики — с ~33% до ~55%.

[![Результаты верификатора](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Karl Cobbe и др. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и метод самосогласованности, этот подход может быть дорогим. Генерация, например, 100 вариантов ответов увеличит расходы примерно в 100 раз.

## Теории надежности

Хотя приведенные техники различаются по подходу, они все стремятся повысить надежность в сложных задачах, главным образом через:

- разбиение ненадежных операций на более мелкие, надежные (например, selection-inference prompting)
- использование множества шагов и взаимосвязей, чтобы надежность системы была выше, чем у любого из компонентов по отдельности (например, maieutic prompting)

### Вероятностные графовые модели

Подход построения надежной системы из менее надежных компонентов напоминает методы вероятностного программирования. Многие аналитические методы из этой области можно применить и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и др. интерпретируют описанные выше техники с точки зрения вероятностных графовых моделей:

#### Цепочка рассуждений (chain of thought prompting)

[![Графовая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Тонко настроенная цепочка рассуждений / Self-taught reasoner

[![Графовая модель тонко настроенной цепочки рассуждений](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![Графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![Графовая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и др. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формулировка техник в виде вероятностных графовых моделей вряд ли поможет решить конкретную задачу сразу, она может облегчить выбор, комбинирование и открытие новых методов.

## Заключительные мысли

Исследования в области больших языковых моделей ведутся очень активно и быстро развиваются. Не только модели становятся лучше, но и наше понимание того, как их эффективнее применять. Все приведённые выше статьи были опубликованы за последние 12 месяцев (на момент написания — сентябрь 2022).

В будущем будут выходить более продвинутые модели и методы. Даже если конкретные техники здесь устареют, общие принципы останутся ключевой частью арсенала любого опытного пользователя.

## Библиография

| Урок                                                                                                                           | Статья                                                                                                                                 | Дата     |
| ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на подзадачи (и рассмотрите возможность показывать промежуточные результаты пользователям)             | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт 2021 |
| Можно улучшить результат, генерируя множество вариантов и выбирая лучший                                                        | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                    | Окт 2021 |
| В задачах рассуждений модели лучше отвечают, если рассуждают шаг за шагом                                                       | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                             | Янв 2022 |
| Улучшить рассуждения шаг за шагом можно, генерируя много ответов с объяснениями и выбирая наиболее популярный                   | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                           | Мар 2022 |
| Можно обучить пошагового рассуждателя, используя только данные с вопросами с множественным выбором                              | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                      | Мар 2022 |
| Метод рассуждений шаг за шагом работает даже без примеров                                                                     | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                     | Май 2022 |
| Можно обойтись лучше, если чередовать подсказки «выбор» и «вывод»                                                              | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)         | Май 2022 |
| Для длинных цепочек рассуждений можно разбивать задачи, решая эти кусочки по очереди                                            | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                        | Май 2022 |
| Модель может анализировать и хорошие, и плохие объяснения, чтобы выявить наиболее согласованный набор                          | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                    | Май 2022 |
| Эти методы можно представить в виде вероятностных программ, где системы состоят из ненадежных компонентов                        | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                           | Июль 2022 |
| Галлюцинации можно устранить, используя метки предложений, а неверные ответы уменьшить с помощью «halter»                        | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                    | Авг 2022 |