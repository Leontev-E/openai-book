---
lang: ru
translationOf: openai-cookbook
---

# Техники для повышения надёжности

Что делать, когда GPT-3 не справляется с задачей?

- Искать лучший запрос, который даст более надёжные ответы?
- Вкладывать усилия в тысячи примеров для тонкой настройки собственной модели?
- Предполагать, что модель не способна решить задачу, и переходить к другой?

Однозначного ответа нет — всё зависит от ситуации. Тем не менее, если задача требует логического рассуждения или представляет сложность, попробуйте техники из этой статьи, чтобы создать более надёжные и эффективные запросы.

## Почему GPT-3 ошибается в сложных задачах

Если вас попросить перемножить 13 на 17, сразу придёт ли ответ? Для большинства из нас — вряд ли. Однако это не значит, что человек не умеет умножать двухзначные числа. Несложно за пару секунд с помощью бумаги и ручки понять, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 дать задачу слишком сложную, чтобы успеть решить её за время предсказания следующего токена, он может сочинить неверный ответ. Но, подобно людям, это не обязательно свидетельствует о неспособности модели решить задачу. Дайте время и возможность рассуждать, и модель, скорее всего, сможет ответить надёжно.

Например, если задать &lt;&lt;&lt;INL_0>>> следующую задачу по математике о жонглировании мячами, он ответит неправильно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Означает ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, при подаче запроса, оформленного как &lt;&lt;&lt;INL_1>>>, модель решает задачу надёжно:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно сказать, работает ли трюк &lt;&lt;&lt;INL_2>>> вообще или просто повезло в данном конкретном случае. Но он действительно работает. На наборе задач слово-математики трюк &lt;&lt;&lt;INL_3>>> значительно повысил точность GPT-3, с жалких 18% до приличных 79%!

## Возможности модели зависят от контекста

При работе с GPT-3 часто встречается заблуждение, что её возможности фиксированы во всех контекстах. Например, если GPT-3 ошибается в простом логическом вопросе, значит, она неспособна решать простую логику.

Но как показывает пример &lt;&lt;&lt;INL_4>>>, кажущиеся ошибки GPT-3 можно исправить с помощью лучшего запроса, который помогает модели удерживаться на правильном ответе.

## Как повысить надёжность при сложных задачах

Далее мы рассмотрим техники повышения надёжности больших языковых моделей на сложных задачах. Хотя некоторые из них специфичны для определённого типа проблем, многие основаны на общих принципах, применимых в широком спектре задач, например:

- Давать более чёткие инструкции
- Делить сложные задачи на более простые подзадачи
- Структурировать инструкцию, чтобы держать модель в рамках задачи
- Просить модель пояснить ход рассуждений перед ответом
- Запрашивать обоснования для нескольких возможных ответов и затем синтезировать итог
- Генерировать множество ответов и выбирать лучший с помощью модели
- Тонко настраивать кастомные модели для максимальной производительности

## Разделение сложных задач на более простые

Один из способов дать модели больше времени и места для размышления — разбить задачу на более простые части.

Рассмотрим задачу с вопросом с несколькими вариантами ответа по тексту — например, по игре в Clue. При прямом вопросе &lt;&lt;&lt;INL_5>>> не удаётся объединить подсказки 3 и 5, и он отвечает неправильно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 показывают, что полковник Горчица был единственным в обсерватории, а тот, кто был в обсерватории, держал подсвечник, модель не сумела объединить их и дала неверный ответ (a) Да.

Однако вместо прямого запроса ответа можно разбить задачу на три части:

- Сначала пройтись по подсказкам по одной и оценить, может ли подсказка быть релевантной
- Затем объединить релевантные подсказки, чтобы вывести ответ на вопрос
- Наконец, записать итоговый ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Даже давая модели больше времени и поэтапно направляя её по плану рассуждений, модель смогла найти правильный ответ (a) Да.

Ещё одна польза от разбивания сложных инструкций на подзадачи — это возможность удержать модель сфокусированной на каждой из них.

Например, когда мы просим &lt;&lt;&lt;INL_6>>> резюмировать текст на исходном языке, модель может переключиться обратно на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Но если сначала попросить модель определить язык текста, а затем написать резюме, это работает гораздо надёжнее:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просьба сначала объяснить, а затем ответить

Ещё одна мощная техника повышения надёжности — заставить модель поэтапно рассуждать над ответом, вместо того чтобы сразу выдавать итог. Мысля вслух, модель гораздо чаще приходит к правильному ответу.

### Zero-shot

#### Метод

Опубликованная [Такеши Кодзимой и др. в 2022](https://arxiv.org/abs/2205.11916) самая простая техника — просто добавить в начало ответа &lt;&lt;&lt;INL_7>>>. На рисунке 2 показан пример:

[![zero-shot reasoning example](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ (Такеши Кодзима и др., 2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применение этого трюка к датасету MultiArith по математике показало, что точность возросла в 4 раза — с 18% до 79%!

[![zero-shot reasoning example](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ (Такеши Кодзима и др., 2022).](https://arxiv.org/abs/2205.11916)

#### Значение

Хотя трюк &lt;&lt;&lt;INL_8>>> хорошо работает для математических задач, он не универсален. Авторы отметили, что он наиболее полезен для арифметики с несколькими шагами, задач символических рассуждений, стратегических задач и других с многозвенным рассуждением. Для простых математических задач или вопросов здравого смысла он не помогает, и, вероятно, не будет полезен в ряде других нетехнических задач.

[![zero-shot reasoning example](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ (Такеши Кодзима и др., 2022).](https://arxiv.org/abs/2205.11916)

Чтобы узнать подробнее, прочтите [оригинальную статью](https://arxiv.org/abs/2205.11916).

Если решите применить этот метод к вашим задачам, не бойтесь экспериментировать с настройками инструкции. &lt;&lt;&lt;INL_9>>> слишком общий, возможно, лучше использовать более жёсткий формат, адаптированный под ваш кейс. Например, попробуйте структурированные варианты вроде &lt;&lt;&lt;INL_10>>>. Можно даже привести модели пример формата, чтобы она не сбивалась, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Few-shot примеры

#### Метод

Побуждение модели к рассуждениям может происходить по-разному. Например, показывать пару примеров («few-shot») — как изучили [Джейсон Вэй и Дэнни Чжоу из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot цепочки рассуждений:

[![chain of thought example](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ (Джейсон Вэй и Дэнни Чжоу, 2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё примеры цепочек рассуждений, созданных людьми:

[![chain of thought example](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ (Джейсон Вэй и Дэнни Чжоу, 2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание: спорно, действительно ли груши плавают)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При проверке на школьных задачах с математикой авторы обнаружили, что цепочка рассуждений увеличила количество решённых задач в 3 раза — с 18% до 57%.

[![chain of thought example](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ (Джейсон Вэй и Дэнни Чжоу, 2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Помимо математики, техника также повысила точность ответов по вопросам спорта, отслеживания подбрасывания монеты и конкатенации последних букв. Обычно для максимального эффекта требуется не так много примеров — менее восьми.

[![chain of thought example](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ (Джейсон Вэй и Дэнни Чжоу, 2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Для изучения подробностей прочитайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Значение

Преимущество few-shot примеров перед техникой &lt;&lt;&lt;INL_11>>> в том, что можно точнее задать формат, длину и стиль рассуждений, которые хотите видеть у модели перед итоговым ответом. Это особо полезно, если модель изначально не рассуждает нужным образом или глубиной.

### Fine-tuned

#### Метод

Для максимальной производительности на задаче обычно нужно тонко настраивать собственную модель. Но для настройки с использованием объяснений потребуется тысячи примеров, что дорого.

В 2022 году Эрик Зеликман и Юхуаи Ву предложили умный метод: использовать few-shot запрос для генерации датасета с объяснениями, а затем на его основе тонко настраивать модель. Идея: создать кандидатов объяснений, оставить только те, что подходят по правильному ответу. Для неправильных примеров повторять few-shot, задавая в вопросе правильные ответы. Авторы назвали метод STaR (Self-taught Reasoner — Самообучающийся рассуждатель):

[![STaR procedure](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ (Эрик Зеликман и др., 2022)](https://arxiv.org/abs/2203.14465)

Этот способ сочетает преимущество тонкой настройки и цепочек рассуждений без необходимости писать тысячи объяснений.

#### Результаты

Применение STaR к датасету по здравому смыслу показало превосходство над лишь цепочками рассуждений (73% vs 37%) и тонкой настройкой без объяснений (73% vs 60%):

[![STaR results](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ (Эрик Зеликман и др., 2022)](https://arxiv.org/abs/2203.14465)

Подробности в [полной статье](https://arxiv.org/abs/2203.14465).

#### Значение

Использование few-shot запроса для расширения или модификации датасета тонкой настройки — идея, которую можно применять шире, не только для написания объяснений. Например, если у вас есть большой объём неструктурированного текста, можно получать структурированный набор данных с помощью запроса и затем тонко настраивать модель.

## Расширения цепочек рассуждений

Существует несколько расширений техники цепочек рассуждений.

### Selection-inference prompting

#### Метод

Предложено Антонией Кресвелл и др. — один из вариантов цепочки рассуждений разбивает запрос на части. Сначала выделяется релевантный набор фактов из текста («selection prompt»). Затем другой запрос делает вывод из выбранных фактов («inference prompt»). Запросы чередуются, создавая несколько шагов рассуждений и приводя к финальному ответу. Идея иллюстрируется на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 миллиардами параметров selection-inference prompting значительно улучшил результаты по задачам bAbi и Proof Writer, которые требуют длинных цепочек рассуждений. Лучший результат получился при сочетании с тонкой настройкой.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2205.09712)

#### Значение

Хотя прирост в этих задачах большой, они были специально выбраны из-за необходимости длинных рассуждений. В задачах с короткими цепочками выгода, скорее всего, меньше.

Результаты подчеркивают два важных урока для работы с большими языковыми моделями. Во-первых, разбивка сложных задач на атомарные подзадачи — отличный способ повысить надёжность и качество; чем мельче задачи, тем меньше вероятность ошибки. Во-вторых, максимальная эффективность достигается сочетанием тонкой настройки и выбранного метода.

Подробнее см. [оригинальную статью](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Несколько месяцев спустя после публикации selection-inference prompting авторы расширили идею следующими новшествами:

- механизм определения, когда остановить или продолжить цикл selection-inference
- добавление ценностной функции, помогающей искать лучшие пути рассуждений
- снижение галлюцинаций фиктивных фактов путём тонкой настройки модели, работающей с метками предложений (например, sen1), а не с самими предложениями

#### Метод

В оригинальной технике чередуются специализированные selection и inference запросы, собирающие факты и делающие из них выводы, формируя последовательность рассуждений.

Авторы добавили два компонента.

Первый — модель «прекращения» (halter), которая после каждого шага inference отвечает, достаточно ли сделанных выводов, чтобы ответить на вопрос. Если да — генерируется итоговый ответ.

Модель прекращения даёт два преимущества:

- она может останавливать или продолжать процесс, когда нужно
- если процесс не останавливается, вместо галлюцинаций будет отсутствие ответа — что часто предпочтительнее

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2208.14271)

Второй — ценностная функция, которая оценивает качество рассуждений и ищет среди множества путей к ответу. Это отражает общую идею повышения надёжности: генерировать множество ответов и выбирать лучший с помощью функции оценки или верификатора.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2208.14271)

Кроме того, для снижения галлюцинаций фиктивных фактов используется приём: модель не пишет предложения, а работает с их метками (например, sen1). Это предотвращает появление в ответах несуществующих фактов, не упомянутых в контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы проверили метод на ProofWriter (не показано) и EntailmentBankQA (на рисунке). Точность заметно повысилась, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)

Кроме того, трюк с метками предложений практически устранил галлюцинацию!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ (Антония Кресвелл и др., 2022)

#### Значение

Эта статья даёт полезные уроки по повышению надёжности:

- делите сложные задачи на более надёжные подзадачи
- генерируйте ответы шаг за шагом, оценивая качество на каждом шаге
- генерируйте много возможных ответов и выбирайте лучшие с помощью другой модели или функции
- снижайте галлюцинации, ограничивая возможности модели (например, используя метки предложений)
- добивайтесь максимума, тонко настраивая модели на специализированные задачи

Подробнее в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Кроме того, что chain-of-thought плохо работает с длинными цепочками рассуждений (где selection-inference хорош), она особенно страдает, если примеры короткие, а задача длинная.

#### Метод

Least-to-most prompting — ещё одна техника, разбивающая рассуждение на более простые подзадачи. Идея — стимулировать модель выделить подзадачу с помощью запроса вроде &lt;&lt;&lt;INL_12>>>. Далее, имея эту подзадачу, модель решает её. Решение прикладывается к исходному вопросу и процесс повторяется, пока не получится окончательный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ (Дэнни Чжоу и др., 2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На задачах с длинными цепочками рассуждений, используя &lt;&lt;&lt;INL_13>>> (оптимизированный для кода, но понимающий и текст), рост точности был от 16% до 99.7%!

[
![Least-to-most prompting results on last-letter-concatenation task](/images/least-to-most_tab4.png)
![Least-to-most prompting results on SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting results on DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ (Дэнни Чжоу и др., 2022)](https://arxiv.org/abs/2205.10625)

#### Значение

Хотя прирост впечатляющий, эксперименты сделаны на узком наборе задач с длинными цепочками.

Тем не менее, результаты подтверждают общую идею: повышайте надёжность, (a) разбивая сложные задачи на подзадачи и (b) давая модели больше времени и пространства для работы.

Подробнее: [полная статья](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Маевтическое побуждение (maieutic prompting)

#### Метод

В отличие от предыдущих техник, направленных повысить вероятность правильного ответа, другой подход — заставить GPT-3 создавать дерево объяснений (как правильных, так и неправильных), а затем анализировать их взаимосвязи, чтобы определить наиболее вероятный правильный набор. Такая техника была названа maieutic prompting [Джэхуном Джунгом и др. (май 2022)](https://arxiv.org/abs/2205.11822) (маевтика — метод сократического диалога).

Метод выглядит так:

- Сначала строится маевтическое дерево, где каждый узел — утверждение, которое может быть верно или неверно:
  - Начинается с многовариантного вопроса или утверждения (например, &lt;&lt;&lt;INL_14>>>)
  - Для каждого возможного ответа модель генерирует объяснение (с подсказкой вроде &lt;&lt;&lt;INL_15>>>)
  - Далее модель получает вопрос и объяснение и генерирует ответ. Если инвертирование объяснения (с префиксом &lt;&lt;&lt;INL_16>>>) меняет ответ на обратный, то объяснение считается «логически интегральным»
  - Если объяснение не интегрально, процесс повторяется рекурсивно, каждый объяснение превращается в вопрос на истинность, и генерируются новые объяснения для новых вопросов
  - По завершении рекурсивных объяснений формируется дерево, где каждый лист инвертирует ответ модели при инверсии объяснения
- Во-вторых, преобразуется дерево в граф отношений:
  - Для каждого узла в дереве вычисляется относительная степень убежденности модели (из вероятности получить ответ &lt;&lt;&lt;INL_17>>> при данном объяснении)
  - Для каждой пары узлов модель определяет, подразумеваются ли эти узлы друг другом или противоречат
- В-третьих, находится максимально согласованный набор убеждений, принимаемый за истину:
  - Задаётся задача взвешенной максимальной выполнимости (MAX-SAT) с учётом значений убеждений и логических связей между ними
  - Решатель находит наиболе самосогласованный набор узлов, считающийся истинным

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ (Джэхун Джунг и др., 2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Maieutic prompting results](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ (Джэхун Джунг и др., 2022)](https://arxiv.org/abs/2205.11822)

#### Значение

Несмотря на сложность, метод ограничен задачами многовариантного выбора.

Подробнее: [полная статья](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность (Self-consistency)

#### Метод

Для задач с ограниченным набором ответов простой способ повысить надёжность — сгенерировать несколько объяснений и ответов (с положительной температурой) и выбрать итоговый ответ, который встречается чаще всего.

[![Self-consistency method](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ (Сюэчжи Ванг и др., 2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Эта техника повысила точность на 1–24 п.п. на наборе математических и логических бенчмарков. (На рисунке — результаты с моделью LaMDA от Google; с PaLM (более крупной) базовый уровень был выше, но прирост ниже.)

[![Self-consistency results](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ (Сюэчжи Ванг и др., 2022)](https://arxiv.org/abs/2203.11171)

#### Значение

Техника проста, но затратна. Генерация 10 ответов увеличит стоимость примерно в 10 раз.

Кроме того, метод подходит только для задач с ограниченным набором ответов. В открытых задачах, где ответов бесконечное множество (например, написание стихов), выбор наиболее частого ответа бессмысленен.

Это будет эффективно, если существует несколько способов и формулировок ответа; если путь единственный — метод не поможет. Крайний пример: если задача — сгенерировать одно слово, то выбор наиболее частого из 100 вариантов равен выбору слова с наивысшей вероятностью (что и так можно получить с temperature=0 за один проход).

### Верификаторы (Verifiers)

Другой способ повысить качество — обучить модель, которая проверяет ответы основной генеративной модели. Если верификатор отклоняет ответ, генерация повторяется до получения приемлемого результата. В ряде случаев оценить ответ проще, чем создать его, что объясняет силу этого подхода.

#### Метод

В 2021 году исследователи OpenAI использовали этот метод на школьных задачах по математике, следующим образом:

- Сначала модель тонко настраивали на вопросы и решения
- Для каждого вопроса из обучающего набора было сгенерировано по 100 решений
- Каждое решение автоматически размечалось как правильное или неправильное в зависимости от итогового ответа
- Затем из этих данных обучался верификатор, классифицирующий, верно ли решение
- Наконец, при тесте генеративная модель создаёт 100 решений, а итоговым выбирается то, что оценено верификатором выше всех

[![Verifier method](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ (Карл Кобб и др., 2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С 175-миллиардной моделью GPT-3 и 8000 примеров обучение принесло существенный прирост точности — с ~33% до ~55%.

[![Verifier results](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ (Карл Кобб и др., 2021)](https://arxiv.org/abs/2110.14168)

#### Значение

Подобно самосогласованности, техника затратна — 100 сгенерированных решений на задачу увеличивают стоимость примерно в 100 раз.

## Теории надёжности

Несмотря на разнообразие техник, их цель — повысить надёжность на сложных задачах. Главное:

- декомпозировать ненадёжные операции на множество более надёжных (например, selection-inference prompting)
- использовать несколько шагов и связей, чтобы надёжность всей системы была выше, чем у отдельного компонента (например, maieutic prompting)

### Вероятностные графовые модели

Подход создания надёжной системы из менее надёжных компонентов напоминает вероятностное программирование, и многие методы анализа той области применимы здесь.

В статье _Language Model Cascades_ Дэвид Дохан и др. рассматривают описанные техники с точки зрения вероятностных графовых моделей:

#### Цепочка рассуждений (chain of thought prompting)

[![graphical model of chain of thought prompting](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ (Дэвид Дохан и др., 2022)](https://arxiv.org/abs/2207.10342)

#### Тонко настроенная цепочка рассуждений / Самообучающийся рассуждатель

[![graphical model of fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ (Дэвид Дохан и др., 2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![graphical model of selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ (Дэвид Дохан и др., 2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![graphical model of verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ (Дэвид Дохан и др., 2022)](https://arxiv.org/abs/2207.10342)

#### Значение

Хотя формализация в виде вероятностных графовых моделей может не решить конкретные задачи сразу, она может помочь при выборе, комбинировании и поиске новых техник.

## Заключение

Исследования больших языковых моделей активно развиваются. Учёные не только совершенствуют модели, но и всё лучше понимают, как их использовать. Обратите внимание: все представленные статьи опубликованы за последние 12 месяцев (на момент написания в сентябре 2022).

В будущем появятся новые модели и методы. Даже если представленные здесь техники окажутся устаревшими, их основные принципы останутся важной частью арсенала любого специалиста.

## Библиография

| Урок                                                                                                                        | Статья                                                                                                                                    | Дата     |
| ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на более простые подзадачи (и рассматривайте возможность показывать промежуточные результаты)       | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт 2021 |
| Можно улучшить результаты, генерируя много вариантов и выбирая лучший                                                       | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Окт 2021 |
| В задачах рассуждений модели работают лучше, если рассуждают пошагово перед ответом                                          | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Янв 2022 |
| Можно улучшить пошаговое рассуждение, генерируя множество объяснений и ответов и выбирая наиболее популярный ответ          | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Мар 2022 |
| Тонкую настройку пошагового рассуждателя можно проводить только на данных вопросов с многовариантными ответами             | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Мар 2022 |
| Метод пошагового рассуждения работает хорошо даже без примеров                                                             | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Можно улучшить рассуждения, чередуя запросы «выбор» и «вывод»                                                               | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных задач с рассуждениями можно улучшить результат, решая задачу по частям                                          | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                             | Май 2022 |
| Можно заставить модель анализировать и хорошие, и плохие объяснения, чтобы найти наиболее согласованную группу объяснений    | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Можно рассматривать техники с точки зрения вероятностного программирования, где системы строятся из ненадёжных компонентов    | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июль 2022 |
| Галлюцинации можно устранить, используя метки предложений, а неверные ответы снижать с помощью запроса «прекращения»         | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Авг 2022 |