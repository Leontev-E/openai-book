---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, когда GPT-3 не справляется с задачей?

- Искать лучший промпт, который даст более надежные ответы?
- Тратить усилия на тысячи примеров для дообучения кастомной модели?
- Предположить, что модель не способна решить задачу, и перейти к другой?

Однозначного ответа нет — всё зависит от ситуации. Однако если ваша задача требует логического рассуждения или обладает сложностью, попробуйте техники из этой статьи для создания более надежных и высокоэффективных промптов.

## Почему GPT-3 ошибается на сложных задачах

Если вас попросить перемножить 13 на 17, ответ сразу придёт в голову? Для большинства из нас — нет. Однако это не значит, что люди не умеют умножать двузначные числа. Потратив пару секунд и используя бумагу с ручкой, несложно вычислить, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если вы даёте GPT-3 задачу, слишком сложную для выполнения за время, необходимое для вычисления следующего токена, модель может выдать ошибочный ответ. Но, как и люди, это ещё не значит, что модель не способна решить задачу. Если дать ей дополнительное время и пространство для рассуждений, модель может ответить надежно.

Например, если задать `gpt-3.5-turbo-instruct` задачу по математике с жонглированием мячами, модель ответит неверно:

&lt;&lt;&lt;CODE_0&gt;>>

&lt;&lt;&lt;CODE_1&gt;>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, как оказалось, если в промпт добавить `Let's think step by step`, модель надежно решает задачу:

&lt;&lt;&lt;CODE_2&gt;>>

&lt;&lt;&lt;CODE_3&gt;>>

Конечно, по одному примеру сложно понять, работает ли трюк `Let's think step by step` вообще или просто повезло в этом конкретном случае. Но он действительно работает. На тесте с задачами по вычислению слов «Let's think step by step» поднял точность GPT-3 с никчемных 18% до приличных 79%!

## Возможности модели зависят от контекста

При изучении работы с GPT-3 распространённая концептуальная ошибка — считать, что возможности модели фиксированы для всех контекстов. Например, если GPT-3 ошибается в простом логическом вопросе, значит, она не способна решать простую логику.

Но, как показывает пример с `Let's think step by step`, видимые ошибки GPT-3 часто можно устранить более продуманным промптом, который помогает модели направить свои рассуждения к правильному ответу.

## Как повысить надежность при решении сложных задач

Остальная часть статьи описывает методы повышения надежности больших языковых моделей при решении сложных задач. Некоторые техники специфичны для определённых типов задач, но многие основаны на общих принципах, применимых в широком спектре случаев, например:

- Давать более чёткие инструкции
- Разбивать сложные задачи на более простые подзадачи
- Структурировать инструкцию для удержания модели на задаче
- Просить модель объяснять ход рассуждений до ответа
- Запрашивать обоснования разных ответов с последующим синтезом
- Генерировать много вариантов ответа и выбирать лучший
- Дообучать кастомные модели для максимизации производительности

## Разбивайте сложные задачи на простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на более простые части.

Например, рассмотрим задачу: спросить у модели вопрос с несколькими вариантами ответов по тексту — в нашем случае по игре Clue. При прямом вопросе `gpt-3.5-turbo-instruct` не способен объединить подсказки 3 и 5 и отвечает неверно:

&lt;&lt;&lt;CODE_4&gt;>>

&lt;&lt;&lt;CODE_5&gt;>>

Хотя подсказки 3 и 5 устанавливают, что полковник Горчица был единственным человеком в обсерватории, а именно у человека в обсерватории был подсвечник, модель не объединяет эти данные в правильный ответ (a) Да.

Вместо того, чтобы требовать сразу ответ, можно разбить задачу на три шага:

- Сначала просмотреть подсказки одну за другой и оценить их потенциальную релевантность
- Затем объединить релевантные подсказки для рассуждения и вывода ответа на вопрос
- Наконец, написать окончательный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6&gt;>>

&lt;&lt;&lt;CODE_7&gt;>>

Давая модели больше времени и пространства для размышлений и направляя её по плану рассуждений, она способна правильно ответить (a) Да.

Дополнительный плюс разбиения сложного задания на подзадачи — помогает удержать фокус модели на каждом конкретном этапе.

Например, если попросить `gpt-3.5-turbo-instruct` резюмировать текст на оригинальном языке, модель может переключиться на английский:

&lt;&lt;&lt;CODE_8&gt;>>

&lt;&lt;&lt;CODE_9&gt;>>

Однако если сначала попросить модель определить язык текста, а затем сделать резюме, надёжность возрастает:

&lt;&lt;&lt;CODE_10&gt;>>

&lt;&lt;&lt;CODE_11&gt;>>

## Просите модель объяснять перед тем, как ответить

Ещё одна мощная техника повышения надёжности — попросить модель постепенно рассуждать вслух, а не сразу выдавать конечный ответ. Такой «проговор мысли» значительно увеличивает шансы получить правильный ответ.

### Zero-shot

#### Метод

Опубликованный [Такеши Кодзимой и др. в 2022 году](https://arxiv.org/abs/2205.11916) самый простой способ заставить модель рассуждать — просто добавить в начало ответа фразу `Let's think step by step`. На рисунке 2 показан пример:

[![пример рассуждения zero-shot](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применяя этот трюк к датасету MultiArith, авторы увеличили точность в 4 раза — с 18% до 79%!

[![результаты zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк `Let's think step by step` хорошо работает с математическими задачами, он неэффективен для всех типов задач. Авторы отметили, что он полезен для многошаговых арифметических задач, задач символических рассуждений, стратегий и других рассуждений. Он не помогал при решении простых математических или вопросов здравого смысла, и, вероятно, не поможет во многих нересонансных задачах.

[![результаты zero-shot](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.11916).

Если используете эту технику, не бойтесь экспериментировать с текстом инструкции. Фраза `Let's think step by step` общая, возможно, лучше сработают более строгие и кастомизированные для вашей задачи инструкции. Например, можно попробовать более структурированные варианты: `Сначала подумайте пошагово, почему X может быть верным. Затем поочередно рассмотрите, почему Y может быть верным. Наконец, оцените, что логичнее — X или Y.` Можно даже показать модели пример формата, чтобы помочь ей соблюдать нужный порядок, например:

&lt;&lt;&lt;CODE_12&gt;>>

&lt;&lt;&lt;CODE_13&gt;>>

### Few-shot примеры

#### Метод

Стимулировать модель рассуждать можно и иным способом — показать несколько примеров («few-shot»), как исследовали [Джейсон Вэй и Дэнни Чжоу и др. из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё примеры цепочек рассуждений от разметчиков-человеков:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание, что вызывается сомнение по поводу того, плавают ли груши)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При тестировании на школьных задачах по математике авторы заметили, что prompting с цепочкой рассуждений увеличивал точность более чем в 3 раза — с 18% до 57%.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочка рассуждений улучшила результаты для вопросов по спорту, отслеживанию монет при подбрасывании и конкатенации последних букв. Для насыщения выгоды обычно требовалось не более 8 примеров.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Выводы

Одно из преимуществ few-shot подхода в сравнении с `Let's think step by step` — возможность чётко задавать формат, длину и стиль рассуждений, которые вы хотите видеть. Это особенно полезно, если модель изначально рассуждает неправильно или поверхностно.

### Дообучение

#### Метод

Чтобы выжать максимум из задачи, обычно требуется дообучать кастомную модель. Однако обучение через объяснения может требовать тысячи примеров, что дорого и долго.

В 2022 году Эрик Зеликман и Юхуаи Ву и др. предложили хитрый метод, использующий few-shot prompting для генерации датасета объяснений, подходящего для дообучения. Идея: сгенерировать кандидатские объяснения через few-shot prompt, оставить только те, где ответ верен. Для получения объяснений неверных ответов повторить prompt, но с правильным ответом в вопросе. Авторы назвали этот подход STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Эта техника сочетает преимущества дообучения и prompting с цепочкой рассуждений, не требуя писать тысячи объяснений вручную.

#### Результаты

Применив STaR к датасету Common Sense Q&A, авторы получили, что STaR превзошёл как prompting с цепочкой рассуждений (73% > 37%), так и только дообучение (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot prompting для расширения или модификации датасета дообучения можно расширить и на другие задачи. Например, если у вас много неструктурированного текста, можно использовать промпт для извлечения структурированного датасета, а потом дообучить модель на этих данных.

## Расширения техники prompting с цепочкой рассуждений

Было опубликовано несколько расширений техники prompting с цепочкой рассуждений.

### Selection-inference prompting

#### Метод

Опубликованная Антонией Кресвелл и др. техника разделяет один prompt для объяснений и ответов на части. Сначала prompt выбирает релевантный поднабор фактов из текста («selection prompt»). Затем второй prompt делает выводы из выбранных фактов («inference prompt»). Промпты чередуются, генерируя несколько шагов рассуждений до вывода окончательного ответа. Идея иллюстрирована на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 миллиардами параметров авторы показывают значительный рост результатов по сравнению с prompting цепочки рассуждений, на задачах bAbi и Proof Writer (обе требуют длинных цепочек рассуждений). Лучший результат — комбинация prompting selection-inference и дообучения.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя приросты большие, эти тесты выбраны именно за длинные цепочки рассуждений. Для задач с немногими шагами рассуждений польза может быть меньше.

Результаты показывают два общих урока:

1. Делите сложные задачи на простые — это повышает надежность и точность; чем атомарнее задача, тем меньше шанс ошибки модели.
2. Максимальный результат достигается сочетанием дообучения и выбранного подхода.

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.09712).

### Архитектура надежных рассуждений

Спустя пару месяцев после публикации selection-inference prompting авторы выпустили дополнение с идеями:

- определять, когда цикл selection-inference должен остановиться или продолжиться
- добавлять функцию ценности для оценки нескольких путей рассуждений и поиска оптимального
- снижать галлюцинации, дообучая модель рассуждать о метках предложений (например, sen1), а не писать сами предложения

#### Метод

В изначальной selection-inference технике чередуются специальные промпты «selection» и «inference» для выбора фактов и вывода из них, создавая цепочку рассуждений.

Авторы добавляют два компонента.

Во-первых, «halter» модель, которая после каждого шага inference оценивает, достаточно ли уже сделано выводов для ответа на вопрос. Если да, то даёт окончательный ответ.

Преимущества halter-модели:

- она контролирует, когда остановить или продолжить цикл selection-inference
- если процесс не останавливается, ответ не выдаётся вовсе — это лучше, чем ошибочный ответ

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавляют функцию ценности, которая оценивает качество шагов рассуждений и ищет лучшее из множества путей. Это отражает общий принцип повышения надежности: не генерировать один ответ, а множество и затем выбирать лучший по некоторой функции, дискриминатору или верификатору.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Помимо этого, авторы используют трюк, снижая галлюцинации несуществующих фактов: дообучают модель рассуждать о метках предложений вместо самих предложений. Это помогает избежать выдумывания фактов, не упомянутых в контексте промпта.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Оценили технику на ProofWriter (не показано) и EntailmentBankQA (показано). Точность заметно выросла, особенно на сложных задачах рассуждения.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Трюк с метками предложений практически полностью ликвидировал галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

В статье много полезных уроков для повышения надежности больших языковых моделей:

- Разбивайте сложные задачи на более простые и надёжные подзадачи
- Генерируйте ответ шаг за шагом, оценивая его по ходу
- Создавайте много вариантов ответов и используйте другую модель или функцию для выбора лучшего
- Боритесь с галлюцинациями, ограничивая допустимые ответы модели (например, метками предложений вместо предложений)
- Максимизируйте производительность моделей через дообучение на специализированных задачах

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Кроме плохих результатов на длинных цепочках рассуждений (где хорош selection-inference), prompting с цепочкой рассуждений особенно слабо работает, когда примеры короткие, а задача длинная.

#### Метод

Least-to-most prompting — ещё один способ разбить задачи рассуждений на более простые надёжные подзадачи. Идея — сначала попросить модель выделить первую подзадачу командой вроде `Чтобы решить {вопрос}, нам нужно сначала решить: "`. Затем, решив подзадачу, прикрепить решение к исходному вопросу и повторять процесс до окончательного ответа.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На тестах с длинными цепочками рассуждений с `code-davinci-002` (оптимизирован под код, но понимает и текст) приросты были от 16% до 99.7%!

[
![Least-to-most prompting результаты последний-буква-конкатенация](/images/least-to-most_tab4.png)
![Least-to-most prompting результаты на SCAN](/images/least-to-most_tab9.png)
![Least-to-most prompting результаты на DROP численные рассуждения](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя приросты впечатляют, измерялись они на узком наборе задач с длинными рассуждениями.

Но это отражает общую идею: повышайте надёжность, (а) разбивая задачу на подзадачи и (б) давая модели больше времени на вычисления.

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.10625).

## Похожие идеи

### Маевтический prompting

#### Метод

В отличие от предыдущих подходов, которые стремятся увеличить вероятность правильных ответов, другой метод использует GPT-3 для генерации дерева возможных объяснений (как правильных, так и неправильных) и анализа их взаимосвязей, чтобы определить, какой набор более вероятен. Эту технику назвали маевтическим prompting, по аналогии с сократическим методом, в работе [Джэхун Чжун и др. в мае 2022](https://arxiv.org/abs/2205.11822) (маевтический — относящийся к сократическому методу задавания вопросов).

Метод сложен и работает так:

- Сначала строится маевтическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинается с многовариантного вопроса или утверждения (например, `Война не может закончиться вничью`)
  - Для каждого возможного ответа с помощью модели генерируется объяснение (например, промпт `Война не может закончиться вничью? Правда, потому что`)
  - Затем модель получает вопрос и объяснение, и надо получить ответ. Если инверсия объяснения (с префиксом `Неверно говорить, что {объяснение}`) меняет ответ, значит объяснение «логически интегрально»
  - Если объяснение не логически интегрально, процесс повторяется рекурсивно, превращая пояснение в новый вопрос с True или False, генерируя новые объяснения
  - В итоге строится дерево объяснений, в каждом листе которого инверсия объяснения меняет ответ модели
- Второй этап — преобразование дерева в граф отношений:
  - Для каждого узла рассчитывается вероятность веры (на основании вероятности ответа True на это объяснение)
  - Для каждой пары узлов модель определяет, является ли одно из них следствием другого или противоречит ему
- Третья стадия — поиск наиболее согласованного набора верований и взятие их за истину:
  - Задача формулируется как задача максимальной удовлетворимости с весами (MAX-SAT), где веса — показатели веры, а ограничения — логические отношения
  - Решатель ищет максимально согласованный набор и считает его истинным

[
![Маевтический prompting](/images/maieutic_fig2.png)
![Маевтический prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты маевтического prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Кроме сложности, один минус метода — он расширяется только на вопросы с вариантами ответов.

Чтобы узнать больше, прочитайте [полную статью](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с ограниченным набором ответов простой способ повысить надежность — сэмплировать много объяснений и ответов с температурой > 0 и выбрать самый частый окончательный ответ.

[![Метод Self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Этот метод повысил точность на 1–24 процента на ряде математических и логических задач. (На графиках показаны результаты модели LaMDA от Google; с использованием более крупной PaLM базовые значения были выше, а приросты меньше.)

[![Результаты Self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя метод прост в реализации, он может быть дорогим: 10 ответов в 10 раз увеличат ваши затраты.

Кроме того, он применим только к задачам с ограниченным числом ответов. Для открытых задач (например, написать стихотворение) необязательно понятно, как определить наиболее частый ответ.

Наконец, этот метод лучше всего работает, когда есть несколько путей или вариантов формулировки ответа; если путь один — метод не поможет. Крайний пример: если задача — сгенерировать один токен, тогда частый токен из 100 генераций равнозначен максимальному вероятностному токену (что можно получить одной генерацией при temperature=0).

### Верификаторы

Другой важный метод повышения качества — обучить верификатор или дискриминатор, который оценивает результаты генеративной модели. Если верификатор отвергает ответ, генеративная модель сэмплируется заново. Во многих случаях проверить правильность проще, чем написать ответ, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи из OpenAI применили этот метод на школьных задачах по математике по следующей схеме:

- Сперва они дообучили модель на вопросах и решениях
- Для каждого вопроса в тренировочном наборе сгенерировали 100 решений
- Каждое решение автоматически пометили как правильное или неправильное по финальному ответу
- На основе размеченных решений дообучили модель верификатора классифицировать верно/неверно
- При тестировании генеративная модель создавала 100 решений, из которых выбирался лучший по баллу верификатора

[![Метод верификаторов](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 175B и 8,000 примерами обучение подняло точность школьной математики с ~33% до ~55%.

[![Результаты верификаторов](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и self-consistency, этот подход дорог в применении: 100 решений на задачу увеличит затраты примерно в 100 раз.

## Теории надежности

Хотя техники отличаются, все они направлены на повышение надежности в сложных задачах. В основном это достигается:

- разбиением ненадежных операций на более мелкие, надежные (например, selection-inference prompting)
- использованием нескольких шагов и отношений для повышения надежности системы выше надёжности каждой части (например, маевтический prompting)

### Вероятностные графовые модели

Построение надёжной системы из менее надёжных компонентов напоминает вероятностное программирование, и многие методы анализа оттуда применимы к данной области.

В работе _Language Model Cascades_ Дэвид Дохан и др. интерпретируют описанные техники в терминах вероятностных графовых моделей:

#### Prompting с цепочкой рассуждений

[![графовая модель prompting с цепочкой рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Fine-tuned prompting цепочки рассуждений / Self-taught reasoner

[![графовая модель дообученного prompting цепочки рассуждений](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![графовая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формулирование техник в виде вероятностных графовых моделей не всегда помогает решить конкретную задачу, этот подход может быть полезен для выбора, объединения и разработки новых методов.

## Заключительные мысли

Исследования в области больших языковых моделей активно развиваются. Учёные не только совершенствуют сами модели, но и углубляют понимание, как лучше их применять. Чтобы подчеркнуть скорость развития, отметим, что все статьи, рассмотренные выше, опубликованы в последние 12 месяцев (на момент написания в сентябре 2022).

В будущем ждите выхода новых лучших моделей и техник. Даже если некоторые из описанных здесь техник будут заменены, общие принципы, лежащие в их основе, скорее всего, останутся важной частью арсенала любого продвинутого пользователя.

## Библиография

| Урок                                                                                                                   | Статья                                                                                                                                    | Дата     |
| ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на более простые подзадачи (и подумайте о том, чтобы показывать результаты промежуточных шагов) | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт 2021 |
| Повышайте качество, генерируя множество вариантов и выбирая лучший                                                    | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Окт 2021 |
| В задачах рассуждений модели лучше справляются, если рассуждают шаг за шагом до ответа                                  | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Янв 2022 |
| Улучшайте пошаговые рассуждения, генерируя много объяснений с ответами и выбирая самый популярный                      | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Мар 2022 |
| Для дообучения шагающих по рассуждениям моделей можно использовать данные только с вопросами и множественным выбором   | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Мар 2022 |
| Метод пошаговых рассуждений отлично работает даже без примеров                                                        | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Можно улучшить пошаговые рассуждения, чередуя «selection» и «inference» промпты                                          | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных цепочек рассуждений помогает разбивать задачу на шаги                                                       | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022 |
| Модель может анализировать и хорошие, и плохие объяснения, чтобы выбрать наиболее согласованный набор                  | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Эти техники можно рассматривать как вероятностные модели с ненадёжными компонентами                                     | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июл 2022 |
| Галлюцинации можно устранить через работу с метками предложений и использовать «halter» модель для уменьшения ошибок   | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Авг 2022 |