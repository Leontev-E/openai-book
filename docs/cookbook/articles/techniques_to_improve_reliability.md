---
lang: ru
translationOf: openai-cookbook
---

# Техники для повышения надёжности

Что делать, если GPT-3 не справляется с задачей?

- Искать более удачный запрос, который вызывает более надёжные ответы?
- Вкладывать средства в создание тысячи примеров для дообучения кастомной модели?
- Считать, что модель неспособна выполнить задачу, и перейти к следующей?

Простого ответа нет — всё зависит от ситуации. Однако если ваша задача связана с логическим рассуждением или сложностью, стоит попробовать техники из этой статьи, которые помогут создавать более надёжные, высокоэффективные запросы.

## Почему GPT-3 ошибается на сложных задачах

Если бы вас попросили умножить 13 на 17, ответ пришёл бы прямо в голову? Для большинства из нас — скорее нет. Но это не значит, что люди не умеют умножать двузначные числа. С несколькими секундам и листом бумаги вполне реально понять, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 дать задание слишком сложное для того времени, которое требуется на предсказание следующего токена, оно может выдумать неправильный ответ. Но, как и у людей, это не значит, что модель неспособна решить задачу. Если у неё будет достаточно времени и пространства для рассуждений, она всё ещё может дать надёжный ответ.

Например, если задать `gpt-3.5-turbo-instruct` задачу по жонглированию мячами, он ответит неверно:

&lt;&lt;&lt;CODE_0&gt;>>

&lt;&lt;&lt;CODE_1&gt;>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет — на самом деле, если подать запрос с фразой `Let's think step by step`, модель решит задачу правильно:

&lt;&lt;&lt;CODE_2&gt;>>

&lt;&lt;&lt;CODE_3&gt;>>

Конечно, по одному примеру сложно понять, работает ли эта хитрость с `Let's think step by step` в целом или просто повезло в конкретном случае. Но она действительно эффективна. На наборе задач по словесной математике, трюк `Let's think step by step` значительно увеличил точность GPT-3 — с 18% до 79%!

## Возможности модели зависят от контекста

Одной из распространённых концептуальных ошибок при работе с GPT-3 является убеждение, что её возможности фиксированы во всех контекстах. Например, если GPT-3 ошибается на простом вопросе логики, значит оно неспособно решать простую логику.

Однако пример с `Let's think step by step` показывает, что с видимыми ошибками GPT-3 можно справиться, изменив запрос и помогая модели направлять себя к правильному ответу.

## Как повысить надёжность на сложных задачах

Далее в статье описаны техники, которые улучшают надёжность больших языковых моделей в сложных задачах. Хотя некоторые техники специфичны для конкретных типов задач, многие основаны на общих принципах, применимых к широкому спектру задач, например:

- Давайте более чёткие инструкции
- Делите сложные задачи на более простые подзадачи
- Структурируйте инструкцию, чтобы модель оставалась в теме
- Просите модель сначала объяснить, а потом ответить
- Запрашивайте обоснования для множества возможных ответов, затем синтезируйте их
- Генерируйте много вариантов и затем выбирайте лучший
- Дообучайте индивидуальные модели для максимума производительности

## Делите сложные задачи на простые

Один из способов дать модели больше времени и пространства для рассуждений — разбивать задачи на простые части.

Например, рассмотрим задачу, где модели задан вопрос с несколькими вариантами ответа на основе текста — в данном случае игра Clue. При прямом запросе `gpt-3.5-turbo-instruct` не может объединить подсказки 3 и 5 и даёт неправильный ответ:

&lt;&lt;&lt;CODE_4&gt;>>

&lt;&lt;&lt;CODE_5&gt;>>

Хотя подсказки 3 и 5 показывают, что полковник Горчичный был единственным в обсерватории, и у человека в обсерватории был подсвечник, модель не смогла объединить это в правильный ответ (a) Да.

Однако вместо прямого запроса ответа, мы можем разбить задачу на три части:

- Сначала последовательно пройтись по подсказкам и решить, какие из них релевантны
- Затем объединить релевантные подсказки и рассуждать, какой ответ получить
- Наконец, написать финальный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6&gt;>>

&lt;&lt;&lt;CODE_7&gt;>>

Давая модели больше времени и направления для размышлений, она смогла найти правильный ответ (a) Да.

Ещё одно преимущество разбивки сложных инструкций на подзадачи — это помочь модели сфокусироваться на каждой из них.

Например, если попросить `gpt-3.5-turbo-instruct` обобщить текст на исходном языке, модель может неожиданно перейти на английский:

&lt;&lt;&lt;CODE_8&gt;>>

&lt;&lt;&lt;CODE_9&gt;>>

Но если сначала попросить модель определить язык текста, а потом его обобщить, результат становится надёжнее:

&lt;&lt;&lt;CODE_10&gt;>>

&lt;&lt;&lt;CODE_11&gt;>>

## Просите модель объяснять перед тем, как отвечать

Другой эффективный способ повысить надёжность ответов — заставлять модель рассуждать постепенно, а не прыгать сразу к финальному ответу. «Проговаривая» рассуждения вслух, модель с гораздо большей вероятностью придёт к правильному решению.

### Zero-shot

#### Метод

Опубликованный [Такаши Кодзимой и соавторами в 2022 году](https://arxiv.org/abs/2205.11916) самый простой способ подтолкнуть модель к рассуждениям — начать ответы с `Let's think step by step.` На рисунке 2 показан пример:

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ — Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применив этот простой трюк к набору задач MultiArith, авторы нашли, что `Let's think step by step` увеличивает точность вчетверо — с 18% до 79%!

[![результаты zero-shot](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ — Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк `Let's think step by step` очень хорошо работает на математических задачах, он не универсален. Авторы отметили, что он особенно полезен для многошаговых арифметических задач, задач на символические рассуждения, стратегии и другие логические задачи. Он не помогал с простыми арифметическими задачами или вопросами здравого смысла, и, предположительно, не будет полезен во многих других нерешающих задачах.

[![zero-shot reasoning example](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ — Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.11916).

Если вы примените эту технику самостоятельно, не бойтесь экспериментировать с формулировками инструкций. Фраза `Let's think step by step` довольно общая, и вы можете получить лучший результат, если использовать более строгий и адаптированный под задачу формат. Например, можно попробовать структурированные варианты вроде `Во-первых, подумай шаг за шагом, почему X может быть верным. Во-вторых, подумай шаг за шагом, почему Y может быть верным. В-третьих, подумай шаг за шагом, что из X или Y более вероятно.` Также можно привести пример формата, который помогает модели не сбиваться:

&lt;&lt;&lt;CODE_12&gt;>>

&lt;&lt;&lt;CODE_13&gt;>>

### Few-shot примеры

#### Метод

Побуждение модели рассуждать можно делать разными способами. Один из них — показать несколько примеров («few-shot»), как изучали [Джейсон Вэй и Денни Чжоу с Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример запроса с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Дополнительные примеры рассуждений, написанные людьми:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание, что вызывает сомнения, плавают ли действительно груши)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

На задачах школьной математики авторы обнаружили, что запрос с цепочкой рассуждений увеличивает точность втрое — с 18% до 57%.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Кроме математики, цепочка рассуждений повышала точность в вопросах о спорте, отслеживании подбрасывания монеты и конкатенации последних букв. В большинстве случаев достаточно было менее 8 примеров, чтобы достичь максимальной эффективности.

[![результаты цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ — Jason Wei and Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Подробнее в [полной статье](https://arxiv.org/abs/2201.11903).

#### Выводы

Одно из преимуществ подхода с несколькими примерами перед техникой `Let's think step by step` в том, что с ним легче задать формат, длину и стиль рассуждений, которые вы хотите получить от модели перед финальным ответом. Это особенно полезно, когда модель изначально не рассуждает правильно или недостаточно глубоко.

### Fine-tuned

#### Метод

В общем случае, чтобы вытянуть максимум из задачи, нужно дообучить кастомную модель. Однако дообучение с объяснениями может потребовать тысячи примеров, что дорого создавать.

В 2022 году Эрик Зеликман и Юхуай Ву опубликовали умный метод, использующий few-shot запрос для генерации набора данных с объяснениями, для дообучения модели. Идея — использовать few-shot запрос для генерации кандидатских объяснений и оставлять только объяснения, приводящие к правильному ответу. Чтобы получить объяснения для неверных ответов, повторять запрос, но указывать правильные ответы как часть вопроса. Авторы назвали процедуру STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ — Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

С этой техникой можно совместить преимущества дообучения и цепочки рассуждений без необходимости создавать тысячи объяснений.

#### Результаты

Применив технику к набору данных Common Sense Q&A, авторы увидели, что STaR превзошёл как рядовую цепочку рассуждений (73% > 37%), так и только fine-tuning (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ — Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Подробнее — в [полной статье](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot запроса для расширения или модификации датасета дообучения — идея, которую можно применять не только к написанию объяснений. Например, если у вас есть много неструктурированного текста для обучения, вы можете попытаться с помощью запроса извлечь из него структурированный набор данных, а затем дообучить модель на нём.

## Расширения цепочки рассуждений

Опубликовано несколько расширений цепочки рассуждений.

### Selection-inference prompting

#### Метод

Предложенный Антонией Кресвелл и соавторами, один из вариантов техники — разделить запрос для генерации объяснений и ответов на более мелкие части. Сначала один запрос выбирает релевантные факты из текста («selection prompt»). Потом второй запрос генерирует вывод из выбранных фактов («inference prompt»). Эти запросы чередуются, формируя несколько шагов рассуждений и в итоге приводя к финальному ответу. Авторы показывают идею на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели с 7 млрд параметров авторы обнаружили, что selection-inference prompting значительно улучшает качество по сравнению с цепочкой рассуждений на задачах bAbi и Proof Writer (обе требуют длинных последовательностей рассуждений). Лучший результат достигается в комбинации selection-inference с тонкой настройкой.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя улучшения на этих бенчмарках значительны, их выбирали именно за требование длинных цепочек рассуждений. Для задач с небольшим числом шагов выигрыш, вероятно, будет меньше.

Результаты подчеркивают два общих урока при работе с большими языковыми моделями. Во-первых, деление сложных задач на более мелкие — отличный способ повысить надёжность и эффективность; чем атомарнее задача, тем меньше у модели шансов ошибиться. Во-вторых, максимальная производительность часто достигается комбинацией тонкой настройки и выбранного подхода.

Подробнее читайте в [полной статье](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture

Спустя несколько месяцев после публикации selection-inference prompting, авторы дополнили технику в следующей работе, предложив:

- определять, когда цикл selection-inference следует остановить или продолжить
- добавлять функцию оценки (value function) для поиска среди множества путей рассуждений
- снижать галлюцинации выдуманных фактов, дообучая модель рассуждать об обозначениях предложений (например, sen1), а не записывать целиком предложения

#### Метод

В исходной технике selection-inference поочерёдно используются специализированные запросы для выбора фактов и вывода логических заключений, формируя цепочку рассуждений.

Авторы добавили два компонента.

Сначала 'halter' — модель, которая после каждого шага вывода спрашивается, достаточно ли сделанных выводов для ответа на вопрос. Если да — генерируется финальный ответ.

Достоинства halter:

- может приостанавливать или продолжать процесс selection-inference в зависимости от необходимости
- если процесс так и не остановится, ответ не будет сгенерирован, что лучше, чем выдуманный ответ

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Во-вторых, добавлена функция оценки, которая помогает оценивать качество рассуждений и искать по разным траекториям рассуждений. Это соответствует общему приёму повышения надёжности: вместо одного варианта генерировать множество ответов и затем выбирать лучший с помощью функции оценки / дискриминатора / верификатора.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, с помощью хитрости с обозначениями предложений (например, sen1) вместо полного текста удалось значительно снизить количество выдуманных фактов.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Оценка техники проводилась на двух бенчмарках: ProofWriter (не показан) и [EntailmentBankQA](https://allenai.org/data/entailmentbank) (на рисунках). Техника значительно увеличила точность, особенно на сложных задачах рассуждений.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, трюк с обозначениями предложений практически полностью устранил галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ — Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Эта статья содержит ряд полезных уроков для повышения надёжности больших языковых моделей:

- Делите сложные задачи на более мелкие, надёжные подзадачи
- Формируйте ответ пошагово, оценивая результат по ходу
- Генерируйте множество вариантов и используйте отдельную модель или функцию для выбора лучшего
- Снижайте галлюцинации, ограничивая то, что модель может говорить (например, с помощью обозначений предложений)
- Добивайтесь максимума производительности моделей через их дообучение на специализированных задачах

Подробнее — в [полной статье](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting

Кроме проблем с длинными цепочками рассуждений (где отлично проявляет себя selection-inference), цепочка рассуждений может испытывать сложности, если примеры короткие, а задача длинная.

#### Метод

Least-to-most prompting — ещё одна техника разбиения задач рассуждений на более мелкие и надёжные подзадачи. Идея — получить подзадачу, задав модель с чем-то вроде `To solve {question}, we need to first solve: "`. Затем, имея подзадачу, модель её решает. Решение добавляется к исходному вопросу, и процесс повторяется до финального ответа.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ — Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На бенчмарках с длинными цепочками рассуждений с использованием `code-davinci-002` (оптимизированного на код, но понимающего текст) прирост достигал от 16% до 99.7%!

[
![Результаты least-to-most на last-letter-concatenation](/images/least-to-most_tab4.png)
![Результаты least-to-most на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ — Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя перечисленные приросты впечатляют, они сделаны по очень узкому набору задач с длинными цепочками рассуждений.

Тем не менее, они подчёркивают общую идею: повысьте надёжность, (а) разбив сложную задачу на более мелкие части и (б) дав модели больше времени и пространства для решения.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.10625).

## Похожие идеи

### Маевтическое (маиетическое) побуждение

#### Метод

В отличие от предыдущих методов, стремящихся максимизировать вероятность правильных ответов, другой подход — попросить GPT-3 построить дерево возможных объяснений (как правильных, так и ошибочных), а затем проанализировать их взаимосвязи, чтобы определить, какое множество объяснений правильное. Этот метод назван maieutic prompting (маиетическим побуждением) в статье [Джехуна Джунга и соавторов в мае 2022](https://arxiv.org/abs/2205.11822) (маиетика связана с сократическим методом задавания вопросов для вызова идей).

Сложный метод работает так:

- Сначала построить маиетическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начать с вопроса с несколькими вариантами ответов или с истинно/ложного утверждения (например, `War cannot have a tie`)
  - Для каждого варианта ответа использовать модель, чтобы сгенерировать объяснение (например, с запросом `War cannot have a tie? True, because`)
  - Затем запросить модель ответить на вопрос с этим объяснением. Если изменение объяснения на противоположное (с префиксом вроде `It is wrong to say that {explanation}`) меняет ответ модели, объяснение считается «логически целостным»
  - Если объяснение не целостное, повторить процесс рекурсивно, превращая объяснение в отдельный вопрос True/False и генерируя для него объяснения
  - В итоге получается дерево объяснений, где каждый лист имеет свойство изменения ответа модели при инверсии объяснения
- Затем преобразовать дерево в граф взаимосвязей:
  - Для каждого узла дерева вычисляется относительная уверенность модели (основываясь на вероятности ответа `True`, заданного объяснением)
  - Для каждой пары узлов модель определяет, подразумевают ли они (entail) или противоречат друг другу
- Затем найти максимально согласованный набор убеждений и принять их за истинные:
  - Конкретно, используя силу веры в каждый узел и логические отношения между ними, сформулировать задачу как задачу максимальной выполнимости со взвешиванием (MAX-SAT)
  - Решателем найти самый самосогласованный набор убеждений и принять его как истину

[
![Маевтическое побуждение](/images/maieutic_fig2.png)
![Маевтическое побуждение](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ — Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ — Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Помимо сложности реализации, метод, по-видимому, применим только к вопросам, формализуемым как выбор из вариантов.

Подробнее — в [полной статье](https://arxiv.org/abs/2205.11822).

## Расширения

### Self-consistency

#### Метод

Для задач с дискретным множеством ответов простой способ повысить надёжность — сгенерировать множество объяснений и ответов из модели (с температурой > 0), затем выбрать наиболее часто встречающийся итоговый ответ.

[![Метод self-consistency](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ — Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Техника повысила точность от 1 до 24 процентных пунктов на различных математических и логических наборах данных. (Ниже результаты по модели LaMDA от Google; для более крупной модели PaLM базовые показатели были выше, а эффект чуть меньше.)

[![Результаты self-consistency](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ — Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Хотя техника простая, она может быть дорогой: генерация 10 ответов увеличивает расходы в 10 раз.

Также, как и многие подходы, она применима только к задачам с ограниченным набором ответов. Для открытых задач, где каждый ответ уникален (например, написание поэмы), выбор «самого частого» не имеет смысла.

Наконец, техника наиболее полезна, если возможно несколько путей или формулировок решения; при одном единственном пути эффект может отсутствовать. Например, если задача сгенерировать один токен, выбор наиболее частого из 100 генераций равносилен выбору токена с максимальной вероятностью (то есть нулевая температура, одного прохода достаточно).

### Верификаторы

Ещё один важный приём для повышения качества — обучить модель-проверяющую (верификатор, дискриминатор), оценивающую ответы основной генеративной модели. Если верификатор отвергает ответ, генерация повторяется. Часто проще оценить, чем придумать ответ, что объясняет эффективность этого метода.

#### Метод

В 2021 году исследователи OpenAI применили метод к задачам школьной математики:

- Сначала дообучили модель на наборе вопросов и ответов
- Для каждого вопроса из обучающего набора сгенерировали 100 ответов
- Каждое из 100 решений автоматически помечено как правильное или неправильное по итоговому ответу
- На этих примерах дообучили верификатор классифицировать верность решения
- При тестировании генеративная модель создаёт 100 решений, и выбирается то, что набрало наибольшую оценку верификатора

[![Метод верификаторов](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ — Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С моделью GPT-3 175B и 8,000 примерами этот метод существенно увеличил точность школьной математики с ~33% до ~55%.

[![Результаты верификаторов](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ — Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как и self-consistency, метод может быть дорогим, так как генерация 100 ответов на задачу уходит в ~100 раз дороже.

## Теории надёжности

Хотя методы выше разнообразны, у них общая цель — повысить надёжность в сложных задачах. В основном они этого достигают через:

- разбиение ненадёжных операций на более мелкие, надёжные (например, selection-inference prompting)
- использование нескольких шагов и множественных связей для повышения надёжности сверх уровня отдельного компонента (например, maieutic prompting)

### Вероятностные графические модели

Парадигма построения надёжной системы из менее надёжных компонентов напоминает вероятностное программирование, и многие методы анализа из той области применимы и здесь.

В статье _Language Model Cascades_ Дэвид Доган и соавторы рассматривают описанные методы в рамках вероятностных графических моделей:

#### Цепочка рассуждений

[![Графическая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ — David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Fine-tuned chain of thought prompting / Self-taught reasoner

[![Графическая модель для fine-tuning цепочки рассуждений](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ — David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![Графическая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ — David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Верификаторы

[![Графическая модель верификаторов](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ — David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формулировка методов как вероятностных графических моделей не всегда даёт прямое решение, она помогает при выборе, комбинировании и поиске новых методов.

## Заключительные мысли

Исследования больших языковых моделей очень активны и стремительно развиваются. Учёные не только улучшают модели, но и понимание того, как правильно их применять. Чтобы оценить темп, все упомянутые статьи опубликованы за последние 12 месяцев (на момент сентябрь 2022).

В будущем стоит ждать улучшенных моделей и техник. Даже если конкретные методы будут заменены, общие принципы, лежащие в их основе, останутся ключевой частью арсенала любого эксперта.

## Список литературы

| Урок                                                                                                                          | Статья                                                                                                                                        | Дата     |
| ----------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Делите сложные задачи на подзадачи (и, возможно, показывайте промежуточные результаты пользователям)                          | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691)      | Окт 2021 |
| Улучшайте результат, генерируя множество кандидатов и выбирая лучший из них                                                  | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                         | Окт 2021 |
| На задачах рассуждений модели лучше работают, когда рассуждают шаг за шагом перед ответом                                     | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                    | Янв 2022 |
| Повышайте эффективность рассуждений, генерируя множество объяснений-ответов и выбирая самый популярный                      | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                                  | Мар 2022 |
| Если хотите дообучать модель рассуждений шаг за шагом, можете использовать данные с вопросами и ответами с выбором из вариантов | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                             | Мар 2022 |
| Метод пошаговых рассуждений успешен даже в zero-shot режиме                                                                   | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                            | Май 2022 |
| Улучшайте пошаговые рассуждения, чередуя «selection» и «inference» запросы                                                     | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)                  | Май 2022 |
| На длинных рассуждениях улучшайте качество, разбивая проблему на части и решая по очереди                                    | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                               | Май 2022 |
| Можете проанализировать и правильные, и ошибочные объяснения, чтобы найти максимально согласованный набор объяснений         | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                           | Май 2022 |
| Можно рассматривать эти методы в парадигме вероятностного программирования, где система строится из недостоверных компонентов | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                                    | Июль 2022 |
| Устраняйте галлюцинации с помощью манипуляций с обозначениями предложений и уменьшайте ошибки с помощью «halter» запроса     | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                           | Авг 2022 |