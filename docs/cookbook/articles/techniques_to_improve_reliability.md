---
lang: ru
translationOf: openai-cookbook
---

# Методы повышения надежности

Что делать, если GPT-3 не справляется с задачей?

- Искать лучший запрос, который вызовет более надежные ответы?
- Тратить тысячи примеров на дообучение кастомной модели?
- Предположить, что модель не способна решить задачу, и перейти к следующей?

Простого ответа нет — всё зависит от ситуации. Однако если ваша задача включает логическое мышление или требует сложных рассуждений, рассмотрите применение методов из этой статьи для создания более надежных и эффективных запросов.

## Почему GPT-3 ошибается в сложных задачах

Если бы вас попросили умножить 13 на 17, сразу ли ответ возник бы у вас в голове? Для большинства из нас — скорее нет. Но это не значит, что человек не умеет умножать двузначные числа. Немного времени, лист бумаги и ручка — и несложно прийти к ответу: 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если дать GPT-3 задачу слишком сложную, чтобы она могла решить её за время вычисления следующего токена, модель может сгенерировать ошибочный ответ. Но, как и у людей, это не значит, что модель не способна выполнить задачу. Если дать ей время и возможность рассуждать, модель вполне может выдать надежный ответ.

Например, если задать &lt;&lt;&lt;INL_0>>> следующую математическую задачу про жонглирование мячами, он ответит неправильно:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет; на самом деле, если подсказать модели &lt;&lt;&lt;INL_1>>>, она надежно решит задачу:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, по одному примеру сложно понять, работает ли трюк с &lt;&lt;&lt;INL_2>>> в общем случае или это просто удача. Но на самом деле он работает. На бенчмарке задач по словам и математике трюк с &lt;&lt;&lt;INL_3>>> повысил точность решения GPT-3 с 18% — абсолютно неудовлетворительного уровня — до «приличных» 79%!

## Возможности модели зависят от контекста

При работе с GPT-3 часто встречается заблуждение, что возможности модели фиксированы для всех контекстов. Например, если GPT-3 ошибается в простом вопросе логики, значит, она неспособна решать простые логические задачи.

Однако, как показывает пример с &lt;&lt;&lt;INL_4>>>, видимые ошибки GPT-3 иногда можно исправить, подобрав более удачный запрос, который поможет модели направить себя к правильному ответу.

## Как повысить надежность в сложных задачах

В остальной части статьи мы рассмотрим методы повышения надежности крупных языковых моделей на сложных задачах. Некоторые методы специфичны для определенных типов задач, но многие основаны на общих принципах, применимых в широком спектре задач:

- Давать более четкие инструкции
- Делить сложные задачи на более простые подзадачи
- Структурировать инструкции, чтобы удерживать модель в теме
- Предлагать модели объяснить решение перед ответом
- Запрашивать обоснования для множества возможных ответов и потом синтезировать их
- Генерировать множество вариантов и использовать модель для выбора лучшего
- Дообучать кастомные модели для максимизации производительности

## Деление сложных задач на более простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачи на более простые части.

Например, возьмем задачу, в которой мы задаём модели вопрос с вариантами ответов по тексту — в нашем случае по игре "Клуэ". Если спросить напрямую, &lt;&lt;&lt;INL_5>>> не может объединить улики 3 и 5 и отвечает неверно:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя улики 3 и 5 показывают, что полковник Горчичный был единственным человеком в обсерватории и что у этого человека был подсвечник, модель не связывает эти факты в правильный ответ (a) Да.

Однако, вместо того чтобы спрашивать ответ напрямую, задачу можно разбить на три части:

- Сначала проанализировать улики по одной и решить, какие из них могут быть релевантны
- Затем объединить релевантные улики и рассуждать, чтобы ответить на вопрос
- И наконец, записать окончательный ответ: (a), (b) или (c)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Дав модели больше времени и пространства для обдумывания и направляя ее пошаговым планом рассуждений, мы добиваемся правильного ответа (a) Да.

Еще одно преимущество разбиения сложных инструкций на более мелкие подзадачи — помощь модели в концентрации на каждой подзадаче.

Например, если попросить &lt;&lt;&lt;INL_6>>> сделать резюме текста на его исходном языке, модель может «соскочить» на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Однако если сначала попросить модель определить язык текста, а потом сделать резюме, она будет работать надежнее:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Предлагайте модели объяснить перед тем, как ответить

Еще один мощный способ повысить надежность ответов — попросить модель шаг за шагом обдумать ответ, а не сразу выдавать финальное решение. Выражая «мысли вслух», модель значительно чаще приходит к правильному ответу.

### Zero-shot (без примеров)

#### Метод

Опубликованный [Такеши Кодзимой и соавторами в 2022 году](https://arxiv.org/abs/2205.11916), самый простой способ заставить модель рассуждать — просто добавить в начало ответа &lt;&lt;&lt;INL_7>>>. На рисунке 2 показан пример:

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

Применяя этот простой трюк к набору математических задач MultiArith, авторы получили &lt;&lt;&lt;INL_8>>> четырехкратное увеличение точности — с 18% до 79%!

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Последствия

Хотя трюк &lt;&lt;&lt;INL_9>>> хорошо работает на математических задачах, он неэффективен во всех случаях. Авторы выяснили, что он наиболее полезен для многошаговых арифметических задач, задач символических рассуждений, стратегий и прочих проблем, требующих рассуждения. Он не помог простым математическим задачам и вопросам здравого смысла, и, вероятно, не будет полезен для многих других задач, не связанных с рассуждением.

[![пример zero-shot рассуждений](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_ by Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Для более подробной информации прочитайте [полный доклад](https://arxiv.org/abs/2205.11916).

Если собираетесь применять этот метод, не бойтесь экспериментировать с формой инструкции. &lt;&lt;&lt;INL_10>>> достаточно универсальна, но вы можете получить лучший результат, если разработаете более строгий шаблон под вашу задачу. Например, можно попробовать более структурированные варианты вроде &lt;&lt;&lt;INL_11>>>. Также можно привести модели пример формата, чтобы удерживать её в нужном русле, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### Few-shot (с примерами)

#### Метод

Принудить модель рассуждать можно и иным способом — продемонстрировать несколько примеров «цепочек рассуждений» («few-shot»). Такой подход исследовали [Джейсон Вей и Дэнни Чжоу с коллегами из Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Вот пример few-shot запроса с цепочкой рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вей и Дэнни Чжоу с соавторами (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Еще больше примеров цепочек рассуждений, написанных людьми:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вей и Дэнни Чжоу с соавторами (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Обратите внимание, что поднимается вопрос, действительно ли груши плавают)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

При проверке на школьных математических задачах авторы обнаружили, что цепочки рассуждений увеличили точность решения втрое — с 18% до 57%.

[![пример цепочки рассуждений](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вей и Дэнни Чжоу с соавторами (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Цепочки рассуждений также улучшили результаты по вопросам, связанным со спортом, отслеживанием подбрасывания монеты и конкатенацией последних букв. Как правило, для достижения максимума роста потребовалось не более восьми примеров.

[![пример цепочки рассуждений](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_ Джейсон Вей и Дэнни Чжоу с соавторами (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Для более подробной информации прочитайте [полный доклад](https://arxiv.org/abs/2201.11903).

#### Последствия

Преимущество few-shot подхода перед &lt;&lt;&lt;INL_12>>> в том, что вы можете задавать более точный шаблон, длину и стиль рассуждений, которым должна следовать модель перед окончательным ответом. Это особенно полезно, если модель изначально рассуждает некорректно или поверхностно.

### Дообучение

#### Метод

Для максимальной производительности на задаче обычно требуется дообучить отдельную модель. Тем не менее, дообучение с использованием объяснений может потребовать тысячи примеров с объяснениями — что дорогостояще.

В 2022 году Эрик Зеликман, Юхауи Ву и коллеги представили умный метод создания датасета объяснений с помощью few-shot запроса для последующего дообучения. Идея: генерировать кандидаты объяснений в few-shot режиме и отбирать только те, которые приводят к правильному ответу. Для некоторых неправильных ответов делается повторный запрос с правильным ответом, включенным в вопрос. Авторы назвали метод STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхауи Ву с коллегами (2022)](https://arxiv.org/abs/2203.14465)

Этот метод соединяет преимущества дообучения с преимущества цепочек рассуждений, при этом не требуя тысячи написанных вручную объяснений.

#### Результаты

При применении к набору данных вопросов здравого смысла STaR показал лучшие результаты, чем только цепочки рассуждений (73% против 37%) и чем только дообучение (73% против 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_ Эрик Зеликман и Юхауи Ву с коллегами (2022)](https://arxiv.org/abs/2203.14465)

Для подробностей — [полный доклад](https://arxiv.org/abs/2203.14465).

#### Последствия

Использование few-shot запроса для расширения или изменения датасета дообучения — идея, которую можно обобщать за рамки написания объяснений. Например, если у вас есть много неструктурированного текста, вы можете пытаться с помощью запроса извлечь из него структурированный датасет и дообучать модель на нем.

## Расширения цепочек рассуждений

Опубликовано несколько расширений для техники цепочек рассуждений.

### Selection-inference prompting (пошаговый выбор и вывод)

#### Метод

Антония Кресвелл с коллегами предложили разделять один запрос на генерацию объяснений и ответов на меньшие части. Сначала специальный запрос выбирает релевантный поднабор фактов из текста ("selection prompt"). Затем второй запрос делает выводы на основе выбранных фактов ("inference prompt"). Эти запросы чередуются в цикле, формируя последовательность рассуждений и приводя к финальному ответу. На рисунке проиллюстрирована идея:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

Для модели с 7 млрд параметров авторы обнаружили значительное улучшение качества по сравнению с цепочками рассуждений в задачах bAbi и Proof Writer (требующих длинных цепочек рассуждений). Лучшие результаты достигались при комбинировании selection-inference и дообучения.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2205.09712)

#### Последствия

Хотя результаты на этих бенчмарках впечатляют, выбранные задачи специально требовали длинных цепочек рассуждений. Для задач с несколькими шагами преимущество меньше.

Выводы:

- Разбивать сложные задачи на более мелкие увеличивает надежность и качество.
- Для максимальной производительности часто требуется комбинировать дообучение и выбранные методы.

Подробнее: [полный доклад](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture (архитектура надежных рассуждений)

Через несколько месяцев после публикации selection-inference, авторы расширили метод:

- как решать, когда процесс выбора и вывода следует остановить
- добавили value-функцию для помощи в поиске по множеству путей рассуждений
- снизили генерацию выдуманных фактов, обучая модель работать с метками предложений (например, sen1), а не воспроизводить предложения дословно

#### Метод

Изначально selection-inference представлял собой чередование «selection» и «inference» промтов для выбора фактов и вывода на их основе, формируя цепочку рассуждений.

В новом подходе добавлены два основных компонента.

Первый — модель "halter", которая после каждого шага рассуждения решает, достаточно ли информации для ответа. Если да — генерируется финальный ответ.

Модель "halter" даёт преимущество:

- она руководит циклом, останавливая или продолжая его
- если процесс не завершился, ответа нет — что лучше, чем плодить выдумки

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

Второй компонент — value-функция, которая оценивает качество рассуждений и исследует разные пути вывода, выбирая оптимальный. Таким образом генерируется множество ответов, а затем выбирается лучший.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, чтобы снизить генерацию выдуманных фактов, модель обучается работать с метками предложений, а не писать предложения полностью — это помогает исключить фальшивые данные, не упомянутые в контексте.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы оценили метод на задачах ProofWriter (результаты не показаны) и [EntailmentBankQA](https://allenai.org/data/entailmentbank) — на последних наблюдалось существенное повышение точности, особенно на сложных задачах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

Трюк с метками предложений практически устранил генерацию выдумок!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_ Антония Кресвелл и коллеги (2022)](https://arxiv.org/abs/2208.14271)

#### Последствия

Важные уроки:

- Разбивайте задачи на более надежные подзадачи
- Генерируйте выводы по шагам, оценивая их на каждом этапе
- Генерируйте множество ответов и используйте дополнительную модель или функцию для выбора лучших
- Снижайте генерацию выдумок, ограничивая то, что модель может сказать (например, используя метки вместо предложений)
- Максимизируйте производительность моделей дообучением на специализированных задачах

Подробнее: [полный доклад](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting (от простого к сложному)

Кроме плохой работы на длинных цепочках рассуждений (где selection-inference эффективен), цепочки рассуждений могут плохо работать, если примеры короткие, а задача — длинная.

#### Метод

Least-to-most prompting — ещё один метод, разбивающий рассуждения на более надежные подзадачи. Суть — попросить модель сформулировать подзадачу с помощью запроса вроде &lt;&lt;&lt;INL_13>>>. Имея эту подзадачу, модель генерирует решение. Его добавляют к исходному вопросу, и процесс повторяется, пока не будет получен финальный ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и коллеги (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

На бенчмарках с длинными цепочками рассуждений, используя &lt;&lt;&lt;INL_14>>> (оптимизированный для кода, но понимающий текст), авторы зафиксировали рост точности с 16% до 99,7%!

[
![Результаты least-to-most на last-letter-concatenation](/images/least-to-most_tab4.png)
![Результаты least-to-most на SCAN](/images/least-to-most_tab9.png)
![Результаты least-to-most на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_ Дэнни Чжоу и коллеги (2022)](https://arxiv.org/abs/2205.10625)

#### Последствия

Хотя приросты впечатляют, они измерены на узком наборе задач, требующих длинных цепочек рассуждений.

Тем не менее, общие принципы: повышайте надежность, разбивая задачи на подзадачи и давая модели больше времени и возможностей для обдумывания.

Подробнее: [полный доклад](https://arxiv.org/abs/2205.10625).

## Смежные идеи

### Маевтический prompting

#### Метод

В отличие от методов, максимизирующих вероятность правильных ответов, другой подход — использовать GPT-3 для генерации дерева возможных объяснений (как правильных, так и неправильных) и анализа их взаимосвязей для определения наиболее вероятной части. Термин «маевтический prompting» введен [Джэхуном Джуном и соавторами в мае 2022](https://arxiv.org/abs/2205.11822) (маевтика связана с сократическим методом вопросов).

Метод сложный, работает следующим образом:

- Сначала строится маевтическое дерево, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Начинаем с вопроса с несколькими вариантами или утверждения верно/неверно (например, &lt;&lt;&lt;INL_15>>>)
  - Для каждого варианта ответа модель генерирует соответствующее объяснение (по запросу вроде &lt;&lt;&lt;INL_16>>>)
  - Затем модель с вопросом и объяснением генерирует ответ. Если инверсия объяснения (с префиксом вроде &lt;&lt;&lt;INL_17>>>) меняет ответ на противоположный, объяснение считается «логически целостным»
  - Если объяснение не целостное, процесс повторяется рекурсивно: каждое объяснение превращается во новый вопрос true/false, для которого генерируются новые объяснения
  - Итог: дерево объяснений, каждый лист которого логически целостен
- Затем дерево преобразуется в граф отношений:
  - Для каждого узла вычисляется степень доверия модели (на основе вероятности ответа &lt;&lt;&lt;INL_18>>> на объяснение)
  - Для каждой пары узлов определяется, находятся ли они в отношении импликации или противоречия
- Далее ищется самый согласованный набор убеждений, считающийся истинным:
  - Задача формулируется как задача нахождения максимальной удовлетворимости с весами (MAX-SAT)
  - Решателем находится оптимальный набор убеждений, принятых за истину

[
![Маевтический prompting](/images/maieutic_fig2.png)
![Маевтический prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Джун и коллеги (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![Результаты маевтического prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_ Джэхун Джун и коллеги (2022)](https://arxiv.org/abs/2205.11822)

#### Последствия

Несмотря на свою сложность, данный метод, по-видимому, применим только к вопросам с вариантами ответов.

Подробнее: [полный доклад](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность

#### Метод

Для задач с дискретным множеством ответов простой способ повысить надежность — сгенерировать множество объяснений и ответов (с положительной температурой) и выбрать наиболее часто встречающийся ответ.

[![Метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ванг и коллеги (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Техника повышала точность на 1–24 процентных пункта по математическим и логическим бенчмаркам. (На графиках показаны результаты модели LaMDA от Google; с более крупной моделью PaLM базовый уровень был выше, а прирост — немного меньше.)

[![Результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_ Сюэчжи Ванг и коллеги (2022)](https://arxiv.org/abs/2203.11171)

#### Последствия

Хотя метод прост, он может быть дорогим. Генерация 10 ответов увеличивает стоимость примерно в 10 раз.

Кроме того, как и у многих методов, самосогласованность применима только к задачам с ограниченным набором ответов. Для открытых задач (например, сочинения стихов), где каждый ответ уникален, непонятно, как выбирать самый частотный.

Наконец, метод будет полезен, если существует множество путей или формулировок для достижения ответа; если путь единственный — он может не помочь вовсе. Пример: если задача — сгенерировать одиночный токен, то выбор наиболее частотного токена из 100 поколений не отличается от выбора токена с максимальной вероятностью (что можно сделать с одним прогоном при temperature=0).

### Проверяющие модели (verifiers)

Другой важный метод повышения качества — обучить модель-проверяльщик для оценки выходов основной генеративной модели. Если проверяющий отвергает ответ, повторяем генерацию до получения удовлетворительного результата. Часто ответ оценить проще, чем его создать, что объясняет эффективность метода.

#### Метод

В 2021 году исследователи OpenAI применили метод к школьной математике:

- Сначала дообучили модель на вопросах и решениях
- Для каждой задачи в тренировочном наборе сгенерировали 100 решений
- Каждое решение автоматически пометили как правильное или неправильное, исходя из корректности итогового ответа
- На основе этих данных обучили проверяющую модель для классификации решений на правильные/неправильные
- При тестировании генеративная модель создавала 100 решений, из которых итоговый выбирал проверяющий

[![Метод verifier](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Кобби и коллеги (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

С GPT-3 c 175 млрд параметров и 8 тыс. примеров техника улучшила точность математических задач с примерно 33% до 55%.

[![Результаты verifier](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_ Карл Кобби и коллеги (2021)](https://arxiv.org/abs/2110.14168)

#### Последствия

Как и самосогласованность, метод может оказаться дорогостоящим — генерация 100 решений на задачу увеличит расходы около в 100 раз.

## Теории надежности

Хотя эти методы отличаются подходами, всех их объединяет цель повышения надежности на сложных задачах. В основном это достигается за счет:

- разбиения ненадежной операции на более мелкие и надежные (например, selection-inference prompting)
- использования множества шагов или отношений, чтобы сделать систему более надежной, чем каждый её компонент в отдельности (например, maieutic prompting)

### Вероятностные графовые модели

Такой подход напоминает вероятностное программирование, и многие методы анализа из этой области применимы в данном контексте.

В работе _Language Model Cascades_ Дэвид Дохан с коллегами интерпретирует перечисленные методы в терминах вероятностных графовых моделей:

#### Цепочки рассуждений

[![Графовая модель цепочки рассуждений](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и соавторы (2022)](https://arxiv.org/abs/2207.10342)

#### Дообученные цепочки рассуждений / Self-taught reasoner

[![Графовая модель дообученных цепочек](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и соавторы (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![Графовая модель selection-inference](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и соавторы (2022)](https://arxiv.org/abs/2207.10342)

#### Проверяющие модели (verifiers)

[![Графовая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_ Дэвид Дохан и соавторы (2022)](https://arxiv.org/abs/2207.10342)

#### Последствия

Хотя формализация этих методов как вероятностных графовых моделей не всегда сразу решает конкретные задачи, рамки позволяют лучше выбирать, комбинировать и изобретать новые методы.

## Заключительные мысли

Исследования в области крупных языковых моделей очень активны и быстро развиваются. Исследователи не только улучшают сами модели, но и расширяют понимание того, как их лучше использовать. Обратите внимание: все приведённые работы опубликованы в последние 12 месяцев (на момент написания — сентябрь 2022).

В будущем ожидайте новые модели и новые методы. Даже если конкретные приемы здесь устареют, их общие принципы, вероятно, останутся в арсенале опытных пользователей.

## Библиография

| Урок                                                                                                                           | Статья                                                                                                                                  | Дата     |
| ------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Разбивайте сложные задачи на более простые подзадачи (и рассматривайте возможность показывать промежуточные результаты пользователям) | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Октябрь 2021 |
| Можно улучшить результат, сгенерировав много кандидатов и выбрав лучший                                                       | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                       | Октябрь 2021 |
| В задачах рассуждений модели лучше работают, если они отвечают пошагово                                                       | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                | Январь 2022 |
| Можно улучшить пошаговое рассуждение, сгенерировав множество объяснений и ответов и выбрав самый популярный                     | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                              | Март 2022 |
| Для дообучения пошагового рассуждателя можно использовать только данные в формате вопрос-ответ с вариантами                   | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                         | Март 2022 |
| Метод пошагового рассуждения работает отлично даже без примеров                                                               | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                        | Май 2022 |
| Можно улучшить пошаговое рассуждение, чередуя запросы «выбор» и «вывод»                                                       | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)            | Май 2022 |
| В задачах с длинными рассуждениями улучшается качество при решении по частям                                                  | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                           | Май 2022 |
| Модель можно заставить анализировать как хорошее, так и плохое объяснение, чтобы найти наиболее согласованные объяснения     | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                       | Май 2022 |
| Можно рассматривать эти методы в формате вероятностного программирования, где система состоит из не всегда надежных частей     | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                              | Июль 2022 |
| Используйте маркировку предложений и запрос гасящий («halter»), чтобы свести к минимуму галлюцинации                           | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                       | Август 2022 |