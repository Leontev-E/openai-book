---
lang: ru
translationOf: openai-cookbook
---

# Техники повышения надежности

Что делать, когда GPT-3 не справляется с задачей?

- Искать более удачный запрос, который вызовет более надежные ответы?
- Вкладываться в тысячи примеров для дообучения собственной модели?
- Считать, что модель неспособна решить эту задачу, и перейти к другой?

Однозначного ответа нет — всё зависит от ситуации. Однако если ваша задача связана с логическим рассуждением или сложностью, рассмотрите техники из этой статьи для создания более надежных и эффективных запросов.

## Почему GPT-3 не справляется со сложными задачами

Если вас попросить умножить 13 на 17, сразу придет ли ответ? Большинству — скорее нет. Но это не значит, что человек неспособен на умножение двузначных чисел. За пару секунд с бумагой и ручкой несложно вычислить, что 13 × 17 = 130 + 70 + 21 = 221.

Аналогично, если GPT-3 дают слишком сложную задачу, которую нужно решить в рамках времени предсказания следующего токена, он может сфабриковать неправильный ответ. Но, как и у людей, это не означает неспособность модели к решению. При наличии времени и возможностей для рассуждений модель всё же может дать надежный ответ.

Например, если задать &lt;&lt;&lt;INL_0>>> следующую математическую задачу про жонглирование, ответ будет неправильным:

&lt;&lt;&lt;CODE_0>>>

&lt;&lt;&lt;CODE_1>>>

Значит ли это, что GPT-3 не умеет решать простые математические задачи? Нет — на самом деле, при подсказке &lt;&lt;&lt;INL_1>>> модель надежно решает задачу:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;CODE_3>>>

Конечно, трудно судить по одному примеру, действительно ли трюк с &lt;&lt;&lt;INL_2>>> работает в целом или просто повезло. Но он действительно работает. На бенчмарке решений словесных математических задач &lt;&lt;&lt;INL_3>>> существенно повысил точность GPT-3, с ничтожных 18% до достойных 79%!

## Способности модели зависят от контекста

При обучении работе с GPT-3 распространённая ошибка — считать его способности одинаковыми во всех контекстах. Например, если GPT-3 неправильно ответил на простой логический вопрос, значит, он не умеет простую логику.

Но, как показывает пример &lt;&lt;&lt;INL_4>>>, кажущиеся ошибки GPT-3 иногда можно исправить с помощью лучшего запроса, который помогает модели направить себя на правильный ответ.

## Как повысить надежность при сложных задачах

Далее в статье приведены техники повышения надежности больших языковых моделей на сложных задачах. Некоторые из них подходят для конкретных проблем, но многие основаны на общих принципах, применимых к широкому кругу задач, например:

- Давать более чёткие инструкции
- Делить сложные задачи на простые подзадачи
- Структурировать инструкции, чтобы модель не отклонялась от темы
- Просить модель объяснить ход рассуждений перед ответом
- Запрашивать обоснования для нескольких вариантов и синтезировать ответ
- Генерировать множество вариантов ответов и выбирать лучший с помощью модели
- Дополнительно обучать модели для максимизации качества

## Деление сложных задач на простые

Один из способов дать модели больше времени и пространства для размышлений — разбить задачу на простые части.

Например, рассмотрим задачу выбора варианта ответа по тексту — здесь — игре Clue. При прямом вопросе &lt;&lt;&lt;INL_5>>> не удаётся связать подсказки 3 и 5, и ответ неверен:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

Хотя подсказки 3 и 5 указывают, что единственным человеком в обсерватории был полковник Горчица, а у него был подсвечник, модель не сочетает их для правильного ответа (a) Да.

Вместо прямого вопроса можно разбить задание на три этапа:

- Сначала пройтись по подсказкам поочерёдно и оценить, насколько они релевантны
- Затем объединить релевантные подсказки для вывода ответа на вопрос
- В конце записать итоговый ответ: (а), (б) или (в)

&lt;&lt;&lt;CODE_6>>>

&lt;&lt;&lt;CODE_7>>>

Дав модели больше времени и направляя её рассуждения, удаётся получить правильный ответ (а) Да.

Кроме того, деление сложных инструкций на части помогает сосредоточить модель на каждой подзадаче.

Например, при запросе &lt;&lt;&lt;INL_6>>> сделать реферат текста на исходном языке модель может переключиться обратно на английский:

&lt;&lt;&lt;CODE_8>>>

&lt;&lt;&lt;CODE_9>>>

Если сначала попросить модель определить язык, а затем сделать сводку, результат становится более надежным:

&lt;&lt;&lt;CODE_10>>>

&lt;&lt;&lt;CODE_11>>>

## Просите модель объяснить ход рассуждений перед ответом

Ещё одна эффективная техника для надежности — стимулировать модель пошагово выводить рассуждения, а не сразу выдавать итоговый ответ. «Говоря вслух», модель гораздо чаще приходит к правильному решению.

### Без примеров (zero-shot)

#### Метод

Опубликованный в 2022 году [Такеши Кодзимой и соавторами](https://arxiv.org/abs/2205.11916) простой способ подтолкнуть модель к рассуждениям — просто начать ответ с &lt;&lt;&lt;INL_7>>>. На рисунке 2 иллюстрируется пример:

[![пример zero-shot рассуждения](/images/zero-shot_reasoners_fig2.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Результаты

На датасете MultiArith по математике простой трюк помог увеличить точность в 4 раза — с 18% до 79%!

[![результаты zero-shot рассуждения](/images/zero-shot_reasoners_tab5.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

#### Выводы

Хотя трюк с &lt;&lt;&lt;INL_8>>> отлично работает на математических задачах, он не универсален. Авторы обнаружили, что он полезен в задачах с многошаговой арифметикой, символическими рассуждениями, стратегиях и других задачах рассуждения. Это не помогает с простыми арифметическими задачами или вопросами здравого смысла и, видимо, не будет полезно для многих нерефлексивных задач.

[![результаты zero-shot рассуждения](/images/zero-shot_reasoners_tab1.png)
<br />Источник: _Large Language Models are Zero-Shot Reasoners_, Takeshi Kojima et al. (2022).](https://arxiv.org/abs/2205.11916)

Для подробностей прочитайте [полную статью](https://arxiv.org/abs/2205.11916).

Если вы применяете технику, не бойтесь экспериментировать с форматом инструкции. &lt;&lt;&lt;INL_9>>> довольно общий, возможно, лучше подойдут более строгие инструкции, адаптированные под ваш случай. Можно попробовать более структурированные варианты, например &lt;&lt;&lt;INL_10>>>. А также можно привести пример формата для модели, чтобы удерживать её в рамках, например:

&lt;&lt;&lt;CODE_12>>>

&lt;&lt;&lt;CODE_13>>>

### С примерами (few-shot)

#### Метод

Стимулировать модель рассуждать можно по-разному — один из способов — показать несколько примеров («few-shot»), как описали в работе [Джейсон Вэй и Денни Чжоу с Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html). Пример few-shot prompt для цепочки рассуждений:

[![пример цепочки рассуждений](/images/chain_of_thought_fig1.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Ещё примеры цепочек рассуждений, написанные людьми:

[![пример цепочки рассуждений](/images/chain_of_thought_fig3.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

[(Отметим, что вопрос, действительно ли груши плавают, вызывает споры)](https://twitter.com/Meaningness/status/1561062170074370048?s=20&t=mpHt8f3RRboztXxdhLFnWQ)

#### Результаты

Тестирование на школьных математических задачах показало, что цепочка рассуждений увеличивает точность в 3 раза — с 18% до 57%.

[![цепочка рассуждений: результаты](/images/chain_of_thought_fig5.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Помимо математики, такая стимуляция повысила точность в задачах по пониманию спорта, отслеживанию подбрасываний монет и конкатенации букв. Чаще всего достаточного было небольшого числа примеров (меньше 8).

[![цепочка рассуждений: результаты](/images/chain_of_thought_fig11.png)
<br />Источник: _Chain of Thought Prompting Elicits Reasoning in Large Language Models_, Jason Wei и Denny Zhou et al. (2022)](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)

Для подробностей прочитайте [полную статью](https://arxiv.org/abs/2201.11903).

#### Выводы

Преимущество few-shot по сравнению с &lt;&lt;&lt;INL_11>>> — более простое задание формата, длины и стиля рассуждений, которые вы хотите. Это особенно полезно, если модель изначально рассуждает не так, как нужно.

### Дообучение (fine-tuned)

#### Метод

В целом, чтобы добиться максимума результатов, нужно дообучать специализированную модель. Но дообучение на объяснениях требует тысяч примеров, что дорого.

В 2022 году Эрик Зеликман и Юхуаи Ву с соавторами предложили ловкий способ с few-shot подсказками для генерации датасета объяснений, который затем используют для дообучения. Идея: сгенерировать с few-shot пояснения, оставить только те, из которых получился правильный ответ. Для дополнительных пояснений к ошибочным ответам задать few-shot подсказку с правильными ответами как часть вопроса. Авторы назвали метод STaR (Self-taught Reasoner):

[![процедура STaR](/images/star_fig1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Так можно объединить дообучение и цепочку рассуждений без тысячи вручную написанных объяснений.

#### Результаты

На датасете Common Sense Q&A STaR превзошёл и chain-of-thought (73% > 37%), и просто дообучение (73% > 60%):

[![результаты STaR](/images/star_tab1.png)
<br />Источник: _STaR: Bootstrapping Reasoning With Reasoning_, Eric Zelikman и Yujuai Wu et al. (2022)](https://arxiv.org/abs/2203.14465)

Для подробностей прочитайте [полную статью](https://arxiv.org/abs/2203.14465).

#### Выводы

Использование few-shot для расширения/модификации датасета дообучения — идея, которую можно применять не только для объяснений. Если у вас много неструктурированного текста, можно с помощью подсказок привести его к структурированному виду и обучить модель на нём.

## Расширения цепочек рассуждений

Опубликовано несколько расширений цепочек рассуждений.

### Selection-inference prompting (выбор - инференция)

#### Метод

Предложено Антонией Кресвелл и соавторами — разделить запрос на небольшие части. Сначала выбирается релевантный набор фактов из текста («selection prompt»), затем делается вывод из выбранных фактов («inference prompt»). Эти запросы чередуются в цикле для многих шагов рассуждения и в итоге получают ответ. Идею иллюстрируют на рисунке:

[![Selection-inference prompting](/images/selection-inference_fig1.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Результаты

На модели 7B параметров такая техника значительно улучшила результаты на задачах bAbi и Proof Writer (с длительными цепочками рассуждений). Лучший результат — сочетание selection-inference и дообучения.

[![Selection-inference prompting](/images/selection-inference_fig4.png)
<br />Источник: _Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2205.09712)

#### Выводы

Хотя рост результатов на этих бенчмарках был значительным, эти задачи специально выбраны за длинные цепочки рассуждений. На задачах с простыми рассуждениями эффект будет меньше.

Выводы:

- Деление сложных задач на более простые повышает надежность и качество — чем атомарнее задача, тем меньше шанс ошибки
- Максимальный результат часто достигается сочетанием дообучения и выбранного подхода

Для подробностей читайте [полную статью](https://arxiv.org/abs/2205.09712).

### Faithful reasoning architecture (надежная архитектура рассуждений)

Через несколько месяцев после selection-inference авторы дополнили технику идеями:

- как понять, когда прекращать или продолжать цикл выбора-инференции
- добавить функцию оценки для поиска по разным траекториям рассуждений
- уменьшить галлюцинации вымышленных фактов с помощью дообучения, в котором модели дают не текст предложений, а только метки предложений (например, sen1)

#### Метод

В оригинальной технике чередуются специализированные «selection» и «inference» подсказки для выбора фактов и выведения из них — последовательность шагов рассуждений.

Авторы добавляют два компонента.

Первый — модель-прерваватель (halter), которая после каждого шага выводит, достаточно ли сделанных выводов для ответа. Если да — генерируется итоговый ответ.

Преимущества halter-модели:

- она регулирует остановку или продолжение цикла выбора-инференции
- если процесс не останавливается, ответ не будет дан — это лучше, чем выдуманный ответ

[![Faithful reasoning](/images/faithful-reasoning_fig3.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

[![Faithful reasoning](/images/faithful-reasoning_fig5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Второй — функция оценки качества рассуждений, которая используется для выбора лучших путей рассуждения. Это отражает общий принцип повышения надежности: вместо одного ответа генерировать множество, а затем отобрать лучший с помощью функции, дискриминатора или модели-проверяющего.

[![Faithful reasoning](/images/faithful-reasoning_fig7.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Дополнительно, чтобы снизить галлюцинации вымышленных фактов, модель дообучают работать с метками предложений вместо их текстов (например, sen1), что помогает не придумывать факты вне контекста.

[![Faithful reasoning](/images/faithful-reasoning_fig4.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Результаты

Авторы оценили технику на ProofWriter (не показано) и EntailmentBankQA (показано). Техника существенно повысила точность, особенно на сложных вопросах.

![Faithful reasoning](/images/faithful-reasoning_tab2.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

Кроме того, метод меток предложений фактически устранил галлюцинации!

![Faithful reasoning](/images/faithful-reasoning_tab5.png)
<br />Источник: _Faithful Reasoning Using Large Language Models_, Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)

#### Выводы

Несколько важных уроков:

- Делите задачи на более простые, надежные подзадачи
- Создавайте ответы шаг за шагом, оценивая на каждом этапе
- Генерируйте множество ответов и выбирайте лучший с помощью модели на стороне
- Уменьшайте галлюцинации, ограничивая, что можно сказать (например, метки вместо текста)
- Максимизируйте качество моделей, дообучая их на специализированных задачах

Подробнее в [полном тексте](https://arxiv.org/abs/2205.09712).

### Least-to-most prompting (от простого к сложному)

Цепочки рассуждений плохо работают не только на длинных задачах (где selection-inference хорош), но и при коротких примерах, но длинных задачах.

#### Метод

Least-to-most prompting — ещё одна техника разделения рассуждений на более простые подзадачи. Идея — сначала выявить подзадачу с подсказкой вроде &lt;&lt;&lt;INL_12>>>, затем решить её. Решение добавляется к исходному вопросу, процесс повторяется, пока не будет получен итоговый ответ.

[![Least-to-most prompting](/images/least-to-most_fig1.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Результаты

Для задач с длинными цепочками рассуждений и модели &lt;&lt;&lt;INL_13>>> (оптимизированной под код, но понимающей текст) улучшения были от 16% до 99,7%!

[
![результаты Least-to-most auf last-letter-concatenation task](/images/least-to-most_tab4.png)
![результаты Least-to-most на SCAN](/images/least-to-most_tab9.png)
![результаты Least-to-most на DROP numerical reasoning](/images/least-to-most_tab11.png)
<br />Источник: _Least-to-most Prompting Enables Complex Reasoning in Large Language Models_, Denny Zhou et al. (2022)](https://arxiv.org/abs/2205.10625)

#### Выводы

Хотя результаты впечатляют, они относятся к узкому набору задач с длинными цепочками рассуждений.

Но они иллюстрируют важный принцип: повышение надежности за счёт (а) разбивки сложных задач на подзадачи и (б) предоставления модели больше времени и пространства для размышлений.

Подробности — в [полной статье](https://arxiv.org/abs/2205.10625).

## Связанные идеи

### Maieutic prompting (майевтический запрос)

#### Метод

В отличие от предыдущих техник, которые стремятся к максимальной вероятности правильного ответа, другой подход — заставить GPT-3 построить дерево возможных объяснений (и правильных, и ошибочных), а затем проанализировать их взаимосвязи, чтобы определить наиболее правдоподобный набор. Этот метод назван майевтическим (от софистического метода сократических вопросов) [Джэхун Чжунг и др. в мае 2022](https://arxiv.org/abs/2205.11822).

Сложный процесс выглядит так:

- Построение майевтического дерева, где каждый узел — утверждение, которое может быть истинным или ложным:
  - Стартует с вопроса с несколькими вариантами или с утверждения «истина/ложь» (например, &lt;&lt;&lt;INL_14>>>)
  - Для каждого варианта ответа модель генерирует соответствующее объяснение (например, с подсказкой &lt;&lt;&lt;INL_15>>>)
  - Затем по вопросу и объяснению модель генерирует ответ. Если обращение объяснения (с префиксом &lt;&lt;&lt;INL_16>>>) меняет ответ на противоположный, объяснение считается «логически целостным»
  - Если объяснение не логично целостно, процедура повторяется рекурсивно, превращая объяснения в вопросы Истина/Ложь и порождая новые объяснения
  - Итог — дерево, листья которого обладают свойством: обращение объяснения меняет ответ
- Преобразование дерева в граф отношений:
  - Для каждого узла вычисляется относительная степень уверенности (на основе вероятности получить ответ &lt;&lt;&lt;INL_17>>> при данном объяснении)
  - Для каждой пары узлов модель определяет, входят ли они в отношения импликации или противоречат друг другу
- Поиск максимально согласованного набора утверждений и взятие их за истину:
  - Проблема формулируется как задача взвешенного максимального удовлетворения (MAX-SAT)
  - Решатель находит наиболее самосогласованный набор утверждений — они считаются истинными

[
![Maieutic prompting](/images/maieutic_fig2.png)
![Maieutic prompting](/images/maieutic_fig6.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Результаты

[![результаты Maieutic prompting](/images/maieutic_tab1.png)
<br />Источник: _Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations_, Jaehun Jung et al. (2022)](https://arxiv.org/abs/2205.11822)

#### Выводы

Ограничением метода является то, что он применим только к вопросам с вариантами ответов.

Подробнее — [полная статья](https://arxiv.org/abs/2205.11822).

## Расширения

### Самосогласованность

#### Метод

Для задач с дискретным набором ответов простой способ повысить надежность — сэмплировать несколько объяснений и ответов (с положительной температурой), а затем выбрать наиболее часто встречающийся ответ.

[![метод самосогласованности](/images/self-consistency_fig1.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Результаты

Техника повышала точность от 1 до 24 процентных пунктов на разных математических и логических бенчмарках. На рисунке — результаты для модели LaMDA; для более крупной PaLM базовые результаты выше, но прирост меньше.

[![результаты самосогласованности](/images/self-consistency_fig3.png)
<br />Источник: _Self-Consistency Improves Chain of Thought Reasoning in Language Models_, Xuezhi Wang et al. (2022)](https://arxiv.org/abs/2203.11171)

#### Выводы

Техника проста, но может дорого обходиться: 10 ответов — в 10 раз больше затрат.

Также, как и многие техники, она применима только к задачам с конечным набором ответов. Для творческих задач с уникальными ответами (например, написание стихов) не ясно, что значит выбирать «самый популярный» ответ.

Кроме того, техника полезна, когда есть несколько путей или формулировок ответа; если путь один, пользы нет. Например, при ответе одним токеном выбор самого частого токена из 100 сэмплов равен выбору токена с максимальным логарифмом вероятности (что можно получить одним запуском с температурой 0).

### Верификаторы

Ключевая техника — обучение модели-проверяющего (верификатора, дискриминатора) для оценки ответов генеративной модели. Если верификатор отвергает ответ, можно пересэмплировать генеративную модель. Часто судить о качестве проще, чем создавать ответ, что объясняет мощь метода.

#### Метод

В 2021 году исследователи OpenAI применили этот метод к школьным задачам по математике:

- Дообучили модель на вопросах и решениях
- Для каждого вопроса сгенерировали 100 решений
- Автоматически пометили решения как правильные/неправильные, проверив окончательный ответ
- Дообучили верификатор для классификации решений на правильные/неправильные
- В процессе тестирования модель генерирует 100 решений, выбирается то, что верификатор оценил выше всех

[![метод верификатора](/images/verifiers_fig3.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Результаты

На GPT-3 с 175 млрд параметров и 8000 примерами точность поднялась примерно с 33% до 55%.

[![результаты верификаторов](/images/verifiers_fig5.png)
<br />Источник: _Training Verifiers to Solve Math Word Problems_, Karl Cobbe et al. (2021)](https://arxiv.org/abs/2110.14168)

#### Выводы

Как в случае с самосогласованностью, метод дорог — 100 решений на задачу увеличивают затраты примерно в 100 раз.

## Теории надежности

Несмотря на разные подходы, все описанные техники направлены на повышение надежности сложных задач, главным образом через:

- разбиение ненадежных операций на меньшие, более надежные (например, selection-inference prompting)
- использование множественных шагов или связей, чтобы повысить общую надежность выше надежности отдельных компонентов (например, maieutic prompting)

### Вероятностные графовые модели

Данный подход, создание надежной системы из менее надежных компонентов, напоминает вероятностное программирование, и методы из этой области можно применять и здесь.

В статье _Language Model Cascades_ Дэвид Дохан и соавторы интерпретируют описанные методы через призму вероятностных графовых моделей:

#### Chain of thought prompting

[![графовая модель chain of thought prompting](/images/lm_cascades_fig1.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Дообученный chain of thought / Self-taught reasoner

[![графовая модель fine-tuned chain of thought prompting](/images/lm_cascades_fig3.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Selection-inference prompting

[![графовая модель selection-inference prompting](/images/lm_cascades_fig4.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Verifiers

[![графовая модель verifiers](/images/lm_cascades_fig5.png)
<br />Источник: _Language Model Cascades_, David Dohan et al. (2022)](https://arxiv.org/abs/2207.10342)

#### Выводы

Хотя формализация в виде графовых моделей не всегда сразу полезна для решения конкретной задачи, эта структура помогает выбирать, комбинировать и разрабатывать новые методы.

## Заключение

Исследования больших языковых моделей продолжаются и развиваются стремительно. Учёные не только улучшают модели, но и наше понимание их использования. Чтобы почувствовать темп, все статьи выше опубликованы за последние 12 месяцев (на момент написания в сентябре 2022).

В будущем ждите появления новых моделей и техник. Даже если перечисленные методы устареют, общие принципы останутся ключевой частью арсенала экспертов.

## Библиография

| Урок                                                                                                                          | Статья                                                                                                                                     | Дата     |
| ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | -------- |
| Делите сложные задачи на более простые подзадачи (и при возможности показывайте промежуточные результаты пользователям)         | [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) | Окт 2021 |
| Улучшайте ответы, генерируя много кандидатов и выбирая лучший                                                                | [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)                                                        | Окт 2021 |
| На задачах рассуждения модели лучше работают, если рассуждают шаг за шагом перед ответом                                        | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                 | Янв 2022 |
| Улучшайте пошаговые рассуждения, генерируя множество пара объяснение-ответ и выбирая самый популярный                         | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                               | Мар 2022 |
| Для дообучения пошаговых рассуждателей достаточно использовать данные с вопросами и множественным выбором ответов            | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)                                                          | Мар 2022 |
| Метод пошаговых рассуждений отлично работает даже без примеров                                                                | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                                                         | Май 2022 |
| Улучшайте рассуждения, чередуя подсказки для выбора фактов и для вывода выводов                                               | [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.09712)             | Май 2022 |
| Для длинных цепочек рассуждений делите проблему на части и решайте поэтапно                                                   | [Least-to-most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                            | Май 2022 |
| Анализируйте одновременно правильные и ошибочные объяснения, чтобы найти наиболее согласованное множество                     | [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)                        | Май 2022 |
| Представляйте техники в виде вероятностных программ, где система состоит из ненадежных компонентов                             | [Language Model Cascades](https://arxiv.org/abs/2207.10342)                                                                               | Июл 2022 |
| Избегайте галлюцинаций с помощью меток предложений и ограничивающего применения прервавателя                                   | [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)                                                        | Авг 2022 |