---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения связности или сходства между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах бесконтрольного обучения и обучения с переносом.

Эмбеддинги можно применять для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для дополнительной информации прочитайте объявления в блоге OpenAI:

- [Введение эмбеддингов текста и кода (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска как самостоятельно, так и в составе более сложной системы.

Самый простой способ использовать эмбеддинги для поиска следующий:

- Перед поиском (предварительные вычисления):
  - Разбейте корпус текста на куски размером меньше лимита по токенам (8,191 токен для &lt;&lt;&lt;INL_0>>>)
  - Получите эмбеддинги для каждого текстового куска
  - Сохраните эти эмбеддинги в собственной базе данных или у поставщика векторного поиска, например, [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисления в реальном времени):
  - Получите эмбеддинг поискового запроса
  - Найдите близкие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более сложных системах поиска косинусное сходство эмбеддингов может использоваться как одна из характеристик для ранжирования результатов.

## Ответы на вопросы

Лучший способ получить достоверно честные ответы от GPT-3 — предоставить ему исходные документы, в которых он сможет найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете недорого искать релевантную информацию в корпусе документов и затем передавать эту информацию GPT-3 через подсказку для ответа на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо произвольного текстового запроса на вход подаются элементы из набора.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти оценки косинусного сходства можно использовать либо самостоятельно для ранжирования элементов, либо как характеристики в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучать, вы все же можете использовать тренировочные данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) мы предоставляем пример метода настройки эмбеддингов с помощью тренировочных данных. Идея метода состоит в том, чтобы обучить матрицу, которая умножается на векторы эмбеддингов для получения новых настроенных эмбеддингов. При наличии качественных тренировочных данных эта матрица поможет выделить признаки, релевантные вашим меткам. Аналогично, умножение матрицы можно рассматривать как (а) модификацию эмбеддингов или (б) модификацию функции расстояния, используемой для измерения расстояний между эмбеддингами.