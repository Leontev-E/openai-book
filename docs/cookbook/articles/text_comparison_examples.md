---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Эндпоинт OpenAI API для эмбеддингов](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения связности или сходства между текстовыми фрагментами.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на тестах в задачах неуправляемого обучения и переноса обучения.

Эмбеддинги можно применять для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для дополнительной информации прочитайте объявления в блоге OpenAI:

- [Введение текстовых и кодовых эмбеддингов (янв 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (дек 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска как самостоятельно, так и в составе более крупной системы.

Самый простой способ использования эмбеддингов для поиска следующий:

- До поиска (предварительные вычисления):
  - Разделите корпус текста на части, меньшие лимита по токенам (8,191 токен для `text-embedding-3-small`)
  - Вычислите эмбеддинги для каждой части текста  
  - Сохраните эти эмбеддинги в собственной базе данных или у провайдера векторного поиска, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисления в реальном времени):
  - Вычислите эмбеддинг поискового запроса  
  - Найдите ближайшие эмбеддинги в вашей базе данных  
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](../examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов может использоваться как одна из характеристик среди многих для ранжирования результатов поиска.

## Ответы на вопросы

Лучший способ получить надежно правдивые ответы от GPT-3 — предоставить ему исходные документы, в которых он может найти правильные ответы. Используя описанную выше процедуру семантического поиска, можно дешево находить релевантную информацию в корпусе документов и затем передавать эту информацию GPT-3 в подсказке для ответа на вопрос. Это демонстрируется в [Question_answering_using_embeddings.ipynb](../examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации во многом похожи на поиск, только вместо свободного текстового запроса используются элементы из набора.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](../examples/Recommendation_using_embeddings.ipynb).

Как и в поиске, показатели косинусного сходства можно использовать сами по себе для ранжирования элементов или в качестве признаков в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса моделей эмбеддингов OpenAI нельзя дообучать, вы все же можете использовать тренировочные данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](../examples/Customizing_embeddings.ipynb) приведен пример метода настройки эмбеддингов с помощью тренировочных данных. Идея метода в том, чтобы обучить специальную матрицу, умножая на которую векторы эмбеддингов, получать новые настроенные эмбеддинги. При наличии хороших тренировочных данных эта матрица поможет усилить признаки, релевантные вашим меткам. Эквивалентно можно рассматривать умножение на матрицу как (a) модификацию эмбеддингов либо (b) модификацию функции расстояния для измерения дистанций между эмбеддингами.