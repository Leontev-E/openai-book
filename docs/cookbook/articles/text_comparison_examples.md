---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Эндпоинт OpenAI API для эмбеддингов](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения сходства или связности между кусками текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и переноса обучения.

Эмбеддинги могут использоваться для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для получения дополнительной информации прочитайте объявления в блогах OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Таблицу результатов Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска как самостоятельно, так и как один из признаков в более крупной системе.

Самый простой способ использования эмбеддингов для поиска следующий:

- До поиска (предварительный расчет):
  - Разбейте ваш корпус текстов на части меньше лимита токенов (8 191 токен для &lt;&lt;&lt;INL_0>>>)
  - Преобразуйте (вставьте) каждую часть текста в эмбеддинг
  - Сохраните эти эмбеддинги в вашей базе данных или в векторном поисковом сервисе, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисление в реальном времени):
  - Преобразуйте поисковый запрос в эмбеддинг
  - Найдите ближайшие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов может использоваться как один из признаков при ранжировании результатов.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — предоставить ему исходные документы, в которых оно может найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете недорого искать релевантную информацию в корпусе документов, а затем передавать эту информацию GPT-3 через prompt, чтобы ответить на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо свободного текста на вход подаются элементы из набора.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Как и в случае с поиском, оценки косинусного сходства могут использоваться как самостоятельно для ранжирования элементов, так и в качестве признаков в более сложных алгоритмах ранжирования.

## Кастомизация эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучать, вы все же можете использовать обучающие данные для адаптации эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с помощью обучающих данных. Суть метода — обучить настраиваемую матрицу, которая будет умножаться на векторы эмбеддингов, чтобы получать новые адаптированные эмбеддинги. При хорошем обучающем наборе такая матрица поможет выделить признаки, релевантные вашим меткам. Можно рассматривать умножение на матрицу как (а) модификацию эмбеддингов или (б) модификацию функции расстояния, используемой для измерения расстояний между эмбеддингами.