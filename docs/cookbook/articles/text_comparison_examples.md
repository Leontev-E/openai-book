---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) может использоваться для измерения связности или схожести между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и переносного обучения.

Эмбеддинги можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для получения дополнительной информации прочитайте объявления в блогах OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов см. [Таблицу лидеров Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги могут использоваться для поиска как самостоятельно, так и в составе более сложной системы.

Простейший способ использования эмбеддингов для поиска следующий:

- Перед поиском (предварительный подсчет):
  - Разбейте ваш корпус текста на части меньше лимита токенов (8 191 токен для &lt;&lt;&lt;INL_0>>>)
  - Вычислите эмбеддинги для каждой части текста
  - Сохраните эти эмбеддинги в собственной базе данных или у провайдера векторного поиска, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (живой подсчет):
  - Вычислите эмбеддинг поискового запроса
  - Найдите ближайшие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска можно посмотреть в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов может использоваться как одна из множества характеристик для ранжирования результатов поиска.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — предоставить ему исходные документы, в которых он может найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете эффективно найти релевантную информацию в корпусе документов и затем предоставить эту информацию GPT-3 через подсказку для ответа на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо свободного текстового запроса входными данными являются элементы из множества.

Пример использования эмбеддингов для рекомендаций приведен в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти оценки косинусного сходства можно использовать либо самостоятельно для ранжирования элементов, либо как признаки в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучать, вы всё же можете использовать обучающие данные для настройки эмбеддингов под своё приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с помощью обучающих данных. Идея метода заключается в обучении пользовательской матрицы, на которую умножаются векторы эмбеддингов для получения новых настроенных эмбеддингов. При наличии качественных обучающих данных такая матрица поможет выделить признаки, релевантные вашим обучающим меткам. Вы также можете рассматривать умножение матрицы как (a) модификацию эмбеддингов или (b) модификацию функции расстояния, используемой для измерения расстояния между эмбеддингами.