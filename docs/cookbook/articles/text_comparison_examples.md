---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения связности или сходства между фрагментами текста.

Благодаря пониманию текста GPT-3 эти эмбеддинги [добились результатов уровня современного состояния](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и переноса обучения.

Эмбеддинги можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для получения дополнительной информации прочитайте блоги OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги могут использоваться для поиска как самостоятельно, так и в составе более крупной системы.

Самый простой способ использовать эмбеддинги для поиска следующий:

- Перед поиском (предварительный расчет):
  - Разбейте корпус текста на фрагменты меньше лимита по токенам (8 191 токен для `text-embedding-3-small`)
  - Преобразуйте каждый фрагмент текста в эмбеддинг
  - Сохраните эти эмбеддинги в вашей базе данных или у поставщика векторного поиска, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисление на лету):
  - Преобразуйте поисковый запрос в эмбеддинг
  - Найдите ближайшие эмбеддинги в базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](../examples/Semantic_text_search_using_embeddings.ipynb).

В более сложных системах поиска косинусная близость между эмбеддингами может использоваться как одна из множества характеристик при ранжировании результатов поиска.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — предоставить ему исходные документы, в которых он сможет найти правильные ответы. Используя описанную выше процедуру семантического поиска, можно дешево искать в корпусе документов релевантную информацию, а затем передать эту информацию GPT-3 через prompt, чтобы получить ответ на вопрос. Это продемонстрировано в [Question_answering_using_embeddings.ipynb](../examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо свободного текста на вход подаются элементы из множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](../examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти оценки косинусной близости можно использовать как самостоятельно для ранжирования элементов, так и как характеристики в более крупных алгоритмах ранжирования.

## Кастомизация эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучить, вы тем не менее можете использовать тренировочные данные для адаптации эмбеддингов под свое приложение.

В [Customizing_embeddings.ipynb](../examples/Customizing_embeddings.ipynb) представлен пример метода кастомизации эмбеддингов с использованием тренировочных данных. Идея метода в том, чтобы обучить собственную матрицу для умножения на векторы эмбеддингов, чтобы получить новые кастомизированные эмбеддинги. При наличии хороших тренировочных данных эта матрица поможет выделить особенности, важные для ваших меток обучения. Можно эквивалентно рассматривать умножение матрицы как (a) модификацию эмбеддингов, или (b) изменение функции расстояния, используемой для измерения расстояний между эмбеддингами.