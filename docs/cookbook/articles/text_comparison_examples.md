---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения сходства или связности между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и трансферного обучения.

Эмбеддинги можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти-дубликатов и многого другого.

Для получения дополнительной информации прочитайте анонсы в блоге OpenAI:

- [Представляем эмбеддинги текста и кода (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Таблицу лидеров Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска самостоятельно или в качестве признака в более крупной системе.

Самый простой способ использования эмбеддингов для поиска выглядит следующим образом:

- Перед поиском (предварительное вычисление):
  - Разбейте корпус текста на блоки меньше, чем лимит токенов (8,191 токен для `text-embedding-3-small`)
  - Получите эмбеддинг для каждого блока текста
  - Сохраните эти эмбеддинги в собственной базе данных или у провайдера векторного поиска, например, [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (живое вычисление):
  - Получите эмбеддинг поискового запроса
  - Найдите ближайшие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](../examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов может использоваться как один из признаков при ранжировании результатов.

## Ответы на вопросы

Лучший способ получать достоверно честные ответы от GPT-3 — дать ему исходные документы, в которых можно найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете дешево найти в корпусе документов релевантную информацию, а затем предоставить ее GPT-3 через запрос для ответа на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](../examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации во многом похожи на поиск, за исключением того, что вместо свободного текстового запроса входными данными являются элементы множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](../examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти значения косинусного сходства можно использовать самостоятельно для ранжирования элементов или в качестве признаков в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучать, вы тем не менее можете использовать обучающие данные для адаптации эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](../examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с использованием обучающих данных. Идея метода состоит в обучении собственной матрицы для умножения векторных эмбеддингов с целью получения новых, кастомизированных эмбеддингов. При наличии хороших обучающих данных эта матрица поможет выделить признаки, релевантные вашим меткам. Эквивалентно умножение матрицы можно рассматривать как (а) модификацию эмбеддингов или (б) изменение функции расстояния, используемой для измерения расстояний между эмбеддингами.