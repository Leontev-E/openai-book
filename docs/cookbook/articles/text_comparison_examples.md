---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) может использоваться для измерения связанности или схожести между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и переноса обучения.

Эмбеддинги можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для дополнительной информации прочитайте анонсы в блоге OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Таблицу лидеров Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать в поиске как самостоятельно, так и в качестве признака в более крупной системе.

Самый простой способ использования эмбеддингов для поиска выглядит следующим образом:

- Перед поиском (предварительный расчет):
  - Разделите свой корпус текста на чанки, меньшие лимита токенов (8,191 токен для `text-embedding-3-small`)
  - Получите эмбеддинги для каждого чанка текста
  - Сохраните эти эмбеддинги в собственной базе данных или у провайдера векторного поиска, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисление в реальном времени):
  - Получите эмбеддинг поискового запроса
  - Найдите ближайшие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](../examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусная близость эмбеддингов может использоваться как один из признаков среди многих при ранжировании результатов.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — предоставить ему исходные документы, в которых он может найти правильные ответы. Используя описанную выше процедуру семантического поиска, можно дешево искать релевантную информацию в корпусе документов и затем передавать её GPT-3 через подсказку для ответа на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](../examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо свободного текстового запроса входными данными являются элементы множества.

Пример того, как использовать эмбеддинги для рекомендаций, показан в [Recommendation_using_embeddings.ipynb](../examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти оценки косинусной близости можно использовать самостоятельно для ранжирования элементов или как признаки в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучить, вы всё же можете использовать тренировочные данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](../examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с помощью тренировочных данных. Идея метода заключается в обучении специальной матрицы, с помощью которой умножаются векторы эмбеддингов для получения новых настроенных эмбеддингов. При наличии качественных тренировочных данных эта матрица поможет выделить признаки, релевантные вашим меткам. Вы можете рассматривать матричное умножение либо как (а) модификацию эмбеддингов, либо как (б) модификацию функции расстояния, используемой для измерения расстояний между эмбеддингами.