---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения связности или сходства между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [добились передовых результатов](https://arxiv.org/abs/2201.10005) на тестах в задачах обучения без учителя и переноса обучения.

Эмбеддинги могут использоваться для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти идентичных текстов и многого другого.

Для получения дополнительной информации прочитайте анонсы в блоге OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Рейтинг Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска как самостоятельно, так и в составе более крупной системы.

Самый простой способ использовать эмбеддинги для поиска следующий:

- Перед поиском (предварительные вычисления):
  - Разбейте ваш текстовый корпус на фрагменты, меньшие лимита токенов (8,191 токен для &lt;&lt;&lt;INL_0>>>)
  - Вычислите эмбеддинги для каждого текстового фрагмента
  - Сохраните эти эмбеддинги в собственной базе данных или у вендора векторного поиска, например, [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисления онлайн):
  - Вычислите эмбеддинг поискового запроса
  - Найдите в базе данных эмбеддинги, ближайшие к запросу
  - Верните топовые результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых поисковых системах косинусное сходство эмбеддингов может использоваться как один из множества признаков при ранжировании результатов поиска.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — дать ему исходные документы, в которых он сможет найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете недорого искать релевантную информацию в корпусе документов, а затем передавать эту информацию GPT-3 через подсказку, чтобы он ответил на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, но вместо произвольного текстового запроса входными данными являются элементы из множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Как и в поиске, эти показатели косинусного сходства можно использовать как самостоятельно для ранжирования элементов, так и в качестве признаков в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучить, вы всё же можете использовать тренировочные данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) представлен пример метода настройки эмбеддингов с использованием тренировочных данных. Суть метода в том, чтобы обучить специальную матрицу, на которую умножают векторы эмбеддингов, чтобы получить новые настраиваемые эмбеддинги. При наличии хороших тренировочных данных эта матрица помогает выделять признаки, релевантные вашим меткам обучения. Эквивалентно умножение на матрицу можно рассматривать как (а) модификацию самих эмбеддингов или (б) модификацию функции расстояния, используемой для измерения расстояний между эмбеддингами.