---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения схожести или родства между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли результатов на уровне передового опыта](https://arxiv.org/abs/2201.10005) на бенчмарках в условиях неконтролируемого обучения и переноса обучения.

Эмбеддинги можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для дополнительной информации прочитайте блог OpenAI с анонсами:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Лидеров рейтинга Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска либо самостоятельно, либо как часть более сложной системы.

Самый простой способ использовать эмбеддинги для поиска следующий:

- Перед поиском (предварительный расчёт):
  - Разбейте корпус текста на фрагменты меньше лимита токенов (8,191 токен для &lt;&lt;&lt;INL_0>>>)
  - Получите эмбеддинги для каждого фрагмента текста
  - Сохраните эти эмбеддинги в собственной базе данных или в провайдере векторного поиска, таком как [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (онлайн расчет):
  - Получите эмбеддинг поискового запроса
  - Найдите наиболее близкие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов можно использовать как одну из множества характеристик при ранжировании результатов.

## Ответы на вопросы

Лучший способ получить надежно честные ответы от GPT-3 — предоставить ему исходные документы, где он сможет найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете эффективно искать релевантную информацию в корпусе документов и затем предоставлять эту информацию GPT-3 через prompt для ответа на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что вместо произвольного текстового запроса входными данными являются элементы из множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, эти оценки косинусного сходства могут использоваться самостоятельно для ранжирования элементов или в качестве признаков в более крупных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучить, вы всё же можете использовать обучающие данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с использованием обучающих данных. Суть метода заключается в обучении пользовательской матрицы для умножения на векторы эмбеддингов с целью получения новых настроенных эмбеддингов. При хорошем обучающем наборе эта пользовательская матрица поможет выделить признаки, релевантные вашим меткам. Можно рассматривать умножение матрицы как (а) модификацию эмбеддингов либо (б) изменение функции расстояния, используемой для измерения расстояний между эмбеддингами.