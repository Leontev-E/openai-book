---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения степени сходства или связности между фрагментами текста.

Используя понимание текста GPT-3, эти эмбеддинги [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на тестах в задачах обучения без учителя и переносного обучения.

Эмбеддинги можно применять для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для получения дополнительной информации прочитайте объявления в блогах OpenAI:

- [Представляем эмбеддинги текста и кода (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов смотрите [Таблицу лидеров Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги могут использоваться для поиска как самостоятельно, так и в составе более сложной системы.

Самый простой способ использовать эмбеддинги для поиска следующий:

- Перед поиском (прекомпьютинг):
  - Разбейте корпус текста на фрагменты, меньшие лимита токенов (8,191 токен для &lt;&lt;&lt;INL_0>>>)
  - Получите эмбеддинги для каждого фрагмента текста
  - Сохраните эти эмбеддинги в собственной базе данных или в векторном поисковом сервисе, таком как [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисление на лету):
  - Получите эмбеддинг поискового запроса
  - Найдите ближайшие эмбеддинги в своей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более продвинутых системах поиска косинусное сходство эмбеддингов может использоваться как одна из метрик в ранжировании результатов.

## Ответы на вопросы

Лучший способ получить достоверные и честные ответы от GPT-3 — предоставить ему исходные документы, в которых он может найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете эффективно искать релевантную информацию в корпусе документов и затем передавать эту информацию GPT-3 через подсказку, чтобы он ответил на вопрос. Это демонстрируется в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации во многом похожи на поиск, за исключением того, что вместо свободного текстового запроса входными данными являются элементы множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, значения косинусного сходства могут использоваться как самостоятельно для ранжирования элементов, так и в качестве признаков в более крупных алгоритмах ранжирования.

## Кастомизация эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучить, вы всё же можете использовать тренировочные данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) приведён пример метода кастомизации эмбеддингов с помощью тренировочных данных. Идея метода заключается в обучении кастомной матрицы, на которую умножаются векторные эмбеддинги для получения новых, адаптированных эмбеддингов. При наличии хороших тренировочных данных эта кастомная матрица поможет выделить признаки, релевантные вашим меткам. Можно рассматривать умножение на матрицу как (а) модификацию эмбеддингов или (б) модификацию функции расстояния, используемой для измерения дистанций между эмбеддингами.