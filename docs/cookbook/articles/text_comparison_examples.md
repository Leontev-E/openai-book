---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текста

[Endpoint API embeddings OpenAI](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения связанности или сходства между фрагментами текста.

Используя понимание текста GPT-3, эти embeddings [достигли передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в условиях неконтролируемого обучения и обучения с переносом.

Embeddings можно использовать для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти дубликатов и многого другого.

Для получения дополнительной информации прочитайте анонсы в блогах OpenAI:

- [Введение текстовых и кодовых embeddings (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель embeddings (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями embeddings смотрите [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Embeddings можно использовать для поиска как самостоятельно, так и в составе более сложной системы.

Самый простой способ использовать embeddings для поиска следующий:

- Перед поиском (предварительный расчёт):
  - Разбейте ваш корпус текста на фрагменты, меньшие лимита по токенам (8,191 токен для `text-embedding-3-small`)
  - Получите embeddings для каждого фрагмента текста
  - Сохраните эти embeddings в собственной базе данных или в векторном поисковом сервисе, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- В момент поиска (динамический расчёт):
  - Получите embedding для поискового запроса
  - Найдите ближайшие embeddings в вашей базе данных
  - Верните лучшие результаты

Пример использования embeddings для поиска показан в [Semantic_text_search_using_embeddings.ipynb](../examples/Semantic_text_search_using_embeddings.ipynb).

В более сложных поисковых системах косинусное сходство embeddings может использоваться как одна из признаков при ранжировании результатов поиска.

## Ответы на вопросы

Лучший способ получать достоверные ответы от GPT-3 — давать ему исходные документы, в которых он может найти правильные ответы. Используя описанный выше метод семантического поиска, вы можете недорого искать по корпусу документов нужную информацию и затем передавать эту информацию GPT-3 через prompt для ответа на вопрос. Это демонстрируется в [Question_answering_using_embeddings.ipynb](../examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации во многом похожи на поиск, за исключением того, что вместо свободного текста запросом являются элементы из множества.

Пример использования embeddings для рекомендаций показан в [Recommendation_using_embeddings.ipynb](../examples/Recommendation_using_embeddings.ipynb).

Аналогично поиску, значения косинусного сходства можно использовать либо самостоятельно для ранжирования элементов, либо в качестве признаков в более сложных алгоритмах ранжирования.

## Настройка embeddings

Хотя веса модели embeddings OpenAI нельзя дообучать, вы всё же можете использовать обучающие данные для настройки embeddings под своё приложение.

В [Customizing_embeddings.ipynb](../examples/Customizing_embeddings.ipynb) приведён пример метода настройки embeddings с использованием обучающих данных. Суть метода — обучить собственную матрицу для умножения на векторы embeddings, чтобы получить новые настроенные embeddings. При хорошем обучающем наборе эта матрица поможет выделять признаки, релевантные вашим меткам обучения. Эквивалентно, умножение матрицы можно рассматривать как (a) модификацию самих embeddings или (b) модификацию функции расстояния, используемой для измерения расстояний между embeddings.