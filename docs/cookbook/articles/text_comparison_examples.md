---
lang: ru
translationOf: openai-cookbook
---

# Примеры сравнения текстов

[Эндпоинт эмбеддингов OpenAI API](https://beta.openai.com/docs/guides/embeddings) можно использовать для измерения степени близости или сходства между текстовыми фрагментами.

Используя понимание текста GPT-3, эти эмбеддинги достигли [передовых результатов](https://arxiv.org/abs/2201.10005) на бенчмарках в задачах обучения без учителя и переноса обучения.

Эмбеддинги применяются для семантического поиска, рекомендаций, кластерного анализа, обнаружения почти-дубликатов и многого другого.

Для дополнительной информации прочитайте анонсы в блоге OpenAI:

- [Введение текстовых и кодовых эмбеддингов (январь 2022)](https://openai.com/blog/introducing-text-and-code-embeddings/)
- [Новая и улучшенная модель эмбеддингов (декабрь 2022)](https://openai.com/blog/new-and-improved-embedding-model/)

Для сравнения с другими моделями эмбеддингов см. [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Семантический поиск

Эмбеддинги можно использовать для поиска как самостоятельно, так и в составе более сложной системы.

Самый простой способ использовать эмбеддинги для поиска:

- Перед выполнением поиска (предварительный расчет):
  - Разбейте корпус текстов на части, меньшие лимита по токенам (8,191 токен для &lt;&lt;&lt;INL_0>>>)
  - Получите эмбеддинг для каждой части текста
  - Сохраните эти эмбеддинги в собственной базе данных или в сервисе векторного поиска, например [Pinecone](https://www.pinecone.io), [Weaviate](https://weaviate.io) или [Qdrant](https://qdrant.tech)
- Во время поиска (вычисление на лету):
  - Получите эмбеддинг поискового запроса
  - Найдите ближайшие эмбеддинги в вашей базе данных
  - Верните лучшие результаты

Пример использования эмбеддингов для поиска показан в [Semantic_text_search_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb).

В более сложных системах поиска косинусная близость эмбеддингов может использоваться как одна из множества признаков для ранжирования результатов.

## Ответы на вопросы

Лучший способ получить надежно достоверные ответы от GPT-3 — предоставить ему исходные документы, в которых он сможет найти правильные ответы. Используя описанную выше процедуру семантического поиска, вы можете недорого искать релевантную информацию в корпусе документов и затем подать эту информацию GPT-3 через подсказку, чтобы ответить на вопрос. Мы демонстрируем это в [Question_answering_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).

## Рекомендации

Рекомендации очень похожи на поиск, за исключением того, что на вход подаются не произвольные текстовые запросы, а элементы из некоторого множества.

Пример использования эмбеддингов для рекомендаций показан в [Recommendation_using_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb).

Как и в случае поиска, эти оценки косинусной близости можно использовать самостоятельно для ранжирования элементов или как признаки в более сложных алгоритмах ранжирования.

## Настройка эмбеддингов

Хотя веса модели эмбеддингов OpenAI нельзя дообучать, вы тем не менее можете использовать обучающие данные для настройки эмбеддингов под ваше приложение.

В [Customizing_embeddings.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb) мы приводим пример метода настройки эмбеддингов с помощью обучающих данных. Суть метода заключается в обучении собственной матрицы, которой умножают вектор эмбеддинга, чтобы получить новые, настроенные эмбеддинги. При наличии хороших обучающих данных эта матрица поможет усилить признаки, важные для ваших меток обучения. Эквивалентно умножение матрицы можно рассматривать как (а) модификацию эмбеддингов или (б) модификацию функции расстояния, используемой для измерения расстояний между эмбеддингами.