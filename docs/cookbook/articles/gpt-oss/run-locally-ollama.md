---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss локально с Ollama

Хотите запустить [**OpenAI gpt-oss**](https://openai.com/open-models) на своем собственном оборудовании? Это руководство проведет вас через процесс использования [Ollama](https://ollama.ai) для настройки **gpt-oss-20b** или **gpt-oss-120b** локально, чтобы общаться с моделью офлайн, использовать её через API и даже подключать к Agents SDK.

Обратите внимание, что это руководство предназначено для потребительского оборудования, например, для запуска модели на ПК или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [ознакомьтесь с нашим руководством по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите модель

Ollama поддерживает оба размера модели gpt-oss:

- **&lt;&lt;&lt;INL_0>>>**
  - Меньшая модель
  - Рекомендуется с **≥16GB видеопамяти (VRAM)** или **унифицированной памятью**
  - Идеально подходит для высокопроизводительных потребительских GPU или Mac на Apple Silicon
- **&lt;&lt;&lt;INL_1>>>**
  - Наша большая полноразмерная модель
  - Рекомендуется с **≥60GB видеопамяти (VRAM)** или **унифицированной памятью**
  - Отлично подходит для мульти-GPU или мощных рабочих станций

**Несколько примечаний:**

- Эти модели поставляются **с квантованием MXFP4** “из коробки”, в настоящее время не доступно другое квантование
- Вы _можете_ выгружать часть нагрузки на CPU, если не хватает VRAM, но ожидайте, что производительность снизится.

## Быстрая настройка

1. **Установите Ollama** → [Скачать здесь](https://ollama.com/download)
2. **Загрузите нужную модель:**

&lt;&lt;&lt;CODE_0>>>

## Общение с gpt-oss

Готовы пообщаться с моделью? Можно запустить чат в приложении или в терминале:

&lt;&lt;&lt;CODE_1>>>

Ollama применяет **чат-шаблон** по умолчанию, который имитирует [формат OpenAI harmony](https://cookbook.openai.com/articles/openai-harmony). Напишите сообщение и начните разговор.

## Использование API

Ollama предоставляет **API, совместимый с Chat Completions**, поэтому вы можете использовать OpenAI SDK без существенных изменений. Вот пример на Python:

&lt;&lt;&lt;CODE_2>>>

Если вы раньше работали с OpenAI SDK, вам это покажется сразу знакомым.

Кроме того, вы можете использовать SDK Ollama напрямую на [Python](https://github.com/ollama/ollama-python) или [JavaScript](https://github.com/ollama/ollama-js).

## Использование инструментов (вызов функций)

Ollama умеет:

- Вызывать функции
- Использовать **встроенный браузерный инструмент** (в приложении)

Пример вызова функции через Chat Completions:

&lt;&lt;&lt;CODE_3>>>

Поскольку модели могут выполнять вызовы инструментов в рамках цепочки рассуждений (chain-of-thought, CoT), важно возвращать обоснование, полученное от API, обратно в последующий вызов инструмента, предоставляя ответ, пока модель не придет к окончательному результату.

## Обходные решения для Responses API

Ollama пока не поддерживает **Responses API** нативно.

Если вы хотите использовать Responses API, можно воспользоваться [**прокси Hugging Face для &lt;&lt;&lt;INL_2>>>**](https://github.com/huggingface/responses.js), который преобразует Chat Completions в Responses API.

Для базовых сценариев вы также можете [**запустить наш пример Python-сервера с Ollama в качестве backend.**](https://github.com/openai/gpt-oss?tab=readme-ov-file#responses-api) Этот сервер — базовый пример и не содержит

&lt;&lt;&lt;CODE_4>>>

## Интеграция с Agents SDK

Хотите использовать gpt-oss с OpenAI **Agents SDK**?

В обоих Agents SDK вы можете переопределить базовый клиент OpenAI, чтобы направлять его на Ollama через Chat Completions или на свой прокси Responses.js для локальных моделей. Кроме того, можно использовать встроенный функционал для подключения Agents SDK к сторонним моделям.

- **Python:** Используйте [LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/), чтобы проксировать запросы к Ollama через LiteLLM
- **TypeScript:** Используйте [AI SDK](https://openai.github.io/openai-agents-js/extensions/ai-sdk/) с [адаптером ollama](https://ai-sdk.dev/providers/community-providers/ollama)

Пример на Python с использованием Agents SDK и LiteLLM:

&lt;&lt;&lt;CODE_5>>>