---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss локально с Ollama

Хотите запустить [**OpenAI gpt-oss**](https://openai.com/open-models) на своем оборудовании? В этом руководстве мы расскажем, как с помощью [Ollama](https://ollama.ai) настроить **gpt-oss-20b** или **gpt-oss-120b** локально, чтобы общаться с моделью офлайн, использовать её через API и даже подключать к Agents SDK.

Обратите внимание, что это руководство рассчитано на потребительское оборудование, например запуск модели на ПК или Mac. Для серверных приложений с выделенными графическими процессорами, такими как NVIDIA H100, [посмотрите наше руководство по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите модель

Ollama поддерживает обе версии модели gpt-oss:

- **&lt;&lt;&lt;INL_0>>>**
  - Меньшая модель
  - Рекомендуется при **≥16 ГБ видеопамяти** или **унифицированной памяти**
  - Идеальна для более мощных потребительских GPU или Mac на Apple Silicon
- **&lt;&lt;&lt;INL_1>>>**
  - Наша большая полноразмерная модель
  - Рекомендуется при **≥60 ГБ видеопамяти** или **унифицированной памяти**
  - Лучший вариант для многогруппового или мощного рабочего места

**Несколько примечаний:**

- Эти модели поставляются **в quantized MXFP4 формате** из коробки, и на данный момент другие параметры квантования не поддерживаются
- Вы _можете_ выгружать вычисления на CPU при нехватке видеопамяти, но производительность будет ниже.

## Быстрая настройка

1. **Установите Ollama** → [Скачать здесь](https://ollama.com/download)
2. **Загрузите нужную модель:**

&lt;&lt;&lt;CODE_0>>>

## Общение с gpt-oss

Готовы начать общение с моделью? Можно запустить чат в приложении или в терминале:

&lt;&lt;&lt;CODE_1>>>

Ollama использует **шаблон чата** по умолчанию, который имитирует [формат OpenAI harmony](https://cookbook.openai.com/articles/openai-harmony). Напишите сообщение и начните разговор.

## Использование API

Ollama предоставляет **совместимый с Chat Completions API**, поэтому вы можете использовать OpenAI SDK практически без изменений. Вот пример на Python:

&lt;&lt;&lt;CODE_2>>>

Если вы уже работали с OpenAI SDK, это покажется вам очень знакомым.

В качестве альтернативы вы можете использовать SDK Ollama на [Python](https://github.com/ollama/ollama-python) или [JavaScript](https://github.com/ollama/ollama-js) напрямую.

## Использование инструментов (вызов функций)

Ollama может:

- Вызывать функции
- Использовать **встроенный браузерный инструмент** (в приложении)

Пример вызова функции через Chat Completions:

&lt;&lt;&lt;CODE_3>>>

Поскольку модели могут выполнять вызовы инструментов в рамках цепочки рассуждений (CoT), важно возвращать рассуждения из API обратно в последующий вызов функции, предоставляя ответ, пока модель не придёт к окончательному выводу.

## Обходы для Responses API

Ollama пока не поддерживает **Responses API** нативно.

Если вы хотите использовать Responses API, вы можете воспользоваться [**прокси Hugging Face &lt;&lt;&lt;INL_2>>>**](https://github.com/huggingface/responses.js) для конвертации Chat Completions в Responses API.

Для базовых сценариев вы можете [**запустить наш пример Python-сервера с Ollama в качестве бэкенда.**](https://github.com/openai/gpt-oss?tab=readme-ov-file#responses-api) Этот сервер является простым примером и не имеет

&lt;&lt;&lt;CODE_4>>>

## Интеграция с Agents SDK

Хотите использовать gpt-oss с OpenAI **Agents SDK**?

Agents SDK позволяет переопределять базового клиента OpenAI, чтобы направлять запросы к Ollama через Chat Completions или ваш Responses.js прокси для локальных моделей. Кроме того, есть встроенная функциональность для связывания Agents SDK с моделями сторонних разработчиков.

- **Python:** Используйте [LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/), чтобы проксировать запросы к Ollama через LiteLLM
- **TypeScript:** Используйте [AI SDK](https://openai.github.io/openai-agents-js/extensions/ai-sdk/) с [адаптером ollama](https://ai-sdk.dev/providers/community-providers/ollama)

Пример использования Agents SDK на Python с LiteLLM:

&lt;&lt;&lt;CODE_5>>>