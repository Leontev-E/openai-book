---
lang: ru
translationOf: openai-cookbook
---

# Как запускать gpt-oss локально с LM Studio

[LM Studio](https://lmstudio.ai) — это производительное и удобное настольное приложение для запуска больших языковых моделей (LLM) на локальном оборудовании. В этом руководстве показано, как настроить и запускать модели **gpt-oss-20b** или **gpt-oss-120b** с помощью LM Studio, включая способы общения с ними, использования MCP-серверов и взаимодействия с моделями через локальное API разработки LM Studio.

Обратите внимание, что это руководство предназначено для потребительского оборудования, например, запуска gpt-oss на ПК или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [ознакомьтесь с нашим руководством по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите модель

LM Studio поддерживает оба размера моделей gpt-oss:

- [**&lt;&lt;&lt;INL_0>>>**](https://lmstudio.ai/models/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует минимум **16 ГБ видеопамяти (VRAM)**
  - Идеально подходит для топовых потребительских GPU или Apple Silicon Mac
- [**&lt;&lt;&lt;INL_1>>>**](https://lmstudio.ai/models/openai/gpt-oss-120b)
  - Большая полноразмерная модель
  - Рекомендуется **≥60 ГБ VRAM**
  - Оптимальна для многогрузовых или мощных рабочих станций

LM Studio поставляется со [llama.cpp](https://github.com/ggml-org/llama.cpp) движком для инференса (работающим с моделями в формате GGUF), а также с движком [Apple MLX](https://github.com/ml-explore/mlx) для Apple Silicon Mac.

## Быстрая настройка

1. **Установите LM Studio**  
   LM Studio доступен для Windows, macOS и Linux. [Скачайте его здесь](https://lmstudio.ai/download).

2. **Скачайте модель gpt-oss** → 

&lt;&lt;&lt;CODE_0>>> 

3. **Загрузите модель в LM Studio**  
  → Откройте LM Studio и используйте интерфейс загрузки моделей для загрузки скачанной модели gpt-oss. Альтернативно можно использовать командную строку:

&lt;&lt;&lt;CODE_1>>>

4. **Используйте модель** → После загрузки вы можете взаимодействовать с моделью напрямую через чат-интерфейс LM Studio или через API.

## Общение с gpt-oss

Используйте чат-интерфейс LM Studio для начала разговора с gpt-oss или воспользуйтесь командой &lt;&lt;&lt;INL_2>>> в терминале:

&lt;&lt;&lt;CODE_2>>>

Примечание по форматированию подсказок: LM Studio использует библиотеку OpenAI [Harmony](https://cookbook.openai.com/articles/openai-harmony) для формирования входных данных к моделям gpt-oss как при работе через llama.cpp, так и MLX.

## Использование gpt-oss с локальным эндпоинтом /v1/chat/completions

LM Studio предоставляет API, совместимый с Chat Completions, чтобы вы могли использовать OpenAI SDK с минимальными изменениями. Вот пример на Python:

&lt;&lt;&lt;CODE_3>>>

Если вы раньше работали с OpenAI SDK, то это покажется вам знакомым, и ваш существующий код должен работать, сменив только базовый URL.

## Как использовать MCP в интерфейсе чата

LM Studio является [MCP-клиентом](https://lmstudio.ai/docs/app/plugins/mcp), что означает возможность подключения к MCP-серверам. Это позволяет предоставлять внешние инструменты моделям gpt-oss.

Файл mcp.json LM Studio находится по адресу:

&lt;&lt;&lt;CODE_4>>>

## Локальное использование инструментов с gpt-oss на Python или TypeScript

SDK LM Studio доступен как для [Python](https://github.com/lmstudio-ai/lmstudio-python), так и для [TypeScript](https://github.com/lmstudio-ai/lmstudio-js). С помощью SDK можно реализовать вызов инструментов и локальное выполнение функций с gpt-oss.

Для этого используется вызов &lt;&lt;&lt;INL_3>>>, который позволяет передавать модели gpt-oss набор инструментов и переключаться между их вызовом и рассуждением, пока задача не будет выполнена.

Ниже пример, показывающий, как предоставить модели один инструмент, который может создавать файлы в вашей локальной файловой системе. Этот пример можно взять за основу и расширять новыми инструментами. Документацию по описанию инструментов смотрите здесь для [Python](https://lmstudio.ai/docs/python/agent/tools) и [TypeScript](https://lmstudio.ai/docs/typescript/agent/tools).

&lt;&lt;&lt;CODE_5>>>

&lt;&lt;&lt;CODE_6>>>

Для разработчиков на TypeScript, которые хотят использовать gpt-oss локально, вот похожий пример с использованием &lt;&lt;&lt;INL_4>>>:

&lt;&lt;&lt;CODE_7>>>

&lt;&lt;&lt;CODE_8>>>