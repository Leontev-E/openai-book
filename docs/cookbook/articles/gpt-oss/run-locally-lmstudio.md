---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss локально с LM Studio

[LM Studio](https://lmstudio.ai) — производительное и удобное настольное приложение для запуска крупных языковых моделей (LLM) на локальном оборудовании. В этом руководстве описано, как настроить и запустить модели **gpt-oss-20b** или **gpt-oss-120b** с помощью LM Studio, а также как общаться с ними, использовать MCP-серверы или взаимодействовать с моделями через локальное API для разработки LM Studio.

Обратите внимание, что это руководство предназначено для потребительского оборудования, например, запуска gpt-oss на ПК или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [ознакомьтесь с нашим руководством по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите модель

LM Studio поддерживает обе версии модели gpt-oss:

- [**&lt;&lt;&lt;INL_0>>>**](https://lmstudio.ai/models/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует минимум **16 ГБ видеопамяти**
  - Отлично подходит для топовых потребительских GPU или Apple Silicon Mac
- [**&lt;&lt;&lt;INL_1>>>**](https://lmstudio.ai/models/openai/gpt-oss-120b)
  - Наша большая полноразмерная модель
  - Рекомендуется использовать при **≥60 ГБ видеопамяти**
  - Идеально для систем с несколькими GPU или мощных рабочих станций

В комплекте LM Studio поставляется как движок для инференса [llama.cpp](https://github.com/ggml-org/llama.cpp) (работающий с моделями в формате GGUF), так и движок [Apple MLX](https://github.com/ml-explore/mlx) для Apple Silicon Mac.

## Быстрая установка

1. **Установите LM Studio**  
   LM Studio доступна для Windows, macOS и Linux. [Скачать можно здесь](https://lmstudio.ai/download).

2. **Скачайте модель gpt-oss** → 

&lt;&lt;&lt;CODE_0>>> 

3. **Загрузите модель в LM Studio**  
   → Откройте LM Studio и через интерфейс загрузки моделей загрузите скачанную модель gpt-oss. В качестве альтернативы можно использовать командную строку:

&lt;&lt;&lt;CODE_1>>>

4. **Используйте модель** → После загрузки вы можете взаимодействовать с моделью напрямую в чат-интерфейсе LM Studio или через API.

## Общение с gpt-oss

Используйте чат-интерфейс LM Studio для начала беседы с gpt-oss или выполните команду &lt;&lt;&lt;INL_2>>> в терминале:

&lt;&lt;&lt;CODE_2>>>

Примечание про форматирование подсказок: LM Studio использует библиотеку OpenAI [Harmony](https://cookbook.openai.com/articles/openai-harmony) для формирования входных данных для моделей gpt-oss, как при работе через llama.cpp, так и через MLX.

## Использование gpt-oss с локальным эндпоинтом /v1/chat/completions

LM Studio предоставляет **API, совместимое с Chat Completions**, чтобы вы могли использовать OpenAI SDK без серьёзных изменений. Вот пример на Python:

&lt;&lt;&lt;CODE_3>>>

Если вы уже использовали OpenAI SDK, то это покажется вам сразу знакомым, и ваш существующий код должен работать, если изменить базовый URL.

## Как использовать MCP в UI чата

LM Studio — это [MCP-клиент](https://lmstudio.ai/docs/app/plugins/mcp), то есть вы можете подключать к нему MCP-серверы. Это позволяет предоставлять внешние инструменты моделям gpt-oss.

Файл mcp.json для LM Studio находится здесь:

&lt;&lt;&lt;CODE_4>>>

## Локальное использование инструментов с gpt-oss на Python или TypeScript

SDK LM Studio доступен как для [Python](https://github.com/lmstudio-ai/lmstudio-python), так и для [TypeScript](https://github.com/lmstudio-ai/lmstudio-js). С помощью SDK вы можете реализовать вызов инструментов и локальное выполнение функций с gpt-oss.

Для этого используется вызов &lt;&lt;&lt;INL_3>>>, который позволяет предоставлять gpt-oss инструменты, а модель может переключаться между вызовами инструментов и собственным рассуждением, пока задача не будет выполнена.

Пример ниже демонстрирует, как предоставить модели один инструмент, который может создавать файлы на вашем локальном диске. Вы можете использовать этот пример как отправную точку и расширять его дополнительными инструментами. Смотрите документацию по описанию инструментов для [Python](https://lmstudio.ai/docs/python/agent/tools) и [TypeScript](https://lmstudio.ai/docs/typescript/agent/tools).

&lt;&lt;&lt;CODE_5>>>

&lt;&lt;&lt;CODE_6>>>

Для разработчиков на TypeScript, которые хотят использовать gpt-oss локально, вот аналогичный пример с использованием &lt;&lt;&lt;INL_4>>>:

&lt;&lt;&lt;CODE_7>>>

&lt;&lt;&lt;CODE_8>>>