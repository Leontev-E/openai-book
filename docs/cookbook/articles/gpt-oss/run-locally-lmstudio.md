---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss локально с LM Studio

[LM Studio](https://lmstudio.ai) — это производительное и удобное настольное приложение для запуска больших языковых моделей (LLM) на локальном железе. В этом руководстве объясняется, как настроить и запустить модели **gpt-oss-20b** или **gpt-oss-120b** с помощью LM Studio, включая общение с ними, использование MCP серверов и взаимодействие с моделями через локальное API разработки LM Studio.

Обратите внимание, что это руководство предназначено для потребительского оборудования, например, запуска gpt-oss на PC или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [ознакомьтесь с нашим руководством по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите свою модель

LM Studio поддерживает оба размера моделей gpt-oss:

- [**`openai/gpt-oss-20b`**](https://lmstudio.ai/models/openai/gpt-oss-20b)
  - Модель меньшего размера
  - Требуется минимум **16 ГБ видеопамяти (VRAM)**
  - Отлично подходит для высокопроизводительных потребительских GPU или Apple Silicon Mac
- [**`openai/gpt-oss-120b`**](https://lmstudio.ai/models/openai/gpt-oss-120b)
  - Наша большая полноразмерная модель
  - Рекомендуется **60 ГБ VRAM и выше**
  - Идеально для многогпу или мощных рабочих станций

LM Studio поставляется с движком вывода на базе [llama.cpp](https://github.com/ggml-org/llama.cpp) (работающим с моделями в формате GGUF) и с движком [Apple MLX](https://github.com/ml-explore/mlx) для Apple Silicon Mac.

## Быстрая настройка

1. **Установите LM Studio**  
   LM Studio доступен для Windows, macOS и Linux. [Скачайте его здесь](https://lmstudio.ai/download).

2. **Скачайте модель gpt-oss** → 

&lt;&lt;&lt;CODE_0&gt;>> 

3. **Загрузите модель в LM Studio**  
  → Откройте LM Studio и используйте интерфейс загрузки моделей, чтобы загрузить скачанную модель gpt-oss. Либо используйте командную строку:

&lt;&lt;&lt;CODE_1&gt;>>

4. **Используйте модель** → После загрузки вы можете взаимодействовать с моделью напрямую в чат-интерфейсе LM Studio или через API.

## Общение с gpt-oss

Используйте чат-интерфейс LM Studio для начала разговора с gpt-oss или команду `chat` в терминале:

&lt;&lt;&lt;CODE_2&gt;>>

Примечание о форматировании подсказок: LM Studio использует библиотеку OpenAI [Harmony](https://cookbook.openai.com/articles/openai-harmony) для построения входных данных моделей gpt-oss, как при запуске через llama.cpp, так и MLX.

## Использование gpt-oss через локальный endpoint /v1/chat/completions

LM Studio предоставляет **API, совместимый с Chat Completions**, что позволяет использовать OpenAI SDK практически без изменений. Вот пример на Python:

&lt;&lt;&lt;CODE_3&gt;>>

Если вы уже работали с OpenAI SDK, это покажется вам знакомым, и ваш существующий код должен работать, достаточно изменить базовый URL.

## Как использовать MCP в чат-интерфейсе

LM Studio является [MCP клиентом](https://lmstudio.ai/docs/app/plugins/mcp), что означает возможность подключения MCP серверов к нему. Это позволяет предоставлять внешние инструменты моделям gpt-oss.

Файл mcp.json LM Studio находится в:

&lt;&lt;&lt;CODE_4&gt;>>

## Локальное использование инструментов с gpt-oss на Python или TypeScript

SDK LM Studio доступен как для [Python](https://github.com/lmstudio-ai/lmstudio-python), так и для [TypeScript](https://github.com/lmstudio-ai/lmstudio-js). Вы можете использовать SDK для реализации вызова инструментов и локального выполнения функций с gpt-oss.

Для этого используется вызов `.act()`, который позволяет предоставлять инструменты модели gpt-oss и переключаться между вызовом инструментов и рассуждениями, пока задача не будет выполнена.

Ниже приведён пример, показывающий, как предоставить модели один инструмент, способный создавать файлы на вашем локальном файловом сервере. Вы можете взять этот пример за основу и расширить его большим количеством инструментов. Документация по определению инструментов доступна здесь для [Python](https://lmstudio.ai/docs/python/agent/tools) и [TypeScript](https://lmstudio.ai/docs/typescript/agent/tools).

&lt;&lt;&lt;CODE_5&gt;>>

&lt;&lt;&lt;CODE_6&gt;>>

Для разработчиков на TypeScript, желающих использовать gpt-oss локально, вот аналогичный пример с использованием `lmstudio-js`:

&lt;&lt;&lt;CODE_7&gt;>>

&lt;&lt;&lt;CODE_8&gt;>>