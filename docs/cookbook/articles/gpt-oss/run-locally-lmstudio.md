---
lang: ru
translationOf: openai-cookbook
---

# Как запускать gpt-oss локально с LM Studio

[LM Studio](https://lmstudio.ai) — это производительное и удобное настольное приложение для запуска больших языковых моделей (LLM) на локальном оборудовании. В этом руководстве мы расскажем, как настроить и запустить модели **gpt-oss-20b** или **gpt-oss-120b** с помощью LM Studio, включая общение с ними в чатах, использование MCP-серверов и взаимодействие с моделями через локальный API для разработки LM Studio.

Обратите внимание, что это руководство предназначено для пользовательского оборудования, например, для запуска gpt-oss на ПК или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [посмотрите наше руководство по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберете вашу модель

LM Studio поддерживает оба размера модели gpt-oss:

- [**`openai/gpt-oss-20b`**](https://lmstudio.ai/models/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует не менее **16 ГБ видеопамяти**
  - Идеально подходит для топовых бытовых видеокарт или Apple Silicon Mac
- [**`openai/gpt-oss-120b`**](https://lmstudio.ai/models/openai/gpt-oss-120b)
  - Более крупная полноразмерная модель
  - Лучше использовать с **≥60 ГБ видеопамяти**
  - Подходит для много-GPU систем или мощных рабочих станций

LM Studio поставляется с движком вывода на основе [llama.cpp](https://github.com/ggml-org/llama.cpp) (запускающим модели в формате GGUF), а также с движком [Apple MLX](https://github.com/ml-explore/mlx) для Mac на Apple Silicon.

## Быстрая установка

1. **Установите LM Studio**  
   LM Studio доступен для Windows, macOS и Linux. [Скачать можно здесь](https://lmstudio.ai/download).

2. **Скачайте модель gpt-oss** → 

&lt;&lt;&lt;FENCE_0>>> 

3. **Загрузите модель в LM Studio**  
   → Запустите LM Studio и воспользуйтесь интерфейсом загрузки модели для загрузки скачанной модели gpt-oss. Или используйте командную строку:

&lt;&lt;&lt;FENCE_1>>>

4. **Используйте модель** → После загрузки вы можете взаимодействовать с моделью напрямую через чат LM Studio или через API.

## Общение с gpt-oss

Используйте чат-интерфейс LM Studio, чтобы начать разговор с gpt-oss, или воспользуйтесь командой `chat` в терминале:

&lt;&lt;&lt;FENCE_2>>>

Примечание по форматированию запроса: LM Studio использует библиотеку OpenAI [Harmony](https://cookbook.openai.com/articles/openai-harmony) для построения входных данных для моделей gpt-oss, как при запуске через llama.cpp, так и MLX.

## Использование gpt-oss с локальным эндпоинтом /v1/chat/completions

LM Studio предоставляет **API, совместимый с Chat Completions**, чтобы вы могли использовать OpenAI SDK без больших изменений. Вот пример на Python:

&lt;&lt;&lt;FENCE_3>>>

Если вы уже работали с OpenAI SDK, это покажется вам знакомым, и ваш существующий код должен работать, если поменять базовый URL.

## Как использовать MCP в чат-интерфейсе

LM Studio является [MCP-клиентом](https://lmstudio.ai/docs/app/plugins/mcp), что позволяет подключать к нему MCP-серверы. Это даёт возможность предоставлять внешние инструменты моделям gpt-oss.

Файл mcp.json LM Studio находится по пути:

&lt;&lt;&lt;FENCE_4>>>

## Локальное использование инструментов с gpt-oss в Python или TypeScript

SDK LM Studio доступен как для [Python](https://github.com/lmstudio-ai/lmstudio-python), так и для [TypeScript](https://github.com/lmstudio-ai/lmstudio-js). С помощью SDK можно реализовать вызов инструментов и выполнение локальных функций с gpt-oss.

Для этого используется вызов `.act()`, позволяющий предоставить инструменты gpt-oss и организовать чередование вызовов инструментов и рассуждений, пока задача не будет выполнена.

Пример ниже показывает, как предоставить модели один инструмент, который умеет создавать файлы в вашей локальной файловой системе. Вы можете использовать этот пример в качестве отправной точки и расширять его новыми инструментами. Смотрите документацию по определению инструментов для [Python](https://lmstudio.ai/docs/python/agent/tools) и [TypeScript](https://lmstudio.ai/docs/typescript/agent/tools).

&lt;&lt;&lt;FENCE_5>>>

&lt;&lt;&lt;FENCE_6>>>

Для разработчиков на TypeScript, желающих работать с gpt-oss локально, вот аналогичный пример с использованием `lmstudio-js`:

&lt;&lt;&lt;FENCE_7>>>

&lt;&lt;&lt;FENCE_8>>>