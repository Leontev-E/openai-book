---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss локально с помощью LM Studio

[LM Studio](https://lmstudio.ai) — это производительное и удобное настольное приложение для запуска больших языковых моделей (LLM) на локальном оборудовании. В этом руководстве мы расскажем, как установить и запустить модели **gpt-oss-20b** или **gpt-oss-120b** с помощью LM Studio, включая общение с ними, использование MCP серверов и взаимодействие с моделями через локальное API для разработки LM Studio.

Обратите внимание, что это руководство предназначено для пользовательского оборудования, например, запуска gpt-oss на ПК или Mac. Для серверных приложений с выделенными GPU, такими как NVIDIA H100, [ознакомьтесь с нашим руководством по vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm).

## Выберите модель

LM Studio поддерживает обе версии gpt-oss:

- [**&lt;&lt;&lt;INL_0>>>**](https://lmstudio.ai/models/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует как минимум **16 ГБ видеопамяти**
  - Отлично подходит для топовых пользовательских GPU или Apple Silicon Mac
- [**&lt;&lt;&lt;INL_1>>>**](https://lmstudio.ai/models/openai/gpt-oss-120b)
  - Более крупная полноформатная модель
  - Рекомендуется иметь **≥60 ГБ видеопамяти**
  - Идеальна для многогpафических систем или мощных рабочих станций

LM Studio поставляется с [движком llama.cpp](https://github.com/ggml-org/llama.cpp) для инференса (поддержка моделей в формате GGUF), а также с движком [Apple MLX](https://github.com/ml-explore/mlx) для Apple Silicon Mac.

## Быстрая установка

1. **Установите LM Studio**  
   LM Studio доступен для Windows, macOS и Linux. [Скачать здесь](https://lmstudio.ai/download).

2. **Скачайте модель gpt-oss** → 

&lt;&lt;&lt;CODE_0>>>

3. **Загрузите модель в LM Studio**  
   → Откройте LM Studio и используйте интерфейс загрузки моделей для загрузки скачанной модели gpt-oss. В качестве альтернативы можно использовать командную строку:

&lt;&lt;&lt;CODE_1>>>

4. **Используйте модель** → После загрузки вы можете взаимодействовать с моделью прямо в чате LM Studio или через API.

## Общение с gpt-oss

Используйте чат-интерфейс LM Studio, чтобы начать беседу с gpt-oss, либо выполните команду &lt;&lt;&lt;INL_2>>> в терминале:

&lt;&lt;&lt;CODE_2>>>

Примечание по форматированию запроса: LM Studio использует библиотеку [Harmony](https://cookbook.openai.com/articles/openai-harmony) от OpenAI для формирования входных данных для моделей gpt-oss, как при запуске через llama.cpp, так и через MLX.

## Использование gpt-oss с локальным эндпоинтом /v1/chat/completions

LM Studio предоставляет **API, совместимое с Chat Completions**, чтобы вы могли использовать OpenAI SDK без значительных изменений. Вот пример на Python:

&lt;&lt;&lt;CODE_3>>>

Если вы уже работали с OpenAI SDK, это будет знакомым, и ваш существующий код должен работать, если изменить базовый URL.

## Как использовать MCP в чат-интерфейсе

LM Studio является [MCP клиентом](https://lmstudio.ai/docs/app/plugins/mcp), что позволяет подключать к нему MCP серверы. Это даёт возможность предоставлять дополнительные внешние инструменты моделям gpt-oss.

Файл mcp.json LM Studio расположен здесь:

&lt;&lt;&lt;CODE_4>>>

## Локальное использование инструментов с gpt-oss на Python или TypeScript

SDK LM Studio доступен как для [Python](https://github.com/lmstudio-ai/lmstudio-python), так и для [TypeScript](https://github.com/lmstudio-ai/lmstudio-js). С помощью SDK вы можете реализовывать вызов инструментов и выполнение функций локально вместе с gpt-oss.

Для этого используется вызов &lt;&lt;&lt;INL_3>>>, который позволяет предоставлять инструменты gpt-oss и переключаться между вызовами инструментов и рассуждениями, пока задача не будет выполнена.

Пример ниже показывает, как предоставить модели один инструмент, который может создавать файлы в вашей локальной файловой системе. Вы можете использовать этот пример как отправную точку и расширять его добавлением других инструментов. Документацию по описанию инструментов см. здесь для [Python](https://lmstudio.ai/docs/python/agent/tools) и [TypeScript](https://lmstudio.ai/docs/typescript/agent/tools).

&lt;&lt;&lt;CODE_5>>>

&lt;&lt;&lt;CODE_6>>>

Для разработчиков на TypeScript, желающих использовать gpt-oss локально, вот похожий пример с использованием &lt;&lt;&lt;INL_4>>>:

&lt;&lt;&lt;CODE_7>>>

&lt;&lt;&lt;CODE_8>>>