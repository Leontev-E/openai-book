---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss с помощью Hugging Face Transformers

Библиотека Transformers от Hugging Face предоставляет гибкий способ загрузки и запуска больших языковых моделей локально или на сервере. В этом руководстве мы покажем, как запустить [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) или [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) с использованием Transformers, как через высокоуровневый pipeline, так и через низкоуровневые &lt;&lt;&lt;INL_0>>> вызовы с сырыми ID токенов.

Мы рассмотрим использование [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) или [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) с высокоуровневой абстракцией pipeline, низкоуровневыми \&lt;&lt;&lt;INL_1>>> вызовами, а также развертывание моделей локально с помощью \&lt;&lt;&lt;INL_2>>>, совместимых с Responses API.

В этом руководстве мы пройдемся по различным оптимизированным методам запуска **моделей gpt-oss через Transformers.**

Бонус: вы также можете дообучить модели через transformers, [ознакомьтесь с нашим руководством по дообучению здесь](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers).

## Выбор модели

Обe **gpt-oss** модели доступны на Hugging Face:

- **&lt;&lt;&lt;INL_3>>>**
  - Требуется около 16 ГБ видеопамяти при использовании MXFP4
  - Отлично подходит для одиночных топовых потребительских GPU
- **&lt;&lt;&lt;INL_4>>>**
  - Требует ≥60 ГБ видеопамяти или мульти-GPU конфигурацию
  - Идеально подходит для оборудования уровня H100

Обе модели по умолчанию **квантованы MXFP4**. Обратите внимание, что MXFP4 поддерживается на архитектуре Hopper и новее. Это включает дата-центровые GPU, такие как H100 или GB200, а также последние потребительские карты RTX 50xx серии.

Если вы используете &lt;&lt;&lt;INL_5>>> вместо MXFP4, потребление памяти будет больше (около 48 ГБ для модели с 20 миллиардами параметров).

## Быстрая настройка

1. **Установите зависимости**  
   Рекомендуется создать новое окружение Python. Установите transformers, accelerate, а также ядра Triton для совместимости с MXFP4:

&lt;&lt;&lt;CODE_0>>>

2. **(Опционально) Включите мульти-GPU**  
   Если вы запускаете большие модели, используйте Accelerate или torchrun для автоматического управления размещением устройств.

## Создание конечной точки OpenAI Responses / Chat Completions

Для запуска сервера просто используйте CLI-команду &lt;&lt;&lt;INL_6>>>:

&lt;&lt;&lt;CODE_1>>>

Самый простой способ взаимодействия с сервером — через transformers chat CLI

&lt;&lt;&lt;CODE_2>>>

или отправляя HTTP-запрос через cURL, например:

&lt;&lt;&lt;CODE_3>>>

Дополнительные кейсы, такие как интеграция &lt;&lt;&lt;INL_7>>> с Cursor и другими инструментами, описаны в [документации](https://huggingface.co/docs/transformers/main/serving).

## Быстрое инференс с pipeline

Самый простой способ запустить модели gpt-oss — использовать высокоуровневый API &lt;&lt;&lt;INL_8>>> из Transformers:

&lt;&lt;&lt;CODE_4>>>

## Продвинутый инференс с помощью &lt;&lt;&lt;INL_9>>>

Если вам нужен больший контроль, можно загрузить модель и токенизатор вручную и вызвать метод &lt;&lt;&lt;INL_10>>>:

&lt;&lt;&lt;CODE_5>>>

## Шаблон чата и вызовы инструментов

Модели OpenAI gpt-oss используют [формат ответов harmony](https://cookbook.openai.com/article/harmony) для структурирования сообщений, включая рассуждения и вызовы инструментов.

Для построения промптов можно использовать встроенный chat template Transformers. В качестве альтернативы, можно установить и использовать библиотеку [openai-harmony](https://github.com/openai/harmony) для расширенного контроля.

Для использования chat template:

&lt;&lt;&lt;CODE_6>>>

Чтобы интегрировать библиотеку [&lt;&lt;&lt;INL_11>>>](https://github.com/openai/harmony) для подготовки промптов и разбора ответов, сначала установите её так:

&lt;&lt;&lt;CODE_7>>>

Вот пример использования библиотеки для создания промптов и их кодировки в токены:

&lt;&lt;&lt;CODE_8>>>

Обратите внимание, что роль &lt;&lt;&lt;INL_12>>> в Harmony соответствует промпту &lt;&lt;&lt;INL_13>>> в chat template.

## Мульти-GPU и распределенный инференс

Большая модель gpt-oss-120b помещается на один GPU H100 при использовании MXFP4. Если вы хотите запустить её на нескольких GPU, вы можете:

- Использовать &lt;&lt;&lt;INL_14>>> для автоматического размещения и параллелизма тензоров
- Запускать с помощью &lt;&lt;&lt;INL_15>>> для распределенных конфигураций
- Использовать Expert Parallelism
- Применять специализированные ядра Flash attention для ускоренного инференса

&lt;&lt;&lt;CODE_9>>>

Затем можно запустить на узле с четырьмя GPU с помощью

&lt;&lt;&lt;CODE_10>>>