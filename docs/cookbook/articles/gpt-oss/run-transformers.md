---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss с Hugging Face Transformers

Библиотека Transformers от Hugging Face предоставляет гибкий способ загрузки и запуска больших языковых моделей локально или на сервере. В этом руководстве мы пройдем через запуск [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) или [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) с помощью Transformers, используя либо высокоуровневый pipeline, либо низкоуровневые вызовы &lt;&lt;&lt;INL_0>>> с сырыми ID токенов.

Мы рассмотрим использование [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) или [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) с высокоуровневым абстрактом pipeline, низкоуровневыми \&lt;&lt;&lt;INL_1>>> вызовами и запуском моделей локально с \&lt;&lt;&lt;INL_2>>>, совместимым с Responses API.

В этом руководстве мы рассмотрим различные оптимизированные способы запуска **gpt-oss моделей через Transformers.**

Бонус: вы также можете дообучать модели с помощью transformers, [ознакомьтесь с нашим руководством по дообучению здесь](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers).

## Выберите модель

Обе **gpt-oss** модели доступны на Hugging Face:

- **&lt;&lt;&lt;INL_3>>>**
  - Требуется примерно 16 ГБ видеопамяти при использовании MXFP4
  - Отлично подходит для одного современного потребительского GPU
- **&lt;&lt;&lt;INL_4>>>**
  - Требуется ≥60 ГБ видеопамяти или мульти-GPU конфигурация
  - Идеально для оборудования класса H100

Обе модели по умолчанию **квантованы в MXFP4**. Обратите внимание, что MXFP4 поддерживается архитектурами Hopper или новее. Это включает в себя дата-центровые GPU, такие как H100 или GB200, а также последние потребительские карты серии RTX 50xx.

Если вы используете &lt;&lt;&lt;INL_5>>> вместо MXFP4, потребление памяти будет больше (~48 ГБ для модели с 20 миллиардами параметров).

## Быстрая настройка

1. **Установите зависимости**  
   Рекомендуется создать новое Python окружение. Установите transformers, accelerate, а также Triton ядра для совместимости MXFP4:

&lt;&lt;&lt;CODE_0>>>

2. **(Опционально) Включите multi-GPU**  
   Если вы запускаете большие модели, используйте Accelerate или torchrun для автоматического распределения устройств.

## Создание конечной точки Open AI Responses / Chat Completions

Чтобы запустить сервер, просто используйте команду CLI &lt;&lt;&lt;INL_6>>>:

&lt;&lt;&lt;CODE_1>>>

Самый простой способ взаимодействия с сервером — через transformers chat CLI

&lt;&lt;&lt;CODE_2>>>

или отправляя HTTP-запрос через cURL, например:

&lt;&lt;&lt;CODE_3>>>

Дополнительные варианты использования, например интеграция &lt;&lt;&lt;INL_7>>> с Cursor и другими инструментами, описаны в [документации](https://huggingface.co/docs/transformers/main/serving).

## Быстрое инференс с pipeline

Самый простой способ запустить gpt-oss модели — использовать высокоуровневый API &lt;&lt;&lt;INL_8>>> из Transformers:

&lt;&lt;&lt;CODE_4>>>

## Продвинутый инференс с &lt;&lt;&lt;INL_9>>>

Если нужен больше контроля, вы можете вручную загрузить модель и токенизатор, а затем вызвать метод &lt;&lt;&lt;INL_10>>>:

&lt;&lt;&lt;CODE_5>>>

## Шаблон чата и вызов инструментов

OpenAI gpt-oss модели используют [формат ответа harmony](https://cookbook.openai.com/article/harmony) для структурирования сообщений, включая рассуждения и вызовы инструментов.

Для составления подсказок вы можете использовать встроенный шаблон чата в Transformers. Либо можно установить и использовать библиотеку [openai-harmony](https://github.com/openai/harmony) для более тонкой настройки.

Чтобы использовать шаблон чата:

&lt;&lt;&lt;CODE_6>>>

Чтобы интегрировать библиотеку [&lt;&lt;&lt;INL_11>>>](https://github.com/openai/harmony) для подготовки подсказок и разбора ответов, сначала установите её так:

&lt;&lt;&lt;CODE_7>>>

Пример использования этой библиотеки для построения подсказок и кодирования их в токены:

&lt;&lt;&lt;CODE_8>>>

Обратите внимание, что роль &lt;&lt;&lt;INL_12>>> в Harmony соответствует подсказке &lt;&lt;&lt;INL_13>>> в шаблоне чата.

## Multi-GPU и распределённый инференс

Большая модель gpt-oss-120b помещается на одном GPU H100 при использовании MXFP4. Если вы хотите запустить её на нескольких GPU, вы можете:

- Использовать &lt;&lt;&lt;INL_14>>> для автоматического распределения и параллелизма по тензорам
- Запускать с помощью &lt;&lt;&lt;INL_15>>> для распределённых конфигураций
- Использовать Expert Parallelism
- Применять специализированные ядра Flash attention для ускоренного инференса

&lt;&lt;&lt;CODE_9>>>

Затем вы можете запустить это на ноде с четырьмя GPU командой

&lt;&lt;&lt;CODE_10>>>
