---
lang: ru
translationOf: openai-cookbook
---

# Верификация реализаций gpt-oss

Модели [OpenAI gpt-oss](https://openai.com/open-models) вводят множество новых концепций в экосистему открытых моделей, и [их достижение ожидаемой производительности может занять некоторое время](https://x.com/ClementDelangue/status/1953119901649891367). Это руководство предназначено для помощи разработчикам, создающим решения для вывода, чтобы проверить свои реализации, или для тех, кто хочет самостоятельно протестировать реализацию любого провайдера и получить уверенность.

## Почему реализация моделей gpt-oss отличается?

Новые модели ведут себя более похоже на некоторые другие модели OpenAI, чем на существующие открытые модели. Вот несколько примеров:

1. **Формат ответа harmony.** Эти модели обучались с использованием нашего [формата OpenAI harmony](https://cookbook.openai.com/articles/openai-harmony) для структурирования беседы. Хотя обычным API-разработчикам в большинстве случаев не нужно работать с harmony, провайдеры вывода, предоставляющие API, совместимые с Chat Completions или Responses, должны корректно сопоставлять входные данные с форматом OpenAI harmony. Если модель не получает подсказки в правильном формате, это может привести к каскадным проблемам генерации и, как минимум, к ухудшению работы вызова функций.
2. **Обработка цепочки рассуждений (chain of thought, CoT) между вызовами инструментов.** Эти модели могут выполнять вызовы инструментов как часть CoT. Следствием этого является то, что модель должна получать CoT при последующем семплинге до тех пор, пока не будет достигнут окончательный ответ. Это означает, что хотя необработанная CoT не должна отображаться конечным пользователям, она должна возвращаться API, чтобы разработчики могли передавать её обратно вместе с вызовом инструмента и его выводом. [Подробнее об этом можно узнать в отдельном руководстве](https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot).
3. **Отличия в самом коде вывода.** Мы опубликовали веса смеси экспертов (MoE) исключительно в формате MXFP4. Это относительно новый формат и, вместе с другими архитектурными решениями, существующий код вывода, написанный для других открытых моделей, потребуется адаптировать под gpt-oss. По этой причине мы опубликовали как базовую (неоптимизированную) [реализацию на PyTorch](https://github.com/openai/gpt-oss/tree/main/gpt_oss/torch), так и [более оптимизированную реализацию на Triton](https://github.com/openai/gpt-oss/tree/main/gpt_oss/triton). Кроме того, мы проверили [реализацию vLLM](https://github.com/vllm-project/vllm/blob/7e3a8dc90670fd312ce1e0d4eba9bf11c571e3ad/vllm/model_executor/models/gpt_oss.py) на корректность. Надеемся, что это будет полезным учебным материалом для других реализаций.

## Дизайн API

### Responses API

Для достижения наилучшей производительности мы рекомендуем провайдерам вывода реализовать наш формат Responses API, так как форма API специально разработана для таких особенностей, как вывод необработанной CoT вместе с суммированной CoT (для отображения пользователям) и вызовов инструментов без добавления лишних свойств в формат. Самое важное для точной работы — возвращать необработанную CoT как часть &lt;&lt;&lt;INL_0>>>.

Для этого мы добавили новый массив &lt;&lt;&lt;INL_1>>> в элементы &lt;&lt;&lt;INL_2>>> Responses API. Необработанная CoT должна быть упакована в элемент типа &lt;&lt;&lt;INL_3>>>, в результате общий элемент вывода будет выглядеть так:

&lt;&lt;&lt;CODE_0>>>

Эти элементы должны приниматься в последующих шагах и затем вставляться обратно в подсказку в формате harmony, как описано в [руководстве по обработке необработанной CoT](https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot).

[Полную спецификацию смотрите в документации Responses API](https://platform.openai.com/docs/api-reference/responses/create).

### Chat Completions

Множество провайдеров предлагают API, совместимый с Chat Completions. Хотя мы не добавляли в нашу официальную документацию способ получения необработанной CoT, важно, чтобы провайдеры, предлагающие модели gpt-oss через такой API, возвращали CoT как часть сообщений и чтобы разработчики имели возможность передавать её обратно.

На данный момент в сообществе нет единой спецификации: в сообщениях обычно используются либо свойства &lt;&lt;&lt;INL_4>>>, либо &lt;&lt;&lt;INL_5>>>. **Для совместимости с клиентами, такими как OpenAI Agents SDK, мы рекомендуем использовать поле &lt;&lt;&lt;INL_6>>> как основное свойство для необработанной CoT в Chat Completions**.

## Быстрая проверка вызова инструментов и формы API

Для проверки работоспособности провайдера вы можете использовать скрипт на Node.js, опубликованный в нашем [репозитории gpt-oss на GitHub](https://github.com/openai/gpt-oss), который также подходит для проведения других проверок. Для запуска тестов вам потребуется [Node.js](http://nodejs.org/) или аналогичная среда выполнения.

Эти тесты выполнят серию запросов с вызовами функций/инструментов к Responses API или Chat Completions API, которые вы хотите проверить. Затем они оценят, был ли вызван правильный инструмент и корректны ли формы API.

По сути это дымовые тесты, но они дают хорошее представление о совместимости API с нашими SDK и способности обрабатывать базовые вызовы функций. Они не гарантируют полной точности реализации вывода (подробнее о точности — в разделе про evals) и не гарантируют полной совместимости с OpenAI API. Тем не менее, они будут полезным индикатором крупных проблем реализации.

Для запуска набора тестов выполните следующие команды:

&lt;&lt;&lt;CODE_1>>>

После этого вы получите результаты по реализации API и детали производительности вызовов функций.

Если тесты прошли успешно, выводу будет показано 0 некорректных запросов и более 90% по метрикам pass@k и pass^k, что подразумевает правильность реализации. Для полноценной уверенности рекомендуется также изучить evals, как описано ниже.

Если вам нужен подробный просмотр отдельных ответов, вы можете открыть созданный в вашей директории файл &lt;&lt;&lt;INL_7>>>.

Также можно включить режим отладки для просмотра фактических данных запросов с помощью &lt;&lt;&lt;INL_8>>>, но это может быть шумно. Для запуска одного теста используйте флаг &lt;&lt;&lt;INL_9>>> для облегчения отладки. Для тестирования событий стриминга применяйте &lt;&lt;&lt;INL_10>>>.

## Верификация корректности с помощью evals

Команда Artificial Analysis проводит AIME и GPQA evals для разных провайдеров. Если вы не уверены в своем провайдере, [просмотрите последние метрики на Artificial Analysis](https://artificialanalysis.ai/models/gpt-oss-120b/providers#evaluations).

Для максимальной надежности рекомендуется запускать evals самостоятельно. В том же репозитории, что и тесты выше, есть папка &lt;&lt;&lt;INL_11>>> с тестовыми каркасами, которые мы использовали для проверки evals AIME (16 попыток на задачу), GPQA (8 попыток на задачу) и Healthbench (1 попытка на задачу) для реализации vLLM и некоторых наших эталонных реализаций. Вы можете использовать те же скрипты для проверки своих реализаций.

Для тестирования API, совместимого с Responses API, запустите:

&lt;&lt;&lt;CODE_2>>>

Для тестирования API, совместимого с Chat Completions API, запустите:

&lt;&lt;&lt;CODE_3>>>

Если вы получаете результаты, похожие на опубликованные нами, и вышеописанные тесты вызова функций прошли успешно, скорее всего у вас корректная реализация gpt-oss.