---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss с vLLM

[vLLM](https://docs.vllm.ai/en/latest/) — это открытый и высокопроизводительный движок инференса, разработанный для эффективного обслуживания больших языковых моделей (LLM) за счёт оптимизации использования памяти и скорости обработки. В этом руководстве показано, как использовать vLLM для настройки **gpt-oss-20b** или **gpt-oss-120b** на сервере, чтобы предоставлять gpt-oss в виде API для ваших приложений и даже подключать его к Agents SDK.

Обратите внимание, что это руководство предназначено для серверных приложений с выделенными GPU, такими как NVIDIA H100. Для локального запуска на потребительских GPU смотрите наши руководства по [Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) или [LM Studio](https://cookbook.openai.com/articles/gpt-oss/run-locally-lmstudio).

## Выберите модель

vLLM поддерживает оба размера моделей gpt-oss:

- [**&lt;&lt;&lt;INL_0>>>**](https://huggingface.co/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует около **16 ГБ видеопамяти**
- [**&lt;&lt;&lt;INL_1>>>**](https://huggingface.co/openai/gpt-oss-120b)
  - Наша большая полноразмерная модель
  - Рекомендуется иметь **≥60 ГБ видеопамяти**
  - Подходит для одного H100 или многогпу конфигураций

Обе модели идут **с MXFP4 квантизацией** «из коробки».

## Быстрая установка

1. **Установите vLLM**  
   vLLM рекомендует использовать [uv](https://docs.astral.sh/uv/) для управления вашей Python-средой. Это поможет подобрать правильную реализацию в зависимости от окружения. [Подробности в их быстром старте](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#installation). Для создания нового виртуального окружения и установки vLLM выполните:

&lt;&lt;&lt;CODE_0>>>

2. **Запустите сервер и скачайте модель**  
   vLLM предоставляет команду &lt;&lt;&lt;INL_2>>>, которая автоматически скачает модель с HuggingFace и запустит сервер, совместимый с OpenAI, на &lt;&lt;&lt;INL_3>>>. Выполните команду для выбранного размера модели в терминале на вашем сервере.

&lt;&lt;&lt;CODE_1>>>

## Использование API

vLLM предоставляет совместимый с API **Chat Completions** и **Responses** интерфейс, чтобы вы могли использовать OpenAI SDK с минимальными изменениями. Вот пример на Python:

&lt;&lt;&lt;CODE_2>>>

Если вы уже использовали OpenAI SDK, всё будет выглядеть знакомо, и ваш существующий код сработает просто при замене базового URL.

## Использование инструментов (вызов функций)

vLLM поддерживает вызов функций и возможность моделе осуществлять просмотр.

Вызов функций работает через API и для Responses, и для Chat Completions.

Пример вызова функции через Chat Completions:

&lt;&lt;&lt;CODE_3>>>

Поскольку модели могут выполнять вызовы инструментов в рамках цепочки рассуждений (chain-of-thought, CoT), важно возвращать логику, возвращаемую API, в последующий вызов инструментов, где вы предоставляете ответ, пока модель не достигнет окончательного результата.

## Интеграция с Agents SDK

Хотите использовать gpt-oss с OpenAI **Agents SDK**?

Оба Agents SDK позволяют переопределять базовый клиент OpenAI для работы с vLLM и вашими моделями, запущенными локально. Кроме того, для Python SDK вы можете использовать [LiteLLM интеграцию](https://openai.github.io/openai-agents-python/models/litellm/), чтобы проксировать запросы к vLLM.

Вот пример использования Agents SDK на Python:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

## Работа с vLLM напрямую

Помимо запуска vLLM как API-сервер с использованием &lt;&lt;&lt;INL_4>>>, вы можете использовать Python-библиотеку vLLM для управления инференсом напрямую.

Если вы используете vLLM для сэмплирования напрямую, важно, чтобы ваши входные подсказки соответствовали [формату ответа harmony](https://cookbook.openai.com/article/harmony), так как без этого модель не будет работать корректно. Для этого можно воспользоваться [SDK &lt;&lt;&lt;INL_5>>>](https://github.com/openai/harmony).

&lt;&lt;&lt;CODE_6>>>

После этого вы можете использовать harmony для кодирования и парсинга токенов, сгенерированных функцией generate в vLLM.

&lt;&lt;&lt;CODE_7>>>