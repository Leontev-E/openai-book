---
lang: ru
translationOf: openai-cookbook
---

# Как запустить gpt-oss с vLLM

[vLLM](https://docs.vllm.ai/en/latest/) — это open-source движок для высокопроизводительного инференса, разработанный для эффективного обслуживания больших языковых моделей (LLM) за счет оптимизации использования памяти и скорости обработки. В этом руководстве показано, как использовать vLLM для настройки **gpt-oss-20b** или **gpt-oss-120b** на сервере, чтобы обслуживать gpt-oss как API для ваших приложений и даже подключать его к Agents SDK.

Обратите внимание, что это руководство предназначено для серверных приложений с выделенными GPU, такими как NVIDIA H100. Для локального инференса на потребительских GPU ознакомьтесь с нашими руководствами по [Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) или [LM Studio](https://cookbook.openai.com/articles/gpt-oss/run-locally-lmstudio).

## Выберите модель

vLLM поддерживает оба размера модели gpt-oss:

- [**&lt;&lt;&lt;INL_0>>>**](https://huggingface.co/openai/gpt-oss-20b)
  - Меньшая модель
  - Требует около **16 ГБ видеопамяти**
- [**&lt;&lt;&lt;INL_1>>>**](https://huggingface.co/openai/gpt-oss-120b)
  - Наша большая полноразмерная модель
  - Рекомендуется с **≥60 ГБ видеопамяти**
  - Может работать на одном H100 или в многогпу конфигурациях

Обe модели идут **MXFP4-квантованные** из коробки.

## Быстрая настройка

1. **Установите vLLM**  
vLLM рекомендует использовать [uv](https://docs.astral.sh/uv/) для управления вашим Python окружением. Это поможет выбрать правильную реализацию в зависимости от среды. [Подробнее в их быстром старте](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#installation). Чтобы создать новое виртуальное окружение и установить vLLM, выполните:

&lt;&lt;&lt;CODE_0>>>

2. **Запустите сервер и скачайте модель**  
vLLM предоставляет команду &lt;&lt;&lt;INL_2>>>, которая автоматически скачает модель с HuggingFace и запустит сервер, совместимый с OpenAI, на &lt;&lt;&lt;INL_3>>>. Выполните следующую команду в терминале на сервере в зависимости от желаемого размера модели.

&lt;&lt;&lt;CODE_1>>>

## Использование API

vLLM предоставляет API, совместимые с **Chat Completions** и с **Responses**, поэтому вы можете использовать OpenAI SDK с минимальными изменениями. Ниже пример на Python:

&lt;&lt;&lt;CODE_2>>>

Если вы уже работали с OpenAI SDK, это покажется вам знакомым, и ваш существующий код будет работать, изменив лишь базовый URL.

## Использование инструментов (function calling)

vLLM поддерживает функцию вызова функций и предоставляет модели возможности для браузинга.

Вызов функций работает как через API Responses, так и через Chat Completions.

Пример вызова функции через Chat Completions:

&lt;&lt;&lt;CODE_3>>>

Поскольку модели могут выполнять вызов инструментов как часть цепочки рассуждений (chain-of-thought, CoT), важно возвращать рассуждения, полученные от API, обратно в последующий вызов инструмента, где вы предоставляете ответ, до тех пор, пока модель не придет к окончательному ответу.

## Интеграция с Agents SDK

Хотите использовать gpt-oss с OpenAI **Agents SDK**?

Оба Agents SDK позволяют переопределить базовый клиент OpenAI, чтобы он указывал на vLLM для ваших моделей, размещённых локально. Кроме того, для Python SDK можно использовать [LiteLLM интеграцию](https://openai.github.io/openai-agents-python/models/litellm/), чтобы делать проксирование к vLLM.

Пример на Python с Agents SDK:

&lt;&lt;&lt;CODE_4>>>

&lt;&lt;&lt;CODE_5>>>

## Использование vLLM для прямого сэмплинга

Помимо запуска vLLM как API-сервера с помощью &lt;&lt;&lt;INL_4>>>, вы можете использовать библиотеку vLLM на Python для непосредственного управления инференсом.

Если вы используете vLLM для прямого сэмплинга, важно убедиться, что ваши входные запросы соответствуют [формату ответа harmony](https://cookbook.openai.com/article/harmony), так как модель иначе работать не будет. Для этого можно использовать [SDK &lt;&lt;&lt;INL_5>>>](https://github.com/openai/harmony).

&lt;&lt;&lt;CODE_6>>>

После этого вы можете использовать harmony для кодирования и парсинга токенов, сгенерированных функцией generate у vLLM.

&lt;&lt;&lt;CODE_7>>>