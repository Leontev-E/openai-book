---
lang: ru
translationOf: openai-cookbook
---

# Формат ответа OpenAI harmony

[&lt;&lt;&lt;INL_0>>> модели](https://openai.com/open-models) были обучены на формате ответа harmony для определения структуры диалогов, генерации рассуждений и организации вызовов функций. Если вы не используете &lt;&lt;&lt;INL_1>>> напрямую, а через API или провайдера вроде Ollama, вам не нужно беспокоиться об этом, так как решение для инференса будет автоматически обрабатывать форматирование. Если вы разрабатываете собственное решение для инференса, это руководство поможет вам с форматом промпта. Формат спроектирован так, чтобы имитировать OpenAI Responses API, поэтому, если вы уже работали с этим API, формат должен показаться вам знакомым. &lt;&lt;&lt;INL_2>>> не следует использовать без формата harmony, так как он будет работать некорректно.

## Концепции

### Роли

Каждое сообщение, которое обрабатывает модель, имеет связанную с ним роль. Модель знает о пяти типах ролей:

| Роль           | Назначение                                                                                                                                                                                   |
| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| &lt;&lt;&lt;INL_3>>>    | Системное сообщение используется для задания усилий при рассуждении, метаинформации, такой как дата ограничения знаний, и встроенных инструментов                                            |
| &lt;&lt;&lt;INL_4>>>    | Сообщение разработчика используется для предоставления информации по инструкциям для модели (обычно это считается «системным промптом») и доступных инструментов функций                   |
| &lt;&lt;&lt;INL_5>>>    | Обычно представляет входные данные для модели                                                                                                                                                |
| &lt;&lt;&lt;INL_6>>>    | Вывод модели, который может быть вызовом инструмента или сообщением. Вывод может быть также связан с определённым «каналом», указывающим целью сообщения                                     |
| &lt;&lt;&lt;INL_7>>>    | Сообщения, представляющие выход инструмента. В роли сообщения будет использоваться конкретное имя инструмента                                                                                 |

Эти роли также отражают иерархию информации, которую модель применяет при конфликте инструкций: &lt;&lt;&lt;INL_8>>> \> &lt;&lt;&lt;INL_9>>> \> &lt;&lt;&lt;INL_10>>> \> &lt;&lt;&lt;INL_11>>> \> &lt;&lt;&lt;INL_12>>>

#### Каналы

Сообщения ассистента могут выводиться в трёх разных «каналах». Они используются для разделения ответов, предназначенных для пользователей, и внутренних сообщений.

| Канал           | Назначение                                                                                                                                                                                                                                                                                                                    |
| :-------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| &lt;&lt;&lt;INL_13>>>    | Сообщения, помеченные как final (финальные), предназначены для показа конечному пользователю и представляют ответы модели.                                                                                                                                                                                                   |
| &lt;&lt;&lt;INL_14>>>    | Сообщения, используемые моделью для цепочки рассуждений (chain of thought, CoT). **Важно:** сообщения в канале analysis не соответствуют стандартам безопасности, характерным для финальных сообщений. Избегайте показа их конечным пользователям.                                                                                  |
| &lt;&lt;&lt;INL_15>>>    | Любой вызов функции обычно инициируется на канале &lt;&lt;&lt;INL_16>>>, в то время как встроенные инструменты обычно вызываются на канале &lt;&lt;&lt;INL_17>>>, но иногда встроенные инструменты также могут выходить в &lt;&lt;&lt;INL_18>>>, этот канал иногда используется моделью для генерации [преамбул](#preambles) при вызове нескольких функций. |

## Библиотека harmony renderer

Рекомендуется использовать наш harmony renderer через [PyPI](https://pypi.org/project/openai-harmony/) или [crates.io](https://crates.io/crates/openai-harmony), поскольку он автоматически обрабатывает рендеринг ваших сообщений в нужном формате и преобразование их в токены для обработки моделью.

Ниже пример использования этого рендерера для создания системного промпта и короткой беседы.

&lt;&lt;&lt;CODE_0>>>

Кроме того, библиотека openai_harmony включает StreamableParser для разбора и декодирования по мере генерации моделью новых токенов. Это может быть полезно, например, для потоковой передачи вывода и обработки unicode-символов во время декодирования.

&lt;&lt;&lt;CODE_1>>>

## Формат промпта

Если вы решите построить собственный рендерер, нужно придерживаться следующего формата.

### Специальные токены

Модель использует набор специальных токенов для идентификации структуры вашего ввода. Если вы используете [tiktoken](https://github.com/openai/tiktoken), эти токены закодированы в кодировке &lt;&lt;&lt;INL_19>>>. Все специальные токены имеют следующий формат &lt;&lt;&lt;INL_20>>>.

| Специальный токен       | Назначение                                                                                                                                        | ID токена  |
| :--------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------ | :--------- |
| &lt;&#124;start&#124;>    | Обозначает начало [сообщения](#message-format). Сразу после следует «заголовок» сообщения с [ролью](#roles)                                      | &lt;&lt;&lt;INL_21>>> |
| &lt;&#124;end&#124;>      | Обозначает конец [сообщения](#message-format)                                                                                                   | &lt;&lt;&lt;INL_22>>> |
| &lt;&#124;message&#124;>  | Обозначает переход от «заголовка» сообщения к основному содержимому                                                                             | &lt;&lt;&lt;INL_23>>> |
| &lt;&#124;channel&#124;>  | Обозначает переход к информации о [канале](#channels) в заголовке                                                                                | &lt;&lt;&lt;INL_24>>> |
| &lt;&#124;constrain&#124;>| Обозначает переход к определению типа данных в [вызове инструмента](#receiving-tool-calls)                                                       | &lt;&lt;&lt;INL_25>>> |
| &lt;&#124;return&#124;>   | Обозначает, что модель завершила генерацию сообщения. Действительный «стоп-токен», по которому нужно прекратить инференс.                        | &lt;&lt;&lt;INL_26>>> |
| &lt;&#124;call&#124;>     | Обозначает желание модели вызвать инструмент. Действительный «стоп-токен», по которому нужно прекратить инференс.                                | &lt;&lt;&lt;INL_27>>> |

### Формат сообщения

Формат ответа harmony состоит из «сообщений», и модель может генерировать несколько сообщений за раз. Общая структура сообщения следующая:

&lt;&lt;&lt;CODE_2>>>

&lt;&lt;&lt;INL_28>>> содержит набор метаинформации, включая [роль](#roles). &lt;&lt;&lt;INL_29>>> означает конец полностью завершённого сообщения, но модель также может использовать другие стоп-токены, например, &lt;&lt;&lt;INL_30>>> для вызова инструментов и &lt;&lt;&lt;INL_31>>> для обозначения окончания генерации.

### Формат чата

Согласно формату сообщения, самый базовый формат чата содержит сообщение с ролью &lt;&lt;&lt;INL_32>>> и начало сообщения &lt;&lt;&lt;INL_33>>>.

#### Пример ввода

&lt;&lt;&lt;CODE_3>>>

Вывод начнётся с указания &lt;&lt;&lt;INL_34>>>. Например, &lt;&lt;&lt;INL_35>>> для вывода цепочки рассуждений. Модель может вывести несколько сообщений (в основном для цепочки рассуждений), для разделения которых используется токен &lt;&lt;&lt;INL_36>>>.

По завершении генерации она остановится либо на токене &lt;&lt;&lt;INL_37>>>, означающем окончательный ответ, либо на &lt;&lt;&lt;INL_38>>>, означающем необходимость вызова функции. В любом случае нужно завершить инференс.

#### Пример вывода

&lt;&lt;&lt;CODE_4>>>

Канал &lt;&lt;&lt;INL_39>>> будет содержать ответ на запрос пользователя. Подробнее про цепочку рассуждений см. в разделе [reasoning](#reasoning).

**Примечание по реализации:** токен &lt;&lt;&lt;INL_40>>> является стоп-токеном только во время декодирования. При добавлении сгенерированного ответа ассистента в историю диалога перед следующим ходом замените конечный &lt;&lt;&lt;INL_41>>> на &lt;&lt;&lt;INL_42>>> так, чтобы сохранённые сообщения полностью формировались как &lt;&lt;&lt;INL_43>>>. Предыдущие сообщения в промптах должны заканчиваться &lt;&lt;&lt;INL_44>>>. Для обучающих примеров/целевых ответов допустимо окончание на &lt;&lt;&lt;INL_45>>>; для хранящейся истории нормализуйте к &lt;&lt;&lt;INL_46>>>.

### Формат системного сообщения

Системное сообщение используется для предоставления общей информации системе. Это отличается от того, что обычно называется «системным промптом» в других форматах. Для этого существует раздел [developer message format](#developer-message-format).

В системном сообщении мы определяем:

1. **Идентичность** модели — всегда должна оставаться &lt;&lt;&lt;INL_47>>>. Если хотите изменить идентичность модели — используйте инструкции из [developer message](#developer-message-format).
2. Мета **даты** — конкретно &lt;&lt;&lt;INL_48>>> и &lt;&lt;&lt;INL_49>>>.
3. **Усилия при рассуждении** — задаваемые уровнями &lt;&lt;&lt;INL_50>>>, &lt;&lt;&lt;INL_51>>>, &lt;&lt;&lt;INL_52>>>.
4. Доступные каналы — для лучшей производительности должны соответствовать &lt;&lt;&lt;INL_53>>>, &lt;&lt;&lt;INL_54>>>, и &lt;&lt;&lt;INL_55>>>.
5. Встроенные инструменты — модель обучена на &lt;&lt;&lt;INL_56>>> и &lt;&lt;&lt;INL_57>>> инструментах. Подробности в разделе [built-in tools](#built-in-tools).

**Если вы определяете функции,** система также должна содержать заметку, что все вызовы инструментов должны идти в канал &lt;&lt;&lt;INL_58>>>.

Для оптимальной работы строго придерживайтесь этого формата.

#### Пример системного сообщения

Самое базовое системное сообщение:

&lt;&lt;&lt;CODE_5>>>

Если в секции сообщения разработчика присутствуют вызовы функций, используйте:

&lt;&lt;&lt;CODE_6>>>

### Формат сообщения разработчика

Сообщение разработчика представляет то, что обычно называется «системным промптом». Содержит инструкции для модели и (опционально) список доступных [функциональных инструментов](#function-calling) или формат вывода, которому должна следовать модель для [структурированных ответов](#structured-output).

Если вы не используете функциональные вызовы, сообщение разработчика будет выглядеть так:

&lt;&lt;&lt;CODE_7>>>

Где &lt;&lt;&lt;INL_59>>> заменяется на ваш «системный промпт».

Для определения вызываемых функций смотрите [соответствующий раздел](#function-calling).  
Для определения формата вывода для структурированных ответов — [раздел по структурированным ответам](#structured-output).

### Рассуждения

Модели gpt-oss — это модели рассуждений. По умолчанию используется средний уровень рассуждений. Чтобы контролировать уровень рассуждения, вы можете указать его в [системном сообщении](#system-message-format) как &lt;&lt;&lt;INL_60>>>, &lt;&lt;&lt;INL_61>>>, или &lt;&lt;&lt;INL_62>>>. Рекомендуемый формат:

&lt;&lt;&lt;CODE_8>>>

Модель выведет необработанную цепочку рассуждений (CoT) в сообщениях ассистента через канал &lt;&lt;&lt;INL_63>>>, а итоговый ответ будет выведен через &lt;&lt;&lt;INL_64>>>.

Например, для вопроса &lt;&lt;&lt;INL_65>>> модель может выдать следующий вывод:

&lt;&lt;&lt;CODE_9>>>

В этом случае цепочка рассуждений:

&lt;&lt;&lt;CODE_10>>>

А сам ответ:

&lt;&lt;&lt;CODE_11>>>

**Важно:** модель не обучалась соблюдать такие же стандарты безопасности в цепочке рассуждений, как в итоговом выводе. Не показывайте цепочку рассуждений пользователям, так как она может содержать вредоносный контент. [Подробнее в карточке модели](https://openai.com/index/gpt-oss-model-card/).

#### Обработка вывода рассуждений при дальнейшем сэмплинге

В общем случае следует сбрасывать предыдущий CoT при следующем сэмплинге, если ответы ассистента закончились сообщением в канале &lt;&lt;&lt;INL_66>>>. То есть, если первый ввод был:

&lt;&lt;&lt;CODE_12>>>

и привёл к выводу:

&lt;&lt;&lt;CODE_13>>>

Для корректной работы модели следующий ввод должен быть:

&lt;&lt;&lt;CODE_14>>>

Исключение — вызов функций/инструментов. Модель может вызывать инструменты в цепочке рассуждений, поэтому предыдущая цепочка рассуждений должна передаваться обратно как часть входа для последующего сэмплинга. Полный пример смотрите в разделе [function calling](#function-calling).

### Вызов функций

#### Определение доступных инструментов

Все функции, доступные модели, должны быть определены в [сообщении разработчика](#developer-message-format) в отдельном разделе &lt;&lt;&lt;INL_67>>>.

Функции описываются в синтаксисе, похожем на TypeScript, и сгруппированы в пространство имён &lt;&lt;&lt;INL_68>>>. Важно строго придерживаться формата для улучшения точности вызова функций. Подробнее о преобразовании JSON-схем в такой формат можно посмотреть в коде harmony renderer. Вот основные правила форматирования:

- Определять функцию как &lt;&lt;&lt;INL_69>>>, если у неё нет аргументов
- Для функций с аргументами называть аргумент &lt;&lt;&lt;INL_70>>> и встроить определение типа
- Добавлять комментарии с описаниями перед описанием поля
- Всегда использовать &lt;&lt;&lt;INL_71>>> как тип возвращаемого значения
- Оставлять пустую строку между определениями функций
- Оборачивать функции в пространство имён, обычно это &lt;&lt;&lt;INL_72>>>, чтобы не конфликтовать с [встроенными инструментами](#built-in-tools), на которых модель обучалась.

Пример полного входа с двумя функциями:

&lt;&lt;&lt;CODE_15>>>

#### Приём вызовов инструментов

Если модель решит вызвать инструмент, в заголовке сообщения она указывает &lt;&lt;&lt;INL_73>>> в формате &lt;&lt;&lt;INL_74>>>. Например, чтобы вызвать функцию &lt;&lt;&lt;INL_75>>> из примера выше, она укажет &lt;&lt;&lt;INL_76>>> в заголовке и &lt;&lt;&lt;INL_77>>> как канал — согласно системному сообщению. **Получатель определяется в роли или в разделе канала заголовка.**

Модель также может указать токен &lt;&lt;&lt;INL_78>>>, обозначающий тип входных данных для вызова функции. Поскольку он передаётся в JSON, &lt;&lt;&lt;INL_79>>> задаётся как &lt;&lt;&lt;INL_80>>>.

&lt;&lt;&lt;CODE_16>>>

#### Обработка вызовов инструментов

После обработки вызова функции нужно передать результат обратно модели в новом сообщении с выводом, следом за сообщением с вызовом.

Формат сообщения с результатом:

&lt;&lt;&lt;CODE_17>>>

В нашем примере получается:

&lt;&lt;&lt;CODE_18>>>

Когда вы собрали вывод для вызова функций, выполните инференс с полным содержанием:

&lt;&lt;&lt;CODE_19>>>

Как видно, мы передаём не только результат вызова функции обратно в модель для дальнейшего сэмплинга, но и предыдущую цепочку рассуждений («Нужно использовать функцию get_current_weather.»), чтобы модель имела все данные для продолжения рассуждений или выдачи окончательного ответа.

#### Преамбулы

Иногда модель генерирует «преамбулу», чтобы сообщить пользователю о планируемых вызовах инструментов. Например, если планируется вызвать несколько функций. В таком случае она сгенерирует сообщение ассистента в канале &lt;&lt;&lt;INL_81>>>, которое, в отличие от цепочки рассуждений, предназначено для конечного пользователя.

&lt;&lt;&lt;CODE_20>>>

Здесь модель сгенерировала план действий, чтобы проинформировать пользователя о нескольких шагов, которые она собирается выполнить.

### Структурированный вывод

Для управления поведением вывода модели вы можете определить формат ответа в конце [сообщения разработчика](#developer-message-format) с такой структурой:

&lt;&lt;&lt;CODE_21>>>

Имя формата работает аналогично имени схемы, которое вы можете указать в [Responses API](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#how-to-use), а схема — это JSON Schema.

Пример сообщения разработчика с определением схемы для списка покупок:

&lt;&lt;&lt;CODE_22>>>

Этот промпт сам по себе влияет только на поведение модели, но не гарантирует полного соответствия схеме. Для этого нужно самостоятельно строить грамматику и контролировать схему во время сэмплинга.

### Встроенные инструменты

Во время обучения моделей &lt;&lt;&lt;INL_82>>> они обучались с двумя распространёнными инструментами: поиск информации через браузер и исполнение кода на Python, чтобы улучшить результаты.

Если вы хотите реализовать такую функциональность, используйте формат ниже для повышения надёжности и точности.

Эти инструменты должны определяться в [системном сообщении](#system-message-format), а не в сообщении разработчика, добавляя туда секцию &lt;&lt;&lt;INL_83>>>.

#### Инструмент браузера

Чтобы определить инструмент браузера, добавьте его в секцию системного промта:

&lt;&lt;&lt;CODE_23>>>

Если модель решит вызвать действия в браузере, она будет использовать формат, аналогичный [вызовам функций](#function-calling), с двумя отличиями:

1. Запросы будут направляться в канал &lt;&lt;&lt;INL_84>>>
2. Получателями будут >>>INL_85>>>, &lt;&lt;&lt;INL_85>>>, &lt;&lt;&lt;INL_86>>> соответственно

#### Инструмент Python

&lt;&lt;&lt;CODE_24>>>

Если модель решит выполнить код Python, она будет использовать формат, аналогичный [вызовам функций](#function-calling), с двумя отличиями:

3. Запросы будут направляться в канал &lt;&lt;&lt;INL_87>>>
4. Получателем всегда будет &lt;&lt;&lt;INL_88>>>